{
  "hash": "1ec0694daf37778abbb5def1566d53bf",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Wrangling\"\nformat: html\nengine: python3\n---\n\n\n\n\n# VIDEO 1 {greenscreen} ~ 4 min\n<!-- ## Data -->\n\nWelcome to the module on data wrangling! In the last lesson, we explored a small, but exciting gapminder dataset, and created quite a few visualizations with it. But in real-world scenarios, datasets are often much larger. This brings new challenges, like focusing on specific subsets of data—perhaps observations from a single time period or a selection of variables related to a particular phenomenon.\n\nIn this module, we’ll learn how to subset data and create meaningful summaries that provide a high-level overview of trends or differences between groups. Summarized data is often presented in tables, so we’ll also introduce a package for creating clear, professional-looking tables.\n\nMost importantly, we’ll dive into the blazingly fast Polars package for data manipulation. As I mentioned earlier, Polars is powered by Rust, a high-performance programming language. Its core functionality is exposed to Python but can also be accessed from other languages. This means the data wrangling skills you gain here will be transferable beyond Python.\n\nBut first, let’s load the necessary packages for this module. In Python, it’s common to use the alias `pl` for Polars. We’ll also use a submodule called polars.selectors, aliasing it as cs — don’t worry, we’ll cover selectors in more detail soon. We’ll also import everything from Plotnine for data visualization and bring in the main function from the `great_tables` package for generating beautifully looking tables.\n\nHere’s the setup code. Place it in its own cell in your notebook and run it:\n\n\nNow, let’s explore our dataset. We’ll be looking at the WHO data about household pollution curated by the Our World In Data website.\n\nHousehold air pollution is primarily caused by burning polluting fuels like wood, animal dung, charcoal, agricultural waste, and kerosene in open fires or inefficient stoves. Globally, 2.1 billion people rely on these fuels for cooking, heating, and lighting. The poor combustion of these fuels leads to numerous health issues, such as pneumonia in children, and chronic diseases like obstructive pulmonary disease, lung cancer, stroke, and cardiovascular problems in adults.\n\nThis is a complex problem, and like many complex problems, it can be examined from different perspectives. We have three datasets to work with:\n\n- Causes of death linked to household pollution\n- Types of fuel used for cooking in various countries\n- Proportion of the population with access to clean cooking fuels\n\nEach dataset offers a unique angle on this issue. Let’s dive in and start exploring!\n\n# VIDEO 2 {greenscreen} ~ 12 min\n<!-- # Part I. Basics -->\n\nIn this section, we’ll start working with the datasets about household air pollution. These datasets are stored as comma-separated values, or CSV files. CSV is a simple text format often used for storing tabular data. Think of it as a stripped-down version of Excel—just the data, no formatting or formulas. In fact, you can even open CSV files directly in Excel if you want to take a look at them.\n\nWe’ve prepared these datasets for you and stored them in the course repository on GitHub. While Polars allows us to read files directly from remote locations, you can also download the files and load them from your local project directory.\n\nHere’s the code to load the three datasets we’ll use:\n\n<!-- # Indoor air pollution datasets -->\n\n\nLet’s break this down. We’re using the `read_csv()` function from Polars to load the data. This function takes a single mandatory argument: the file path, written as a string in quotes. The equal sign indicates that we assign the content of the file to the variable, listed on the left. The `pl.read_csv()` returns a data frame. So, from now on, we can simply refer to this variable name whenever we need to access the dataframe which we read fromm the CSV file without needing to re-import it.\n\n<!-- ## Data Overview -->\n\nTo understand the data we’re working with, it’s helpful to preview it in a few ways. For instance, simply typing the name of a dataset—like clean_fuels—will display a preview of the first five and last five rows of the corresponding data frame.\n\nRows in a data frame are often referred to as observations or records, while columns are known as variables or features. If you want to see more rows than the default preview, you can use the .head() method and specify the number of rows to display:\n\n\nYou may have noticed the parentheses around the code block. This lets you write the code across multiple lines without worrying about indentation.\n\nPreviewing the first few rows gives you an initial sense of the dataset’s structure and content. If you’re curious about the last few rows, there’s also a `.tail()` method you can use in a similar way.\n\nFor a broader overview of your data, you can use the `.describe()` method:\n\n\nThe `.describe()` method provides a statistical summary of numerical columns, including metrics like the mean, standard deviation, minimum, maximum, and various quantiles. It’s especially useful for large datasets when you want to quickly understand key metrics.\n\nOne thing to note: if a column contains missing values, Polars will display these as `null`. Polars treats missing values as “contagious,” so any operation involving them will also result in missing values in the output. This behavior applies to all statistical operations. We’ll see more examples of this later.\n\nBoth `.head()` and `.describe()` return a data frame, which means you can chain these operations together. Method chaining is a powerful coding style that helps you write clean, readable, and maintainable code. \n\nHere are a couple of challenges for you to practice combining the functions into a method chain:\n\n:::{.challenge}\n- Take first 25 records of `clean_fuels` data frame and then calculate statistical summary\n- Compute statistical summary of the whole data and then present only quantile summaries of each column. The quantiles include minimum, maximum, as well as the 25th, 50th and 75th quantile.\n:::\n\n\nSometimes, datasets have many columns, making it difficult to gain a full overview using methods like `head()` or `describe()`. For these cases, Polars provides a particularly useful method called `glimpse()`.\n\nWhen you `glimpse()`, the dataset’s structure is displayed horizontally. Each variable is listed as a row, making it easier to scan through all columns, even if you’re working with a limited screen space. Here's how it looks in action:\n\n\nNow, here's a question for you: what happens if you try to use `glimpse()` in a method chain? \n\n:::{.question}\nCan you chain the operation `head()` after calling `glimpse()`? What do you think the output will be?\n:::\n\nThe answer is: no, you cannot. `glimpse()`, does not return you a data frame. Instead, the output of `glimpse()` is the text printout meant solely for viewing. No further operations can be applied to it. If you attempt to chain additional methods, you’ll encounter an error. Give it a try if you want! Polars will throw an error saying that the `head()` method cannot be applied to a `NoneType`, which is the type of output `glimpse()` returns.\n\nIf you understand how to use `head()`, `tail()`, `describe()`, and `glimpse()`, you have powerful tools at your disposal to explore and familiarize yourself with any dataset before diving deeper into your analysis.\n\n# VIDEO 3 {greenscreen} ~ 12 min\n<!-- ## select/drop -->\n\nOne of the most common tasks in data analysis is selecting specific variables or columns from a dataset. Let’s start by pulling out the country information from the `clean_fuels` data. Pause the video for a moment and try running this code:\n\n\nHere, we’re using the `select()` method to isolate a column. Notice how the column name is wrapped in the `pl.col()` function. This wrapper explicitly tells Polars that we’re referring to a column in the dataframe.\n\nBut here’s something cool—you can skip the `pl.col()` wrapper in certain cases! For example, this code:\n\n\n...does the exact same thing as this:\n\n\nPretty neat, right? The `select()` method can directly interpret strings as column names, making your code a little cleaner and quicker to write.\n\nWhen you wrap a column name in `pl.col()`, you’re creating an expression. An expression is like a recepie — it doesn’t do anything on its own. For example, if you run this code:\n\n\n...nothing happens. It just returns something called an \"unevaluated expression\". But when you evaluate that expression in the context of a dataset, it returns something useful. For instance:\n\n\nHere, the `select()` method acts as an evaluation environment, turning the `pl.col()` expression into actual data. \n\n:::{.callout-tip}\n`select()` is one of the several methods in Polars that can evaluate expressions. While `select()` is highly versatile and can do other things as well, for now, we’ll focus on its simplest use case: extracting columns from a data frame.\n:::\n\nThe `pl.col()` wrapper is super flexible, and it’s going to be central as we build more advanced expressions in Polars. For instance, you can use `pl.col()` to refer to multiple columns simultaneously:\n\n::: {#882e583a .cell execution_count=12}\n``` {.python .cell-code}\n(clean_fuels  \n    .select(pl.col(\"country_code\", \"country\")))  \n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (6_402, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>country_code</th><th>country</th></tr><tr><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;SSD&quot;</td><td>&quot;South Sudan&quot;</td></tr><tr><td>&quot;NIU&quot;</td><td>&quot;Niue&quot;</td></tr><tr><td>&quot;TKL&quot;</td><td>&quot;Tokelau&quot;</td></tr><tr><td>&quot;COK&quot;</td><td>&quot;Cook Islands&quot;</td></tr><tr><td>&quot;PLW&quot;</td><td>&quot;Palau&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;AUT&quot;</td><td>&quot;Austria&quot;</td></tr><tr><td>&quot;DEU&quot;</td><td>&quot;Germany&quot;</td></tr><tr><td>&quot;SWE&quot;</td><td>&quot;Sweden&quot;</td></tr><tr><td>&quot;PRT&quot;</td><td>&quot;Portugal&quot;</td></tr><tr><td>&quot;IND&quot;</td><td>&quot;India&quot;</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nSometimes, typing out long column names can feel like a chore, especially when you’re working with many columns. But don’t worry — Polars makes it easy to select columns by their position in the dataset. For example, this code selects the second and third columns by their numerical index:\n\n::: {#acbb5cbd .cell execution_count=13}\n``` {.python .cell-code}\n(clean_fuels  \n    .select(pl.nth(1,2)))  \n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (6_402, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>country_code</th><th>country</th></tr><tr><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;SSD&quot;</td><td>&quot;South Sudan&quot;</td></tr><tr><td>&quot;NIU&quot;</td><td>&quot;Niue&quot;</td></tr><tr><td>&quot;TKL&quot;</td><td>&quot;Tokelau&quot;</td></tr><tr><td>&quot;COK&quot;</td><td>&quot;Cook Islands&quot;</td></tr><tr><td>&quot;PLW&quot;</td><td>&quot;Palau&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;AUT&quot;</td><td>&quot;Austria&quot;</td></tr><tr><td>&quot;DEU&quot;</td><td>&quot;Germany&quot;</td></tr><tr><td>&quot;SWE&quot;</td><td>&quot;Sweden&quot;</td></tr><tr><td>&quot;PRT&quot;</td><td>&quot;Portugal&quot;</td></tr><tr><td>&quot;IND&quot;</td><td>&quot;India&quot;</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n:::{.callout-caution}\nColumn indices in Polars are 0-based. That means the first column is index 0, the second column is index 1, and so on. \n:::\n\nWhat about negative numbers? They’re a handy shortcut for selecting columns from the back of the dataset. For instance, `-1` refers to the last column, and this code will select the first and last columns:\n\n::: {#5571985f .cell execution_count=14}\n``` {.python .cell-code}\n(clean_fuels  \n    .select(pl.nth(0, -1)))  \n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (6_402, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>region</th><th>prop_clean_fuels_cooking_pct</th></tr><tr><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;Africa&quot;</td><td>0.0</td></tr><tr><td>&quot;Western Pacific&quot;</td><td>98.5</td></tr><tr><td>&quot;Western Pacific&quot;</td><td>28.3</td></tr><tr><td>&quot;Western Pacific&quot;</td><td>72.7</td></tr><tr><td>&quot;Western Pacific&quot;</td><td>29.45</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;Europe&quot;</td><td>100.0</td></tr><tr><td>&quot;Europe&quot;</td><td>100.0</td></tr><tr><td>&quot;Europe&quot;</td><td>100.0</td></tr><tr><td>&quot;Europe&quot;</td><td>100.0</td></tr><tr><td>&quot;South-East Asia&quot;</td><td>11.1</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n:::{.callout-caution}\nA note of caution: Selecting columns by the order of their occurence can be risky. If your dataset’s structure changes, you might accidentally select the wrong columns. So, use the nth() function sparingly.\n:::\n\nNow, let’s talk about the opposite of `select()` — the `drop()` method. The `drop()` method removes specific columns from your dataset, leaving everything else intact. For example:\n\n::: {#26a0682a .cell execution_count=15}\n``` {.python .cell-code}\n(clean_fuels  \n    .drop(pl.col(\"country\"), pl.col(\"region\")))  \n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (6_402, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>country_code</th><th>year</th><th>pop_clean_fuels_cooking_mln</th><th>prop_clean_fuels_cooking_pct</th></tr><tr><td>str</td><td>i64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;SSD&quot;</td><td>2022</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;NIU&quot;</td><td>2022</td><td>0.002</td><td>98.5</td></tr><tr><td>&quot;TKL&quot;</td><td>2022</td><td>0.0004</td><td>28.3</td></tr><tr><td>&quot;COK&quot;</td><td>2022</td><td>0.013</td><td>72.7</td></tr><tr><td>&quot;PLW&quot;</td><td>2022</td><td>0.007</td><td>29.45</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;AUT&quot;</td><td>1990</td><td>7.72</td><td>100.0</td></tr><tr><td>&quot;DEU&quot;</td><td>1990</td><td>79.12</td><td>100.0</td></tr><tr><td>&quot;SWE&quot;</td><td>1990</td><td>8.57</td><td>100.0</td></tr><tr><td>&quot;PRT&quot;</td><td>1990</td><td>9.95</td><td>100.0</td></tr><tr><td>&quot;IND&quot;</td><td>1990</td><td>96.58</td><td>11.1</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nDopping is equivalent to selecting all columns **except** the ones you want to exclude. Here’s how could would write it using in terms of selection:\n\n::: {#8d843c68 .cell execution_count=16}\n``` {.python .cell-code}\n(clean_fuels  \n    .select(pl.all().exclude(\"country\", \"region\")))  \n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (6_402, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>country_code</th><th>year</th><th>pop_clean_fuels_cooking_mln</th><th>prop_clean_fuels_cooking_pct</th></tr><tr><td>str</td><td>i64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;SSD&quot;</td><td>2022</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;NIU&quot;</td><td>2022</td><td>0.002</td><td>98.5</td></tr><tr><td>&quot;TKL&quot;</td><td>2022</td><td>0.0004</td><td>28.3</td></tr><tr><td>&quot;COK&quot;</td><td>2022</td><td>0.013</td><td>72.7</td></tr><tr><td>&quot;PLW&quot;</td><td>2022</td><td>0.007</td><td>29.45</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;AUT&quot;</td><td>1990</td><td>7.72</td><td>100.0</td></tr><tr><td>&quot;DEU&quot;</td><td>1990</td><td>79.12</td><td>100.0</td></tr><tr><td>&quot;SWE&quot;</td><td>1990</td><td>8.57</td><td>100.0</td></tr><tr><td>&quot;PRT&quot;</td><td>1990</td><td>9.95</td><td>100.0</td></tr><tr><td>&quot;IND&quot;</td><td>1990</td><td>96.58</td><td>11.1</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nThe `pl.all()` function refers to all columns, and the `exclude()` method lets you refine the selection by removing specific ones. \n\n:::{.callout-note}\n\nA quick reminder: dropping columns doesn’t modify your original dataset. It only affects the result of that query. Unless you explicitly overwrite the original dataframe, everything stays the same. So feel free to experiment!\n\n:::\n\nNow it’s your turn. Select the columns related to the number and the proportion of the people with access to clean fuels. Try using both selection *by name*, *by index*, and dropping the columns you dont need.\n\n:::{.challenge}\nSelect the columns related to the number and the proportion of people with access to clean fuel for cooking from the `clean_fuels` dataset.\n:::\n\nPlease, pause the video and try couple of different ways of selecting these columns. \n\n::: {#7fc64609 .cell execution_count=17}\n``` {.python .cell-code}\n(clean_fuels  \n    .select(pl.col(\"pop_clean_fuels_cooking_mln\"), pl.col(\"prop_clean_fuels_cooking_pct\"))) \n\n(clean_fuels  \n    .select(pl.nth(-2,-1))) \n\n(clean_fuels  \n    .drop(\"region\", \"country_code\", \"country\", \"year\"))\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (6_402, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>pop_clean_fuels_cooking_mln</th><th>prop_clean_fuels_cooking_pct</th></tr><tr><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0.0</td><td>0.0</td></tr><tr><td>0.002</td><td>98.5</td></tr><tr><td>0.0004</td><td>28.3</td></tr><tr><td>0.013</td><td>72.7</td></tr><tr><td>0.007</td><td>29.45</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>7.72</td><td>100.0</td></tr><tr><td>79.12</td><td>100.0</td></tr><tr><td>8.57</td><td>100.0</td></tr><tr><td>9.95</td><td>100.0</td></tr><tr><td>96.58</td><td>11.1</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nGot it? Great! Both approaches (selecting specific columns or dropping the ones you don’t need) give you the same result. Expressions like these make your analysis more dynamic and efficient, so you can quickly adapt to different datasets or scenarios.\n\n# VIDEO 4 {greenscreen} ~ 7 min\n<!-- ## selectors -->\n\nSelecting columns is such a common task that Polars has a dedicated module for it. It is called `polars.selectors`. This module provides a collection of methods specifically designed to simplify picking columns from a data frame. `polars.selectors` is often aliased as cs for convenience. \n\n<!-- :::{.callout-note}\nHave a look at the [documentation for selectors](https://docs.pola.rs/api/python/stable/reference/selectors.html). \n::: -->\n\nLet's make sure we import the selectors module:\n\n\nAmong the most useful selectors are, of course, selectors by name and by column index (for which we might not really need selectors, because those can be picked out with `pl.col()` and `pl.nth()`). \n\n\nSelecting first and last columns are so common, there are useful shortthands `cs.first()` and `cs.last()`. To select all columns other than the one you specified, you can use the tilde ~ operator. Tilde operator works with all methods in `cs.` module and negates the selection. For example `~cs.last()` refers to all columns other than the last one.\n\nSelectors can target columns based on their data types! For example, `cs.numeric()` picks all numeric columns. And if you want non-numeric columns, you can just negate it with ~.\n\nAnd now it is your turn! Practice selecting first, everyhing other than the first, as well as all non-numeric columns. Use selector class for this. Pause the video and give it a try!\n\n<!-- :::{.challenge}\nUse `polars.selectors` aliased as `cs.` to select\n- first column\n- everyhing other than the first column\n- all non-numeric columns\n::: -->\n\n\nFantastic work! With a wide menu of selector methods, plus column and index-based expressions like `pl.col()` and `pl.nth()`, Polars gives you incredible flexibility in working with your data. These tools will become invaluable as we move into crafting more complex expressions.\n\nStay tuned — there’s a lot more to explore!\n\n# VIDEO 5 {greenscreen} ~ 12 min\n<!-- ## Filter -->\n\nNow let’s talk about filtering — an essential part of data analysis. In Polars, you can use filtering to subset your dataset based on logical conditions, using the magic of expressions. Logical operations are one of the simplest and most common use cases for expressions. For example, you can compare every value in the `region` column to the string \"Europe\". If there’s a match, Polars returns True; otherwise, it returns False.\n\nLet’s see how this works in code:\n\n\nHere, the `filter()` method applies the logical condition, and only rows where the region is \"Europe\" are included in the result. Notice that for exact comparisons, we use the double equals sign `==`. Similarly, for inequalities, we can use operators like `<=`, `>=`, `<`, or `>`. Not equal is spelled out as `!=`.\n\nBut filtering doesn’t stop there — you can combine multiple conditions to create more complex filters. \n\nHere's a challenge for you. Can you find all the countries in Europe where the majority of the population lacked access to clean fuels for cooking at the end of 2022? Take a moment to write this expression. Pause the video if you need to.\n\n<!-- :::{.challenge}\nWere there any countries in Europe in 2022 where the majority of people lacked access to clean fuel for cooking?\n::: -->\n\n\nWhat did you get? Oh, wow! Over half the population of Bosnia still lacks access to clean fuels for cooking. That’s a powerful insight!\n\nLet’s zoom in on Bosnia to better understand the data. Bosnia’s country code is \"BIH\", but you can also use country name to filter, if you prefer.\n\n\nWe are interested in tracking how the proportion of the population with access to clean fuels for cooking has changed over the years. Let's create a plot, with the year on the x-axis and the proportion of population on the y-axis.\n\nIf you remember from the Plotnine module, the dataset goes into the first argument of the ggplot function.\n\nHere’s one way to do this:\n\n\nThis works, but the code feels a little cluttered. It’s not immediately clear where the dataset comes from.\n\nLet’s clean this up using the `.pipe()` method. The `.pipe()` method hands the data to the `ggplot()` function, placing it as the first argument. \n\n<!-- \n```\n(data\n    .some_operation()\n    .other_operation()\n    .pipe(ggplot)\n    + ...)\n\n# the same as\ndf = data.some_operation().other_operation()\nggplot(df)\n+ ...\n\n``` \n-->\n\nThis keeps the code clean and modular. Everything after `.pipe(ggplot)` is Plotnine-specific code.\n\n\nNice!\n\nWhat if you’re not sure how a country’s name is spelled in the dataset? For example, is it “Czech Republic” or just “Czechia”?\n\nIn this case, you can use partial string matching to find it.\n\n\nHere, we use the `str.starts_with()` method, which checks if strings in the `country` column start with the letters “Cz.” Ah, there it is — “Czechia”! \n\nPolars offers several handy string operations. For example:\n\n- str.starts_with()\n- str.ends_with()\n- str.contains()\n\nYou’ll see more of these as we progress, but these three are powerful enough to help you tackle the following challenge.\n\nFilter the data for your own country and visualize the proportion of people with access to clean fuels. Once you’re happy with the subset of your data, use ggplot and everything you’ve learned about Plotnine to create a polished visualization.\n\n<!-- :::{.challenge}\nVisualize the proportion of people with access to clean fuels in your own country\n::: -->\n\n\nThis looks fantastic! Great work visualizing your country’s data.\n\nIn the next section, we’ll explore adding more columns to our dataset and practice advanced subsetting and visualization techniques. Stay tuned!\n\n# VIDEO 6 {greenscreen} ~ 3 min\n<!-- ## More practice with filtering -->\n\nLet’s apply what we’ve learned about filtering to visualize the causes of death in some European countries.\n\n\nThis dataset contains both summarized and detailed breakdowns of deaths for every country and year. Take a look at the column labeled cause_of_death. When this column says \"All causes,\" it represents the total deaths for that country and year—a sum of all the other rows.\n\nLet’s zoom in on Bosnia for a single year, say 2010, to understand this better.\n\n\nOne of the rows is labeled \"All causes\" with 4,816 deaths. This total matches the sum of the individual causes of death. While it’s useful to have the total, it can lead to double counting if we include it in our analysis. \n\nNow let’s expand our view to include all European countries for which we have death data. We’ll exclude the totals and focus on trends for each specific cause of death. Faceting will help us visualize these trends country by country.\n\n\nMost of the trends appear to be decreasing, which is good news. However, even with free y-axis scales for each country, the differences in scale make it hard to compare trends across countries. Look at Moldova! There’s a dramatic improvement in death cases here. Meanwhile, heart- and stroke-related deaths in neighboring Russia are on the rise.\n\nIt would be good to put these numbers in perspective using population of these countries. Is this something we could calculate from our `clean_fuels` dataset?\n\n# VIDEO 7 {greenscreen} ~ 11 min\n<!-- # mutating(with_columns) -->\n\nRemember, the `clean_fuels` data told us both the number of people with access to non-polluting fuels and the proportion of the population they represent, expressed as a percentage. With this, we can reverse-engineer the population for each country.\n\nTo create new variables, we use the `with_columns()` method. This method lets us add or modify columns using expressions.\n\nLet’s start by converting the percentage of people with access to clean fuels into a true proportion by dividing it by 100.\n\n\nHere, we create a new column called `prop`. The expression starts with `pl.col(\"prop_clean_fuels_cooking_pct\")`, and then we specify the operation: dividing by 100. Easy enough, right?\n\nNow that we have the true proportion, we can calculate the total population. Let's divide the population by proportion. \n\n\nNotice that I wrapped the division operation in parentheses to ensure it’s evaluated correctly. I also used the alias() method to specify a name for the new column: `population`. This code calculates and adds two new columns:\n\n- `prop`: the true proportion of the population with access to clean fuels.\n- `population`: the estimated total population for each row.\n\nThis looks good. Lets assign it to a variable and inspect the updated dataset.\n\n\nOh, look! We’ve got some `NaN` values in the `population` column. These appear because we divided by zero wherever the proportion was zero. Division by zero is, understandably, illegal in most places — and in Python, it results in `NaN`.\n\n`NaN`, or \"Not a Number,\" is a special marker for missing or undefined values. It propagates through calculations, which means any further operations on these rows will also result in `NaN`. This is why the mean and standard deviation of the population column are also `NaN` in the summary.\n\nLet's see how many rows in our dataset contain illegal population estimates. We can use the `.is_nan()` method, which evaluates whether a column contains `NaN` values.\n\n\nUh-oh! 35 rows! Perhaps `fuel_types`, the third data source we looked at in the beginning, could be a better source of population data?\n\nThe `fuel_types` dataset contains both the proportion of people using a specific cooking fuel and the absolute number of users. Since it includes multiple estimates for each country and year — one for each fuel type — it might give us more opportunities to calculate valid population estimates.\n\nGo ahead and add a population column to the `fuel_types` dataset. Save the extended data frame under a new name - we will need it later.\n\n<!-- :::{.challenge}\nAdd a population estimate to the `fuel_types` dataset and save it under a new variable name\n::: -->\n\n\nThis looks promising! Let's have a look at our favorite Bosnia\n\n\nInteresting! We still get some `NaN` values, for example, look here: in 2022 no one was using Kerosene to cook food. Thanks goodness! But now we get several estimates of Bosnia’s population — 3.0, 3.5, 3.53, 3.48 million. These slight differences arise from rounding imprecisions in the proportions or the number of people using the specific fuel type. While these variations are minor, they make it tricky to work with the data directly.\n\nWouldn’t it be nice to level out these immaterial differences, by say, averaging? In the next section, we’ll learn how to do just that: grouping and aggregating. Stay tuned!\n\n# VIDEO 8 {greenscreen} ~ 14 min\n<!-- # summarizing (group_by, agg) -->\n\nOne of the most important tasks for data analysts is creating summaries of the data, particularly by groups. In our newly prepared dataset, we’re interested in summarizing the total population for each country by year. In Polars, this is a two-step process. First, we define the groups using `.group_by()`. Then, we aggregate the data using `.agg()`.\n\n\nThis gives us a mean population estimate for each country and year. But you might notice something peculiar—some countries and years still show `NaN` values for the population. These missing values are a result of the division by zero we encountered earlier. To address this, we have a couple of options:\n\n1. Drop the missing values directly before averaging, using .drop_nans().\n2. Filter out zero-valued records in the \"proportion of population with access to clean fuels\", which was used in the denominator when calculating the population column.\n\n\nNow, let’s build on this idea. What if we wanted to summarize populations at a higher level — say, by region — and visualize how total population changes over time? \n\nGive it a try! I suggest you start by by calculating the average population for each country and year. Then, you can group this data by region and year, summing the populations for all countries in each region. Finally, plot the results. \n\nTake your time and think through what type of aggregation you would need to do.\n\n<!-- :::{.challenge}\nPlot the total population per region over time.\n::: -->\n\n\nWhen we visualize the results, the trends are striking. Populations in Africa and Southeast Asia are growing at a much faster pace compared to other regions. For Africa, this growth rate might even be accelerating. On the other hand, regions like Asia and the Western Pacific show signs of population growth slowing down. These insights help us understand global population dynamics and can inform policies in areas like health, infrastructure, and environmental planning.\n\nNow that we’ve explored population estimates and regional dynamics, let’s take on another challenge: examining the changes in the popularity of different cooking fuels worldwide.\n\nHere’s your next task:\n\n<!-- :::{.challenge}\nVisualize the total number of people using each type of fuel for cooking, for every year, worldwide.\n::: -->\n\n\nThis plot looks fantastic! By aggregating by fuel type and year, we can clearly see trends in fuel usage globally. But we can take this further. What if we wanted to break this data down by region? Faceting would allow us to examine the trends within each world region more closely.\n\n\nWith this plot, we can see unique trends for every region. Across the globe, electricity and gas are becoming more prevalent. Yet, biomass remains crucial in regions like Africa and Southeast Asia. However, there’s still a challenge here. Because the number of people using each fuel type varies so widely across regions, it’s difficult to compare the energy mix within each region.\n\nTo make the energy mix clearer, we can represent the share of the population relying on each fuel type within a region. This requires dividing the number of people using each fuel by the annual total population for the region. We’ll use Polars’ `.over()` method to calculate these totals for subgroups without collapsing the data.\n\n\nI wrap the expression into parenthesis and pass it to `.over()` method which defines the scope for the `.sum()` operation. Note that this is different from aggregation, because the number of rows actually does not change. Lets visualize it!\n\n\nWhat a graph! What a story! In Africa, biomass remains dominant, though natural gas is beginning to make inroads. The Americas, Europe, and Eastern Mediterranean rely heavily on gas, with electricity emerging as a significant player in Europe and the Western Pacific. Southeast Asia, meanwhile, is undergoing a rapid transition from biomass to gas — a remarkable shift in just a few decades. This visualization gives us a profound understanding of how cooking fuel usage has evolved globally and regionally over time.\n\n# VIDEO 9 {greenscreen} ~ 3 min\n\nIt’s time for you to take on another challenge — one that highlights the global reliance on coal and charcoal for cooking.\n\n<!-- :::{.challenge}\nCreate a column chart showing the top 10 countries by the number of people using coal and charcoal for cooking in 2022. Color the columns by continent. How many of these top-10 countries are in Africa?\n::: -->\n\n\nSeven — possibly eight, if you include Somalia — of the top-10 countries using coal for cooking are in Africa! Many African nations rely heavily on these highly polluting and dangerous carbon-based fuels. Ensuring access to cleaner, safer energy sources is essential to improving health outcomes and accelerating the transition to sustainable energy.\n\nIn the next section, we’ll learn how to combine datasets through joins and reshape them using pivots—powerful tools for transforming and enriching our analyses. See you soon, as we dive deeper into the art and science of data manipulation!\n\n",
    "supporting": [
      "wrangling_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}