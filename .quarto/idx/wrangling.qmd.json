{"title":"Wrangling","markdown":{"yaml":{"title":"Wrangling","format":"html","engine":"python3"},"headingText":"Data","containsRefs":false,"markdown":"\n\n\nWelcome to the module on data wrangling! In the last lesson, we explored a small, but exciting gapminder dataset, and created quite a few visualizations with it. But in real-world scenarios, datasets are often much larger. This brings new challenges, like focusing on specific subsets of data—perhaps observations from a single time period or a selection of variables related to a particular phenomenon.\n\nIn this module, we’ll learn how to subset data and create meaningful summaries that provide a high-level overview of trends or differences between groups. Summarized data is often presented in tables, so we’ll also introduce a package for creating clear, professional-looking tables.\n\nMost importantly, we’ll dive into the blazingly fast Polars package for data manipulation. As I mentioned earlier, Polars is powered by Rust, a high-performance programming language. Its core functionality is exposed to Python but can also be accessed from other languages. This means the data wrangling skills you gain here will be transferable beyond Python.\n\nBut first, let’s load the necessary packages for this module. In Python, it’s common to use the alias `pl` for Polars. We’ll also use a submodule called polars.selectors, aliasing it as cs — don’t worry, we’ll cover selectors in more detail soon. We’ll also import everything from Plotnine for data visualization and bring in the main function from the `great_tables` package for generating beautifully looking tables.\n\nHere’s the setup code. Place it in its own cell in your notebook and run it:\n```{python}\n#| label: setup\nimport polars as pl\nimport polars.selectors as cs\nfrom plotnine import *\nfrom great_tables import GT\n```\n\nNow, let’s explore our dataset. We’ll be looking at the WHO data about household pollution curated by the Our World In Data website.\n\nHousehold air pollution is primarily caused by burning polluting fuels like wood, animal dung, charcoal, agricultural waste, and kerosene in open fires or inefficient stoves. Globally, 2.1 billion people rely on these fuels for cooking, heating, and lighting. The poor combustion of these fuels leads to numerous health issues, such as pneumonia in children, and chronic diseases like obstructive pulmonary disease, lung cancer, stroke, and cardiovascular problems in adults.\n\nThis is a complex problem, and like many complex problems, it can be examined from different perspectives. We have three datasets to work with:\n\n- Causes of death linked to household pollution\n- Types of fuel used for cooking in various countries\n- Proportion of the population with access to clean cooking fuels\n\nEach dataset offers a unique angle on this issue. Let’s dive in and start exploring!\n\n# Part I. Basics\n\nIn this section, we’ll start working with the datasets about household air pollution. These datasets are stored as comma-separated values, or CSV files. CSV is a simple text format often used for storing tabular data. Think of it as a stripped-down version of Excel—just the data, no formatting or formulas. In fact, you can even open CSV files directly in Excel if you want to take a look at them.\n\nWe’ve prepared these datasets for you and stored them in the course repository on GitHub. While Polars allows us to read files directly from remote locations, you can also download the files and load them from your local project directory.\n\nHere’s the code to load the three datasets we’ll use:\n\n# Indoor air pollution datasets\n\n```{python}\nhhap_deaths = pl.read_csv(\"hhap/hhap_deaths.csv\")\nfuel_types = pl.read_csv(\"hhap/cooking_by_fuel_type.csv\")\nclean_fuels = pl.read_csv(\"hhap/clean_fuels_cooking.csv\")\n```\n\nLet’s break this down. We’re using the `read_csv()` function from Polars to load the data. This function takes a single mandatory argument: the file path, written as a string in quotes. The equal sign indicates that we assign the content of the file to the variable, listed on the left. The `pl.read_csv()` returns a data frame. So, from now on, we can simply refer to this variable name whenever we need to access the dataframe which we read fromm the CSV file without needing to re-import it.\n\n## Data Overview\n\nTo understand the data we’re working with, it’s helpful to preview it in a few ways. For instance, simply typing the name of a dataset—like clean_fuels—will display a preview of the first five and last five rows of the corresponding data frame.\n\nRows in a data frame are often referred to as observations or records, while columns are known as variables or features. If you want to see more rows than the default preview, you can use the .head() method and specify the number of rows to display:\n```{python}\n(clean_fuels  \n    .head(10))\n```\n\nYou may have noticed the parentheses around the code block. This lets you write the code across multiple lines without worrying about indentation.\n\nPreviewing the first few rows gives you an initial sense of the dataset’s structure and content. If you’re curious about the last few rows, there’s also a .tail() method you can use in a similar way.\n\nFor a broader overview of your data, you can use the .describe() method:\n\n```{python}\n(clean_fuels  \n    .describe())\n```\n\nThe .describe() method provides a statistical summary of numerical columns, including metrics like the mean, standard deviation, minimum, maximum, and various quantiles. It’s especially useful for large datasets when you want to quickly understand key metrics.\n\nOne thing to note: if a column contains missing values, Polars will display these as null. Polars treats missing values as “contagious,” so any operation involving them will also result in missing values in the output. This behavior applies to all statistical operations. We’ll see more examples of this later.\n\nBoth .head() and .describe() return a data frame, which means you can chain these operations together. Method chaining is a powerful coding style that helps you write clean, readable, and maintainable code. \n\nHere's the first challenge for you: combine the functions you learned so far to create a method chain:\n\n:::{.challenge}\n- Take first 25 records of clean_fuels data frame and calculate summary statistical summary\n- Compute statistical summary of the whole data and then present only quantile summaries of each column. The quantiles include minimum, maximum as well as the 25th, 50th and 75th quantile.\n:::\n\n```{python}\n(clean_fuels\n    .head(25)\n    .describe())\n\n(clean_fuels\n    .describe()\n    .tail(5))\n```\n\nSometimes, datasets have many columns, making it difficult to gain a full overview using methods like head() or describe(). For these cases, Polars provides a particularly useful method called glimpse().\n\nWhen you use glimpse(), the dataset’s structure is displayed horizontally. Each variable is listed as a row, making it easier to scan through all columns, even if you’re working with a limited screen space. Here's how it looks in action:\n\n```{python}\n(clean_fuels  \n    .glimpse())\n```\n\nNow, here's a question for you what happens if you try to use glimpse() in a method chain? \n\n:::{.question}\nCan you chain the operation `head()` after calling `glimpse()`? What do you think the output will be?\n:::\n\nThe answer is: no, you cannot. glimpse(), does not return you a data frame. Instead, the output of `glimpse()` is the text printout meant solely for viewing. No further operations can be applied to it. If you attempt to chain additional methods, you’ll encounter an error. Give it a try if you want! Polars will throw an error saying that the `head()` method cannot be applied to a NoneType, which is the type of output `glimpse()` returns.\n\nBy understanding how to use `head()`, `tail()`, `describe()`, and `glimpse()`, you have powerful tools at your disposal to explore and familiarize yourself with any dataset before diving deeper into your analysis.\n\n## select/drop\n\nOne of the most common tasks in data analysis is selecting specific variables or columns from a dataset. Let’s start by pulling out the country information from the `clean_fuels` dataset. Pause the video for a moment and try running this code:\n\n```{python}\n(clean_fuels  \n    .select(pl.col(\"country\")))\n```\n\nHere, we’re using the `select()` method to isolate a column. Notice how the column name is wrapped in the `pl.col()` function. This wrapper explicitly tells Polars that we’re referring to a column in the dataframe.\n\nBut here’s something cool—you can skip the pl.col() wrapper in certain cases! For example, this code:\n\n```{python}\n(clean_fuels  \n    .select(\"country_code\", \"country\"))  \n```\n\n...does the exact same thing as this:\n\n```{python}\n(clean_fuels  \n    .select(pl.col(\"country_code\"), pl.col(\"country\")))  \n```\n\nPretty neat, right? The select() method can directly interpret strings as column names, making your code a little cleaner and quicker to write.\n\nWhen you wrap a column name in pl.col(), you’re creating an expression. An expression is like an instruction—it doesn’t do anything on its own. For example, if you run this code:\n\n```{python}\npl.col(\"country_code\")\n```\n\n...nothing happens. It just returns something called an \"unevaluated expression\". But when you evaluate that expression in the context of a dataset, it turns into something powerful. For instance:\n\n```{python}\n(clean_fuels  \n    .select(pl.col(\"country_code\")))  \n```\n\nHere, the `select()` method acts as an evaluation environment, turning the `pl.col()` expression into actual data. `select()` is one of the several methods in Polars that can evaluate expressions. While `select()` is highly versatile and can do other things as well, for now, we’ll focus on its simplest use case: extracting columns from a data frame.\n\nThe pl.col() wrapper is super flexible, and it’s going to be central as we build more advanced expressions in Polars. For instance, you can use pl.col() to refer to multiple columns simultaneously:\n```{python}\n(clean_fuels  \n    .select(pl.col(\"country_code\", \"country\")))  \n```\n\nSometimes, typing out long column names can feel like a chore, especially when you’re working with many columns. But don’t worry—Polars makes it easy to select columns by their position in the dataset. For example, this code selects the second and third columns by their numerical index:\n\n```{python}\n(clean_fuels  \n    .select(pl.nth(1,2)))  \n```\n\nWatch out column indices in Polars are 0-based. That means the first column is index 0, the second column is index 1, and so on. \n\nWhat about negative numbers? They’re a handy shortcut for selecting columns from the end of the dataset. For instance, -1 refers to the last column, and this code will select the first and last columns:\n\n```{python}\n(clean_fuels  \n    .select(pl.nth(0, -1)))  \n```\n\nA note of caution: selecting columns by the order of their occurence can be risky. If your dataset’s structure changes, you might accidentally select the wrong columns. So, use the nth() function sparingly.\n\nNow, let’s talk about the opposite of select()—the drop() method. The drop() method removes specific columns from your dataset, leaving everything else intact. For example:\n\n```{python}\n(clean_fuels  \n    .drop(pl.col(\"country\"), pl.col(\"region\")))  \n```\n\nDopping is equivalent to selecting **all columns except** the ones you want to exclude. Here’s how could would write it using in terms of selection:\n\n```{python}\n(clean_fuels  \n    .select(pl.all().exclude(\"country\", \"region\")))  \n```\n\nThe pl.all() function refers to all columns, and the exclude() method lets you refine the selection by removing specific ones.The pl.all() function refers to all columns, and the exclude() method lets you refine the selection by removing specific ones.\n\nA quick reminder—dropping columns doesn’t modify your original dataset. It only affects the result of that query. Unless you explicitly overwrite the original dataframe, everything stays the same. So feel free to experiment!\n\nNow it’s your turn. Select the columns related to population and the proportion of the population with access to clean fuels. Try using both selection by name and by index, as well as dropping the ones you dont need.\n\n:::{.challenge}\nSelect the columns related to population and the proportion of population with access to clean fuels from the `clean_fuels` dataset.\n:::\n\nPause the video and try couple of different ways of selecting these columns. \n\n```{python}\n(clean_fuels  \n    .select(pl.col(\"pop_clean_fuels_cooking_mln\"), pl.col(\"prop_clean_fuels_cooking_pct\"))) \n\n(clean_fuels  \n    .select(pl.nth(-2,-1))) \n\n(clean_fuels  \n    .drop(\"region\", \"country_code\", \"country\", \"year\"))\n```\n\nGot it? Great! Both approaches—selecting specific columns or dropping the ones you don’t need—give you the same result. Expressions like these make your analysis more dynamic and efficient, so you can quickly adapt to different datasets or scenarios.\n\n## selectors\n\nSelecting columns is such a common task that Polars has a dedicated module for it—polars.selectors. This module provides a collection of methods specifically designed to simplify column selection. These are often aliased as cs for convenience. Have a look at the `polars` [documentation for selectors](https://docs.pola.rs/api/python/stable/reference/selectors.html). To get started, make sure you import the selectors module:\n\n```{python}\nimport polars.selectors as cs\n```\n\nAmong the most useful selectors are, of course, selectors by name and column index (for which we might not really need selectors, because those can be picked up with `pl.col()` and `pl.nth()`). \n\n```{python}\n\n\n(clean_fuels\n    .select(cs.by_name(\"region\", \"country\"))\n)\n\n# note python is 0-based\n(clean_fuels\n    .select(cs.by_index(0,2,5))\n)\n```\n\nSelecting first and last columns are so common, there are useful shortthands `cs.first()` and `cs.last()`. To select all columns other than the one you specified, you can use the tilde ~ operator. Tilde operator works with all methods in cs. module and negates the selection. For example `~cs.last()` refers to all columns other than the last one.\n\nSelectors can target columns based on their data types! For example, cs.numeric() picks all numeric columns. And if you want non-numeric columns, just negate it with ~.\n\nAnd now it is your turn. Try selecting first, everyhing other than the first, as well as all non-numeric columns. Use selector class for this. Pause the video and give it a try!\n\n:::{.challenge}\nUse `polars.selectors` aliased as `cs` to select\n- first column\n- everyhing other than first column\n- all non-numeric columns\n:::\n\n```{python}\n\n(clean_fuels\n    .select(cs.first())\n)\n\n# not first\n(clean_fuels\n    .select(~cs.first())\n)\n\n\n# not numeric\n(clean_fuels\n    .select(~cs.numeric())\n)\n\n```\n\nFantastic work! With a wide menu of selector methods, plus column and index-based expressions like pl.col() and pl.nth(), Polars gives you incredible flexibility in working with your data. These tools will become invaluable as we move into crafting more complex expressions.\n\nStay tuned—there’s a lot more to explore!\n\n## Filter\n\nNow let’s talk about filtering data—an essential part of data analysis. In Polars, filtering allows you to subset your dataset based on logical conditions, and it’s powered by the magic of expressions. Logical operations are one of the simplest and most common use cases for expressions. For example, you can compare every value in the region column to the string \"Europe\". If there’s a match, Polars returns True; otherwise, it returns False.\n\nLet’s see how this works in code:\n\n```{python}\n(clean_fuels\n    .filter(pl.col(\"region\")==\"Europe\")\n)\n```\n\nHere, the filter() method applies the logical condition, and only rows where the region is \"Europe\" are included in the result. Notice that for exact comparisons, we use the double equals sign `==`. Similarly, for inequalities, we can use operators like `<=`, `>=`, `<`, or `>`. Not equal is spelled out as `!=`.\n\nBut filtering doesn’t stop there—you can combine multiple conditions to create more complex filters. \n\nHere's a challenge for you. Can you find all the countries in Europe in the year 2022 where the majority of the population lacks access to clean fuels for cooking? Take a moment to write this expression. Pause the video if you need to.\n\n:::{.challenge}\nWere there any countries in Europe in the year 2022 where the majority of people lack access to clean fuel for cooking.\n:::\n\n```{python}\n\n(clean_fuels\n    .filter(\n        pl.col(\"region\")==\"Europe\",\n        pl.col(\"year\")==2022,\n        pl.col(\"prop_clean_fuels_cooking_pct\")<50\n        )\n)\n```\n\nWhat did you get? Oh, wow! Over half the population of Bosnia still lacks access to clean fuels for cooking. That’s a powerful insight.\n\nNow let’s zoom in on Bosnia to better understand its data. Bosnia’s country code is \"BIH\", but you can also filter by country name if you prefer.\n\n```{python}\n(\nclean_fuels\n    .filter(pl.col(\"country_code\")==\"BIH\")\n)\n```\n\nWe are interested in tracking how the proportion of the population with access to clean fuels for cooking has changed over the years. To do this, we’ll place the year on the x-axis and the population proportion on the y-axis.\n\nIf you remember from the Plotnine module, the dataset goes into the first argument of the ggplot function.\n\nHere’s one way to do this:\n\n```{python}\n(\nggplot(\n    clean_fuels\n        .filter(pl.col(\"country_code\")==\"BIH\")\n    ) +\n    geom_line(mapping=aes(x=\"year\", y=\"prop_clean_fuels_cooking_pct\"))\n)\n```\n\nThis works, but the code feels cluttered. It’s not immediately clear where the dataset comes from.\n\nLet’s clean this up using the .pipe() method.\n\n```{python}\n(clean_fuels\n    .filter(pl.col(\"country_code\")==\"BIH\")\n    .pipe(ggplot) +\n    geom_line(aes(x=\"year\", y=\"pop_clean_fuels_cooking_mln\"))\n)\n```\n\nHere, the .pipe() method passes the filtered clean_fuels dataset into the ggplot function as its first argument. This keeps the code clean and modular. Everything after .pipe(ggplot) is Plotnine-specific code.\n\nNice!\n\nBut what if you’re not sure how a country’s name is spelled in the dataset? For example, is it “Czech Republic” or just “Czechia”?\n\nIn this case, you can use partial string matching to find it.\n\n```{python}\n(clean_fuels\n    .filter(pl.col(\"country\").str.starts_with(\"Cz\")))\n```\n\nHere, we use the str.starts_with() method, which checks if strings in the country column start with the letters “Cz.” Ah, there it is—“Czechia”! Polars offers several handy string operations, like:\n\n- str.starts_with()\n- str.ends_with()\n- str.contains()\n\nYou’ll see more of these as we progress, but these three are powerful enough to help you tackle the following challenge.\n\nFilter the data for your country and visualize the proportion of people with access to clean fuels. Once you’re happy with your data subset, use ggplot and everything you’ve learned about Plotnine to create a polished visualization.\n\n:::{.challenge}\nVisualize the proportion of people with access to clean fuels in your country\n:::\n\n```{python}\n(clean_fuels\n    .filter(pl.col(\"country\").str.starts_with(\"Ukr\"))\n    .pipe(ggplot)\n    + geom_line(aes(x=\"year\", y=\"prop_clean_fuels_cooking_pct\"))\n)\n```\n\nThis looks fantastic! Great work visualizing your country’s data.\n\nIn the next section, we’ll explore adding more columns to our dataset and practice advanced subsetting and visualization techniques. Stay tuned!\n\nLet’s apply what we’ve learned about filtering to visualize the causes of death in some European countries.\n\n```{python}\nhhap_deaths\n```\n\nThis dataset contains both summarized and detailed breakdowns of deaths for every country and year. Take a look at the column labeled cause_of_death. When this column says \"All causes,\" it represents the total deaths for that country and year—a sum of all the other rows.\n\nLet’s zoom in on Bosnia for a single year, say 2010, to understand this better.\n\n```{python}\n(hhap_deaths\n    .filter(pl.col(\"country_code\")==\"BIH\", \n            pl.col(\"year\")==2010)\n    )\n```\n\nOne of the rows is labeled \"All causes\" with 4,816 deaths. This total matches the sum of the individual causes of death. While it’s useful to have the total, it can lead to double counting if we include it in our analysis. \n\nNow let’s expand our view to include all European countries for which we have death data. We’ll exclude the totals and focus on trends for each specific cause of death. Faceting will help us visualize these trends country by country.\n\n```{python}\n\n(hhap_deaths\n    .filter(pl.col(\"region\")==\"Europe\",\n            pl.col(\"deaths\")>0,\n            pl.col(\"cause_of_death\")!=\"All causes\")\n    .pipe(ggplot)+\n    aes(x=\"year\", y=\"deaths\", color=\"cause_of_death\", group=\"cause_of_death\")+\n    geom_smooth(method=\"lm\")+\n    facet_wrap(\"country\", scales=\"free_y\", nrow=2)+\n    theme(figure_size=(20,10), legend_position=\"bottom\")\n    )\n\n```\n\nMost of the trends appear to be decreasing, which is good news. However, even with free y-axis scales for each country, the differences in scale make it hard to compare trends across countries. Look at Moldova! There’s a dramatic improvement in death cases here. Meanwhile, heart- and stroke-related deaths in neighboring Russia are on the rise.\n\nCan we calculate population numbers from our `clean_fuels` dataset?\n\n# mutating(with_columns)\n\nRemember, the `clean_fuels` dataset tells us both the number of people with access to clean fuels and the proportion of the total population they represent, expressed as a percentage. With this, we can reverse-engineer the total population.\n\nTo create new columns, we use the with_columns() method. This method lets us add or modify columns by specifying expressions.\n\nLet’s start by converting the percentage of people with access to clean fuels into a true proportion by dividing it by 100.\n\n```{python}\n(clean_fuels\n    .with_columns(prop=pl.col(\"prop_clean_fuels_cooking_pct\")/100)\n)\n```\n\nHere, we create a new column called prop. The expression starts with pl.col(\"prop_clean_fuels_cooking_pct\"), and then we specify the operation: dividing by 100. Easy enough, right?\n\nNow that we have the true proportion, we can calculate the total population. We’ll divide the population with access to clean fuels by this proportion. \n\n```{python}\nclean_fuels_pop_df=(clean_fuels\n    .with_columns(prop=pl.col(\"prop_clean_fuels_cooking_pct\")/100)\n    .with_columns((pl.col(\"pop_clean_fuels_cooking_mln\")/pl.col(\"prop\")).alias(\"population\"))\n)\n```\n\nNotice that I wrapped the division operation in parentheses to ensure it’s evaluated correctly. I also used the alias() method to specify a name for the new column: population.This code calculates and adds two new columns:\n\n- prop: the true proportion of the population with access to clean fuels.\n- population: the estimated total population for each row.\n\nThis looks good. Lets assign it to a variable and inspect the updated dataset.\n\n```{python}\nclean_fuels_pop_df.describe()\n```\n\nOh, look! We’ve got some NaN values in the population column. These appear because we divided by zero wherever the proportion was zero. Division by zero is, understandably, illegal in most places—and in Python, it results in NaN.\n\nNaN, or \"Not a Number,\" is a special marker for missing or undefined values. It propagates through calculations, which means any further operations on these rows will also result in NaN.\n\nThis is why the mean and standard deviation of the population column are also NaN in the summary.\n\nLet's see how many rows in our dataset contain illegal population estimates. We can use the .is_nan() method, which evaluates whether a column contains NaN values.\n\n```{python}\n(clean_fuels_pop_df\n    .filter(pl.col(\"population\").is_nan()))\n```\n\nUh-oh! 35 rows! Perhaps `fuel_types` could be a better source of population data?\n\nThe `fuel_types` dataset contains both the proportion of people using a specific cooking fuel and the absolute number of users. Since it includes multiple estimates for each country and year—one for each fuel type—it might give us more opportunities to calculate valid population estimates.\n\nGo ahead and add a population column to the `fuel_types` dataset. Save the extended data frame under a new name - we will need it later.\n\n:::{.challenge}\nAdd a population estimate to a `fuel_types` dataset and save it under a new name\n:::\n\n```{python}\nfuel_types_pop_df = (fuel_types\n    .with_columns(population=pl.col(\"pop_cooking_mln\")/pl.col(\"prop_cooking_pct\")*100)\n)\n```\n\nThis looks promising! Let's have a look at our favorite Bosnia\n\n```{python}\n(fuel_types_pop_df\n    .filter(pl.col(\"country\").str.starts_with(\"Bosni\"))\n)\n```\n\nInteresting! We still get some `NaN` values, for example, look here: in 2022 no one was using Kerosene to cook food. Thanks goodness! But now get several estimates of Bosnia’s population — 3.0, 3.5, 3.53, 3.48 million. These slight differences arise from rounding imprecisions in the proportions or the number of people using specific fuel types. While these variations are minor, they make it tricky to work with the data directly.\n\nWouldn’t it be nice to level out these immaterial differences, by say, averaging? In the next section, we’ll learn how to do just that: grouping and aggregation. Stay tuned!\n\n# summarizing (group_by, agg)\n\nOne of the most important operations data analysts do is producing various summaries of the data. Often we are interested in some sort of summary of the data by group. In our newly produced dataset, we could interested in total population for all countries in each of the regions. In polar we do it in two steps: first specify the groups using group_by() and then aggregate the data using .agg().\n\n\n```{python}\n(fuel_types_pop_df\n    .group_by(\"region\",\"year\", \"country\")\n    .agg(pl.col(\"population\").mean())\n)\n```\n\nThis is nice, but population in some years seem to show as NaN. This is because of division by zero in the previous section. We can drop these values before summarizing with `.drop_nans()`. Alternatively, we could drop the zero-valued records (`drop_nulls()`) in the \"proportion of population with access to clean fuels\", which is used in the denominator of our `population` column\n\nLets group by year and region and repeat (removing the missing values). and plot the result\n\n```{python}\n(fuel_types_pop_df\n    .group_by(\"region\", \"year\", \"country\")\n    .agg(pl.col(\"population\").drop_nans().mean()) \n    .group_by(\"region\", \"year\")\n    .agg(pl.col(\"population\").mean())\n    .pipe(ggplot)\n    +geom_line(aes(x=\"year\", y=\"population\", color=\"region\"))\n)\n```\n\nAs expected, we observe rapid grown in population of Africa and South-East Asia, while European population growth has stagnated.\n\nExpression can be started from one of the summary functions, with the column name as an argument. Compare two ways of expressing the same thing:\n\n```{python}\n(fuel_types_pop_df\n    .group_by(\"region\", \"year\", \"country\")\n    .agg(pl.col(\"population\").drop_nans().mean().alias(\"mean_pop_fuel_types\")) )\n\n(fuel_types_pop_df\n    .group_by(\"region\", \"year\", \"country\")\n    .agg(pl.mean(\"population\").alias(\"mean_pop_fuel_types\")) )\n\n```\n\nSometimes you dont want to summarize, but rather perform operation per group. Lets start by plotting total population per region for each fuel type over time. We want to group the data by region, year and fuel type and sum over countries. We can use faceted lineplot to see what's going on in each continent clearly. The range of values is quite large. We can use log scale to deal with this.\n\n\n```{python}\n(\n    fuel_types\n    .group_by(\"fuel_type\", \"year\", \"region\")\n    .agg(pl.col(\"pop_cooking_mln\").sum())\n    .pipe(ggplot)\n    +geom_line(aes(x=\"year\", y=\"pop_cooking_mln\", color=\"fuel_type\", group=\"fuel_type\"))\n    +facet_wrap(\"region\", ncol=3)\n    +scale_y_log10()\n)\n```\n\nThis looks good. We have quite different stories per continent: transitioning away from coal and charcoal in Europe and Americas. Rising importance of electricity and gas across the flobe and quite important role of biomass in Africa and South-East Asia.\n\nHowever because the population of these subgroups is so different it's hard to see the energy mix in every continent clearly. Can we represent the share of population relying on each type of fuel per continent?\n\nWhat we need to do is divide these numbers by totals across all fuel types. We start with the code we have so far and add one more variable: proportion of population. Here we take the summed up `pop_cooking_mln` (which after aggregation contains total per region, year and fuel type) and divide it by the sum of the values across fuel type. This is not aggregation, because we dont collapse the number of observations. We just want calulate sums over certain subgroups. In polars this is expressed with the method .over(), which takes the names of the grouping variables over which expression shall be computed. Think of this as adding variables per group. We write out our expression `pop_cooking_mln`) divided by the sum of the same column, but the sum should be calcualated not for all dataset, but rather per year and region, effectively summing across the fuel type, which is the remaining variable not included in the over() statement.\n\n\n```{python}\n(\n    fuel_types\n    .group_by(\"fuel_type\", \"year\", \"region\")\n    .agg(pl.col(\"pop_cooking_mln\").sum())\n    .with_columns(prop_pop=(pl.col(\"pop_cooking_mln\")/pl.col(\"pop_cooking_mln\").sum())\n                            .over(\"year\", \"region\"))\n    .pipe(ggplot)\n    +geom_area(aes(x=\"year\", y=\"prop_pop\", fill=\"fuel_type\"))\n    +facet_wrap(\"region\", ncol=3)\n    +scale_fill_cmap_d()\n    +theme_linedraw()\n    +labs(title=\"Energy mix per world region\",\n        subtitle=\"From biomass to natural gas\",\n        y=\"Share of population\", x=\"Year\", fill=\"Fuel Type\")\n)\n```\n\n\n\n```{python}\n#| eval: false\n\n#\n# When the categories are known up front use Enum. When you don't know the categories or they are not fixed then you use Categorical\n\n# fill_null(strategy=)\n# fill_nan()\n\n```\n\n","srcMarkdownNoYaml":"\n\n## Data\n\nWelcome to the module on data wrangling! In the last lesson, we explored a small, but exciting gapminder dataset, and created quite a few visualizations with it. But in real-world scenarios, datasets are often much larger. This brings new challenges, like focusing on specific subsets of data—perhaps observations from a single time period or a selection of variables related to a particular phenomenon.\n\nIn this module, we’ll learn how to subset data and create meaningful summaries that provide a high-level overview of trends or differences between groups. Summarized data is often presented in tables, so we’ll also introduce a package for creating clear, professional-looking tables.\n\nMost importantly, we’ll dive into the blazingly fast Polars package for data manipulation. As I mentioned earlier, Polars is powered by Rust, a high-performance programming language. Its core functionality is exposed to Python but can also be accessed from other languages. This means the data wrangling skills you gain here will be transferable beyond Python.\n\nBut first, let’s load the necessary packages for this module. In Python, it’s common to use the alias `pl` for Polars. We’ll also use a submodule called polars.selectors, aliasing it as cs — don’t worry, we’ll cover selectors in more detail soon. We’ll also import everything from Plotnine for data visualization and bring in the main function from the `great_tables` package for generating beautifully looking tables.\n\nHere’s the setup code. Place it in its own cell in your notebook and run it:\n```{python}\n#| label: setup\nimport polars as pl\nimport polars.selectors as cs\nfrom plotnine import *\nfrom great_tables import GT\n```\n\nNow, let’s explore our dataset. We’ll be looking at the WHO data about household pollution curated by the Our World In Data website.\n\nHousehold air pollution is primarily caused by burning polluting fuels like wood, animal dung, charcoal, agricultural waste, and kerosene in open fires or inefficient stoves. Globally, 2.1 billion people rely on these fuels for cooking, heating, and lighting. The poor combustion of these fuels leads to numerous health issues, such as pneumonia in children, and chronic diseases like obstructive pulmonary disease, lung cancer, stroke, and cardiovascular problems in adults.\n\nThis is a complex problem, and like many complex problems, it can be examined from different perspectives. We have three datasets to work with:\n\n- Causes of death linked to household pollution\n- Types of fuel used for cooking in various countries\n- Proportion of the population with access to clean cooking fuels\n\nEach dataset offers a unique angle on this issue. Let’s dive in and start exploring!\n\n# Part I. Basics\n\nIn this section, we’ll start working with the datasets about household air pollution. These datasets are stored as comma-separated values, or CSV files. CSV is a simple text format often used for storing tabular data. Think of it as a stripped-down version of Excel—just the data, no formatting or formulas. In fact, you can even open CSV files directly in Excel if you want to take a look at them.\n\nWe’ve prepared these datasets for you and stored them in the course repository on GitHub. While Polars allows us to read files directly from remote locations, you can also download the files and load them from your local project directory.\n\nHere’s the code to load the three datasets we’ll use:\n\n# Indoor air pollution datasets\n\n```{python}\nhhap_deaths = pl.read_csv(\"hhap/hhap_deaths.csv\")\nfuel_types = pl.read_csv(\"hhap/cooking_by_fuel_type.csv\")\nclean_fuels = pl.read_csv(\"hhap/clean_fuels_cooking.csv\")\n```\n\nLet’s break this down. We’re using the `read_csv()` function from Polars to load the data. This function takes a single mandatory argument: the file path, written as a string in quotes. The equal sign indicates that we assign the content of the file to the variable, listed on the left. The `pl.read_csv()` returns a data frame. So, from now on, we can simply refer to this variable name whenever we need to access the dataframe which we read fromm the CSV file without needing to re-import it.\n\n## Data Overview\n\nTo understand the data we’re working with, it’s helpful to preview it in a few ways. For instance, simply typing the name of a dataset—like clean_fuels—will display a preview of the first five and last five rows of the corresponding data frame.\n\nRows in a data frame are often referred to as observations or records, while columns are known as variables or features. If you want to see more rows than the default preview, you can use the .head() method and specify the number of rows to display:\n```{python}\n(clean_fuels  \n    .head(10))\n```\n\nYou may have noticed the parentheses around the code block. This lets you write the code across multiple lines without worrying about indentation.\n\nPreviewing the first few rows gives you an initial sense of the dataset’s structure and content. If you’re curious about the last few rows, there’s also a .tail() method you can use in a similar way.\n\nFor a broader overview of your data, you can use the .describe() method:\n\n```{python}\n(clean_fuels  \n    .describe())\n```\n\nThe .describe() method provides a statistical summary of numerical columns, including metrics like the mean, standard deviation, minimum, maximum, and various quantiles. It’s especially useful for large datasets when you want to quickly understand key metrics.\n\nOne thing to note: if a column contains missing values, Polars will display these as null. Polars treats missing values as “contagious,” so any operation involving them will also result in missing values in the output. This behavior applies to all statistical operations. We’ll see more examples of this later.\n\nBoth .head() and .describe() return a data frame, which means you can chain these operations together. Method chaining is a powerful coding style that helps you write clean, readable, and maintainable code. \n\nHere's the first challenge for you: combine the functions you learned so far to create a method chain:\n\n:::{.challenge}\n- Take first 25 records of clean_fuels data frame and calculate summary statistical summary\n- Compute statistical summary of the whole data and then present only quantile summaries of each column. The quantiles include minimum, maximum as well as the 25th, 50th and 75th quantile.\n:::\n\n```{python}\n(clean_fuels\n    .head(25)\n    .describe())\n\n(clean_fuels\n    .describe()\n    .tail(5))\n```\n\nSometimes, datasets have many columns, making it difficult to gain a full overview using methods like head() or describe(). For these cases, Polars provides a particularly useful method called glimpse().\n\nWhen you use glimpse(), the dataset’s structure is displayed horizontally. Each variable is listed as a row, making it easier to scan through all columns, even if you’re working with a limited screen space. Here's how it looks in action:\n\n```{python}\n(clean_fuels  \n    .glimpse())\n```\n\nNow, here's a question for you what happens if you try to use glimpse() in a method chain? \n\n:::{.question}\nCan you chain the operation `head()` after calling `glimpse()`? What do you think the output will be?\n:::\n\nThe answer is: no, you cannot. glimpse(), does not return you a data frame. Instead, the output of `glimpse()` is the text printout meant solely for viewing. No further operations can be applied to it. If you attempt to chain additional methods, you’ll encounter an error. Give it a try if you want! Polars will throw an error saying that the `head()` method cannot be applied to a NoneType, which is the type of output `glimpse()` returns.\n\nBy understanding how to use `head()`, `tail()`, `describe()`, and `glimpse()`, you have powerful tools at your disposal to explore and familiarize yourself with any dataset before diving deeper into your analysis.\n\n## select/drop\n\nOne of the most common tasks in data analysis is selecting specific variables or columns from a dataset. Let’s start by pulling out the country information from the `clean_fuels` dataset. Pause the video for a moment and try running this code:\n\n```{python}\n(clean_fuels  \n    .select(pl.col(\"country\")))\n```\n\nHere, we’re using the `select()` method to isolate a column. Notice how the column name is wrapped in the `pl.col()` function. This wrapper explicitly tells Polars that we’re referring to a column in the dataframe.\n\nBut here’s something cool—you can skip the pl.col() wrapper in certain cases! For example, this code:\n\n```{python}\n(clean_fuels  \n    .select(\"country_code\", \"country\"))  \n```\n\n...does the exact same thing as this:\n\n```{python}\n(clean_fuels  \n    .select(pl.col(\"country_code\"), pl.col(\"country\")))  \n```\n\nPretty neat, right? The select() method can directly interpret strings as column names, making your code a little cleaner and quicker to write.\n\nWhen you wrap a column name in pl.col(), you’re creating an expression. An expression is like an instruction—it doesn’t do anything on its own. For example, if you run this code:\n\n```{python}\npl.col(\"country_code\")\n```\n\n...nothing happens. It just returns something called an \"unevaluated expression\". But when you evaluate that expression in the context of a dataset, it turns into something powerful. For instance:\n\n```{python}\n(clean_fuels  \n    .select(pl.col(\"country_code\")))  \n```\n\nHere, the `select()` method acts as an evaluation environment, turning the `pl.col()` expression into actual data. `select()` is one of the several methods in Polars that can evaluate expressions. While `select()` is highly versatile and can do other things as well, for now, we’ll focus on its simplest use case: extracting columns from a data frame.\n\nThe pl.col() wrapper is super flexible, and it’s going to be central as we build more advanced expressions in Polars. For instance, you can use pl.col() to refer to multiple columns simultaneously:\n```{python}\n(clean_fuels  \n    .select(pl.col(\"country_code\", \"country\")))  \n```\n\nSometimes, typing out long column names can feel like a chore, especially when you’re working with many columns. But don’t worry—Polars makes it easy to select columns by their position in the dataset. For example, this code selects the second and third columns by their numerical index:\n\n```{python}\n(clean_fuels  \n    .select(pl.nth(1,2)))  \n```\n\nWatch out column indices in Polars are 0-based. That means the first column is index 0, the second column is index 1, and so on. \n\nWhat about negative numbers? They’re a handy shortcut for selecting columns from the end of the dataset. For instance, -1 refers to the last column, and this code will select the first and last columns:\n\n```{python}\n(clean_fuels  \n    .select(pl.nth(0, -1)))  \n```\n\nA note of caution: selecting columns by the order of their occurence can be risky. If your dataset’s structure changes, you might accidentally select the wrong columns. So, use the nth() function sparingly.\n\nNow, let’s talk about the opposite of select()—the drop() method. The drop() method removes specific columns from your dataset, leaving everything else intact. For example:\n\n```{python}\n(clean_fuels  \n    .drop(pl.col(\"country\"), pl.col(\"region\")))  \n```\n\nDopping is equivalent to selecting **all columns except** the ones you want to exclude. Here’s how could would write it using in terms of selection:\n\n```{python}\n(clean_fuels  \n    .select(pl.all().exclude(\"country\", \"region\")))  \n```\n\nThe pl.all() function refers to all columns, and the exclude() method lets you refine the selection by removing specific ones.The pl.all() function refers to all columns, and the exclude() method lets you refine the selection by removing specific ones.\n\nA quick reminder—dropping columns doesn’t modify your original dataset. It only affects the result of that query. Unless you explicitly overwrite the original dataframe, everything stays the same. So feel free to experiment!\n\nNow it’s your turn. Select the columns related to population and the proportion of the population with access to clean fuels. Try using both selection by name and by index, as well as dropping the ones you dont need.\n\n:::{.challenge}\nSelect the columns related to population and the proportion of population with access to clean fuels from the `clean_fuels` dataset.\n:::\n\nPause the video and try couple of different ways of selecting these columns. \n\n```{python}\n(clean_fuels  \n    .select(pl.col(\"pop_clean_fuels_cooking_mln\"), pl.col(\"prop_clean_fuels_cooking_pct\"))) \n\n(clean_fuels  \n    .select(pl.nth(-2,-1))) \n\n(clean_fuels  \n    .drop(\"region\", \"country_code\", \"country\", \"year\"))\n```\n\nGot it? Great! Both approaches—selecting specific columns or dropping the ones you don’t need—give you the same result. Expressions like these make your analysis more dynamic and efficient, so you can quickly adapt to different datasets or scenarios.\n\n## selectors\n\nSelecting columns is such a common task that Polars has a dedicated module for it—polars.selectors. This module provides a collection of methods specifically designed to simplify column selection. These are often aliased as cs for convenience. Have a look at the `polars` [documentation for selectors](https://docs.pola.rs/api/python/stable/reference/selectors.html). To get started, make sure you import the selectors module:\n\n```{python}\nimport polars.selectors as cs\n```\n\nAmong the most useful selectors are, of course, selectors by name and column index (for which we might not really need selectors, because those can be picked up with `pl.col()` and `pl.nth()`). \n\n```{python}\n\n\n(clean_fuels\n    .select(cs.by_name(\"region\", \"country\"))\n)\n\n# note python is 0-based\n(clean_fuels\n    .select(cs.by_index(0,2,5))\n)\n```\n\nSelecting first and last columns are so common, there are useful shortthands `cs.first()` and `cs.last()`. To select all columns other than the one you specified, you can use the tilde ~ operator. Tilde operator works with all methods in cs. module and negates the selection. For example `~cs.last()` refers to all columns other than the last one.\n\nSelectors can target columns based on their data types! For example, cs.numeric() picks all numeric columns. And if you want non-numeric columns, just negate it with ~.\n\nAnd now it is your turn. Try selecting first, everyhing other than the first, as well as all non-numeric columns. Use selector class for this. Pause the video and give it a try!\n\n:::{.challenge}\nUse `polars.selectors` aliased as `cs` to select\n- first column\n- everyhing other than first column\n- all non-numeric columns\n:::\n\n```{python}\n\n(clean_fuels\n    .select(cs.first())\n)\n\n# not first\n(clean_fuels\n    .select(~cs.first())\n)\n\n\n# not numeric\n(clean_fuels\n    .select(~cs.numeric())\n)\n\n```\n\nFantastic work! With a wide menu of selector methods, plus column and index-based expressions like pl.col() and pl.nth(), Polars gives you incredible flexibility in working with your data. These tools will become invaluable as we move into crafting more complex expressions.\n\nStay tuned—there’s a lot more to explore!\n\n## Filter\n\nNow let’s talk about filtering data—an essential part of data analysis. In Polars, filtering allows you to subset your dataset based on logical conditions, and it’s powered by the magic of expressions. Logical operations are one of the simplest and most common use cases for expressions. For example, you can compare every value in the region column to the string \"Europe\". If there’s a match, Polars returns True; otherwise, it returns False.\n\nLet’s see how this works in code:\n\n```{python}\n(clean_fuels\n    .filter(pl.col(\"region\")==\"Europe\")\n)\n```\n\nHere, the filter() method applies the logical condition, and only rows where the region is \"Europe\" are included in the result. Notice that for exact comparisons, we use the double equals sign `==`. Similarly, for inequalities, we can use operators like `<=`, `>=`, `<`, or `>`. Not equal is spelled out as `!=`.\n\nBut filtering doesn’t stop there—you can combine multiple conditions to create more complex filters. \n\nHere's a challenge for you. Can you find all the countries in Europe in the year 2022 where the majority of the population lacks access to clean fuels for cooking? Take a moment to write this expression. Pause the video if you need to.\n\n:::{.challenge}\nWere there any countries in Europe in the year 2022 where the majority of people lack access to clean fuel for cooking.\n:::\n\n```{python}\n\n(clean_fuels\n    .filter(\n        pl.col(\"region\")==\"Europe\",\n        pl.col(\"year\")==2022,\n        pl.col(\"prop_clean_fuels_cooking_pct\")<50\n        )\n)\n```\n\nWhat did you get? Oh, wow! Over half the population of Bosnia still lacks access to clean fuels for cooking. That’s a powerful insight.\n\nNow let’s zoom in on Bosnia to better understand its data. Bosnia’s country code is \"BIH\", but you can also filter by country name if you prefer.\n\n```{python}\n(\nclean_fuels\n    .filter(pl.col(\"country_code\")==\"BIH\")\n)\n```\n\nWe are interested in tracking how the proportion of the population with access to clean fuels for cooking has changed over the years. To do this, we’ll place the year on the x-axis and the population proportion on the y-axis.\n\nIf you remember from the Plotnine module, the dataset goes into the first argument of the ggplot function.\n\nHere’s one way to do this:\n\n```{python}\n(\nggplot(\n    clean_fuels\n        .filter(pl.col(\"country_code\")==\"BIH\")\n    ) +\n    geom_line(mapping=aes(x=\"year\", y=\"prop_clean_fuels_cooking_pct\"))\n)\n```\n\nThis works, but the code feels cluttered. It’s not immediately clear where the dataset comes from.\n\nLet’s clean this up using the .pipe() method.\n\n```{python}\n(clean_fuels\n    .filter(pl.col(\"country_code\")==\"BIH\")\n    .pipe(ggplot) +\n    geom_line(aes(x=\"year\", y=\"pop_clean_fuels_cooking_mln\"))\n)\n```\n\nHere, the .pipe() method passes the filtered clean_fuels dataset into the ggplot function as its first argument. This keeps the code clean and modular. Everything after .pipe(ggplot) is Plotnine-specific code.\n\nNice!\n\nBut what if you’re not sure how a country’s name is spelled in the dataset? For example, is it “Czech Republic” or just “Czechia”?\n\nIn this case, you can use partial string matching to find it.\n\n```{python}\n(clean_fuels\n    .filter(pl.col(\"country\").str.starts_with(\"Cz\")))\n```\n\nHere, we use the str.starts_with() method, which checks if strings in the country column start with the letters “Cz.” Ah, there it is—“Czechia”! Polars offers several handy string operations, like:\n\n- str.starts_with()\n- str.ends_with()\n- str.contains()\n\nYou’ll see more of these as we progress, but these three are powerful enough to help you tackle the following challenge.\n\nFilter the data for your country and visualize the proportion of people with access to clean fuels. Once you’re happy with your data subset, use ggplot and everything you’ve learned about Plotnine to create a polished visualization.\n\n:::{.challenge}\nVisualize the proportion of people with access to clean fuels in your country\n:::\n\n```{python}\n(clean_fuels\n    .filter(pl.col(\"country\").str.starts_with(\"Ukr\"))\n    .pipe(ggplot)\n    + geom_line(aes(x=\"year\", y=\"prop_clean_fuels_cooking_pct\"))\n)\n```\n\nThis looks fantastic! Great work visualizing your country’s data.\n\nIn the next section, we’ll explore adding more columns to our dataset and practice advanced subsetting and visualization techniques. Stay tuned!\n\nLet’s apply what we’ve learned about filtering to visualize the causes of death in some European countries.\n\n```{python}\nhhap_deaths\n```\n\nThis dataset contains both summarized and detailed breakdowns of deaths for every country and year. Take a look at the column labeled cause_of_death. When this column says \"All causes,\" it represents the total deaths for that country and year—a sum of all the other rows.\n\nLet’s zoom in on Bosnia for a single year, say 2010, to understand this better.\n\n```{python}\n(hhap_deaths\n    .filter(pl.col(\"country_code\")==\"BIH\", \n            pl.col(\"year\")==2010)\n    )\n```\n\nOne of the rows is labeled \"All causes\" with 4,816 deaths. This total matches the sum of the individual causes of death. While it’s useful to have the total, it can lead to double counting if we include it in our analysis. \n\nNow let’s expand our view to include all European countries for which we have death data. We’ll exclude the totals and focus on trends for each specific cause of death. Faceting will help us visualize these trends country by country.\n\n```{python}\n\n(hhap_deaths\n    .filter(pl.col(\"region\")==\"Europe\",\n            pl.col(\"deaths\")>0,\n            pl.col(\"cause_of_death\")!=\"All causes\")\n    .pipe(ggplot)+\n    aes(x=\"year\", y=\"deaths\", color=\"cause_of_death\", group=\"cause_of_death\")+\n    geom_smooth(method=\"lm\")+\n    facet_wrap(\"country\", scales=\"free_y\", nrow=2)+\n    theme(figure_size=(20,10), legend_position=\"bottom\")\n    )\n\n```\n\nMost of the trends appear to be decreasing, which is good news. However, even with free y-axis scales for each country, the differences in scale make it hard to compare trends across countries. Look at Moldova! There’s a dramatic improvement in death cases here. Meanwhile, heart- and stroke-related deaths in neighboring Russia are on the rise.\n\nCan we calculate population numbers from our `clean_fuels` dataset?\n\n# mutating(with_columns)\n\nRemember, the `clean_fuels` dataset tells us both the number of people with access to clean fuels and the proportion of the total population they represent, expressed as a percentage. With this, we can reverse-engineer the total population.\n\nTo create new columns, we use the with_columns() method. This method lets us add or modify columns by specifying expressions.\n\nLet’s start by converting the percentage of people with access to clean fuels into a true proportion by dividing it by 100.\n\n```{python}\n(clean_fuels\n    .with_columns(prop=pl.col(\"prop_clean_fuels_cooking_pct\")/100)\n)\n```\n\nHere, we create a new column called prop. The expression starts with pl.col(\"prop_clean_fuels_cooking_pct\"), and then we specify the operation: dividing by 100. Easy enough, right?\n\nNow that we have the true proportion, we can calculate the total population. We’ll divide the population with access to clean fuels by this proportion. \n\n```{python}\nclean_fuels_pop_df=(clean_fuels\n    .with_columns(prop=pl.col(\"prop_clean_fuels_cooking_pct\")/100)\n    .with_columns((pl.col(\"pop_clean_fuels_cooking_mln\")/pl.col(\"prop\")).alias(\"population\"))\n)\n```\n\nNotice that I wrapped the division operation in parentheses to ensure it’s evaluated correctly. I also used the alias() method to specify a name for the new column: population.This code calculates and adds two new columns:\n\n- prop: the true proportion of the population with access to clean fuels.\n- population: the estimated total population for each row.\n\nThis looks good. Lets assign it to a variable and inspect the updated dataset.\n\n```{python}\nclean_fuels_pop_df.describe()\n```\n\nOh, look! We’ve got some NaN values in the population column. These appear because we divided by zero wherever the proportion was zero. Division by zero is, understandably, illegal in most places—and in Python, it results in NaN.\n\nNaN, or \"Not a Number,\" is a special marker for missing or undefined values. It propagates through calculations, which means any further operations on these rows will also result in NaN.\n\nThis is why the mean and standard deviation of the population column are also NaN in the summary.\n\nLet's see how many rows in our dataset contain illegal population estimates. We can use the .is_nan() method, which evaluates whether a column contains NaN values.\n\n```{python}\n(clean_fuels_pop_df\n    .filter(pl.col(\"population\").is_nan()))\n```\n\nUh-oh! 35 rows! Perhaps `fuel_types` could be a better source of population data?\n\nThe `fuel_types` dataset contains both the proportion of people using a specific cooking fuel and the absolute number of users. Since it includes multiple estimates for each country and year—one for each fuel type—it might give us more opportunities to calculate valid population estimates.\n\nGo ahead and add a population column to the `fuel_types` dataset. Save the extended data frame under a new name - we will need it later.\n\n:::{.challenge}\nAdd a population estimate to a `fuel_types` dataset and save it under a new name\n:::\n\n```{python}\nfuel_types_pop_df = (fuel_types\n    .with_columns(population=pl.col(\"pop_cooking_mln\")/pl.col(\"prop_cooking_pct\")*100)\n)\n```\n\nThis looks promising! Let's have a look at our favorite Bosnia\n\n```{python}\n(fuel_types_pop_df\n    .filter(pl.col(\"country\").str.starts_with(\"Bosni\"))\n)\n```\n\nInteresting! We still get some `NaN` values, for example, look here: in 2022 no one was using Kerosene to cook food. Thanks goodness! But now get several estimates of Bosnia’s population — 3.0, 3.5, 3.53, 3.48 million. These slight differences arise from rounding imprecisions in the proportions or the number of people using specific fuel types. While these variations are minor, they make it tricky to work with the data directly.\n\nWouldn’t it be nice to level out these immaterial differences, by say, averaging? In the next section, we’ll learn how to do just that: grouping and aggregation. Stay tuned!\n\n# summarizing (group_by, agg)\n\nOne of the most important operations data analysts do is producing various summaries of the data. Often we are interested in some sort of summary of the data by group. In our newly produced dataset, we could interested in total population for all countries in each of the regions. In polar we do it in two steps: first specify the groups using group_by() and then aggregate the data using .agg().\n\n\n```{python}\n(fuel_types_pop_df\n    .group_by(\"region\",\"year\", \"country\")\n    .agg(pl.col(\"population\").mean())\n)\n```\n\nThis is nice, but population in some years seem to show as NaN. This is because of division by zero in the previous section. We can drop these values before summarizing with `.drop_nans()`. Alternatively, we could drop the zero-valued records (`drop_nulls()`) in the \"proportion of population with access to clean fuels\", which is used in the denominator of our `population` column\n\nLets group by year and region and repeat (removing the missing values). and plot the result\n\n```{python}\n(fuel_types_pop_df\n    .group_by(\"region\", \"year\", \"country\")\n    .agg(pl.col(\"population\").drop_nans().mean()) \n    .group_by(\"region\", \"year\")\n    .agg(pl.col(\"population\").mean())\n    .pipe(ggplot)\n    +geom_line(aes(x=\"year\", y=\"population\", color=\"region\"))\n)\n```\n\nAs expected, we observe rapid grown in population of Africa and South-East Asia, while European population growth has stagnated.\n\nExpression can be started from one of the summary functions, with the column name as an argument. Compare two ways of expressing the same thing:\n\n```{python}\n(fuel_types_pop_df\n    .group_by(\"region\", \"year\", \"country\")\n    .agg(pl.col(\"population\").drop_nans().mean().alias(\"mean_pop_fuel_types\")) )\n\n(fuel_types_pop_df\n    .group_by(\"region\", \"year\", \"country\")\n    .agg(pl.mean(\"population\").alias(\"mean_pop_fuel_types\")) )\n\n```\n\nSometimes you dont want to summarize, but rather perform operation per group. Lets start by plotting total population per region for each fuel type over time. We want to group the data by region, year and fuel type and sum over countries. We can use faceted lineplot to see what's going on in each continent clearly. The range of values is quite large. We can use log scale to deal with this.\n\n\n```{python}\n(\n    fuel_types\n    .group_by(\"fuel_type\", \"year\", \"region\")\n    .agg(pl.col(\"pop_cooking_mln\").sum())\n    .pipe(ggplot)\n    +geom_line(aes(x=\"year\", y=\"pop_cooking_mln\", color=\"fuel_type\", group=\"fuel_type\"))\n    +facet_wrap(\"region\", ncol=3)\n    +scale_y_log10()\n)\n```\n\nThis looks good. We have quite different stories per continent: transitioning away from coal and charcoal in Europe and Americas. Rising importance of electricity and gas across the flobe and quite important role of biomass in Africa and South-East Asia.\n\nHowever because the population of these subgroups is so different it's hard to see the energy mix in every continent clearly. Can we represent the share of population relying on each type of fuel per continent?\n\nWhat we need to do is divide these numbers by totals across all fuel types. We start with the code we have so far and add one more variable: proportion of population. Here we take the summed up `pop_cooking_mln` (which after aggregation contains total per region, year and fuel type) and divide it by the sum of the values across fuel type. This is not aggregation, because we dont collapse the number of observations. We just want calulate sums over certain subgroups. In polars this is expressed with the method .over(), which takes the names of the grouping variables over which expression shall be computed. Think of this as adding variables per group. We write out our expression `pop_cooking_mln`) divided by the sum of the same column, but the sum should be calcualated not for all dataset, but rather per year and region, effectively summing across the fuel type, which is the remaining variable not included in the over() statement.\n\n\n```{python}\n(\n    fuel_types\n    .group_by(\"fuel_type\", \"year\", \"region\")\n    .agg(pl.col(\"pop_cooking_mln\").sum())\n    .with_columns(prop_pop=(pl.col(\"pop_cooking_mln\")/pl.col(\"pop_cooking_mln\").sum())\n                            .over(\"year\", \"region\"))\n    .pipe(ggplot)\n    +geom_area(aes(x=\"year\", y=\"prop_pop\", fill=\"fuel_type\"))\n    +facet_wrap(\"region\", ncol=3)\n    +scale_fill_cmap_d()\n    +theme_linedraw()\n    +labs(title=\"Energy mix per world region\",\n        subtitle=\"From biomass to natural gas\",\n        y=\"Share of population\", x=\"Year\", fill=\"Fuel Type\")\n)\n```\n\n\n\n```{python}\n#| eval: false\n\n#\n# When the categories are known up front use Enum. When you don't know the categories or they are not fixed then you use Categorical\n\n# fill_null(strategy=)\n# fill_nan()\n\n```\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":true,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true,"brand":{"brand":{"meta":{"name":"Lund Univerisy brand","link":"https://www.medarbetarwebben.lu.se/grafiskprofil"},"color":{"palette":{"lu-blue":"#000080","lu-gold":"#9C6114","bg-pink":"#E9C4C7","bg-pink-50":"#E9C4C780","bg-blue":"#B9D3DC","bg-blue-50":"#B9D3DC80","bg-green":"#ADCAB8","bg-green-50":"#ADCAB880","bg-beige":"#D6D2C4","bg-beige-50":"#D6D2C480","bg-grey":"#BFB8AF","bg-grey-50":"#BFB8AF80"},"background":"#ffffff","foreground":"#4d4c44","primary":"lu-gold","secondary":"#4d4c44","tertiary":"bg-grey","light":"white"},"logo":{"medium":"LU_logo.png"},"typography":{"fonts":[{"family":"EB Garamond","source":"google"},{"family":"Roboto","source":"google"},{"family":"Fira Code","source":"google"}],"base":"Roboto","headings":{"family":"EB Garamond","color":"lu-gold"},"monospace":"Fira Code"},"defaults":{"bootstrap":{"enable-rounded":false,"navbar-fg":"lu-gold"}}},"data":{"meta":{"name":"Lund Univerisy brand","link":"https://www.medarbetarwebben.lu.se/grafiskprofil"},"color":{"palette":{"lu-blue":"#000080","lu-gold":"#9C6114","bg-pink":"#E9C4C7","bg-pink-50":"#E9C4C780","bg-blue":"#B9D3DC","bg-blue-50":"#B9D3DC80","bg-green":"#ADCAB8","bg-green-50":"#ADCAB880","bg-beige":"#D6D2C4","bg-beige-50":"#D6D2C480","bg-grey":"#BFB8AF","bg-grey-50":"#BFB8AF80"},"background":"#ffffff","foreground":"#4d4c44","primary":"lu-gold","secondary":"#4d4c44","tertiary":"bg-grey","light":"white"},"logo":{"medium":"LU_logo.png"},"typography":{"fonts":[{"family":"EB Garamond","source":"google"},{"family":"Roboto","source":"google"},{"family":"Fira Code","source":"google"}],"base":"Roboto","headings":{"family":"EB Garamond","color":"lu-gold"},"monospace":"Fira Code"},"defaults":{"bootstrap":{"enable-rounded":false,"navbar-fg":"lu-gold"}}},"brandDir":"/home/dm0737pe/Projects/DATA24WEB/brand","projectDir":"/home/dm0737pe/Projects/DATA24WEB","processedData":{"color":{"lu-blue":"#000080","lu-gold":"#9C6114","bg-pink":"#E9C4C7","bg-pink-50":"#E9C4C780","bg-blue":"#B9D3DC","bg-blue-50":"#B9D3DC80","bg-green":"#ADCAB8","bg-green-50":"#ADCAB880","bg-beige":"#D6D2C4","bg-beige-50":"#D6D2C480","bg-grey":"#BFB8AF","bg-grey-50":"#BFB8AF80","background":"#ffffff","foreground":"#4d4c44","primary":"#9C6114","secondary":"#4d4c44","tertiary":"#BFB8AF","light":"white"},"typography":{"base":"Roboto","headings":{"family":"EB Garamond","color":"lu-gold"},"monospace":{"family":"Fira Code"},"monospace-inline":{"family":"Fira Code"},"monospace-block":{"family":"Fira Code"}},"logo":{"images":{},"medium":{"light":{"path":"LU_logo.png"},"dark":{"path":"LU_logo.png"}}}}}},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["custom-callout"],"email-obfuscation":"javascript","toc":true,"output-file":"wrangling.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.37","brand":"brand/_brand.yml","custom-callout":{"challenge":{"icon-symbol":"🛠️","color":"firebrick"},"question":{"icon-symbol":"🧠","color":"pink"}},"page-layout":"full","grid":{"body-width":"1000px"},"theme":["zephyr","brand"],"anchor-sections":true,"title":"Wrangling"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}