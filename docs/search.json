[
  {
    "objectID": "wrangling.html",
    "href": "wrangling.html",
    "title": "Wrangling",
    "section": "",
    "text": "Welcome to the module on data wrangling! In the last lesson, we explored a small, but exciting gapminder dataset, and created quite a few visualizations with it. But in real-world scenarios, datasets are often much larger. This brings new challenges, like focusing on specific subsets of data—perhaps observations from a single time period or a selection of variables related to a particular phenomenon.\nIn this module, we’ll learn how to subset data and create meaningful summaries that provide a high-level overview of trends or differences between groups. Summarized data is often presented in tables, so we’ll also introduce a package for creating clear, professional-looking tables.\nMost importantly, we’ll dive into the blazingly fast Polars package for data manipulation. As I mentioned earlier, Polars is powered by Rust, a high-performance programming language. Its core functionality is exposed to Python but can also be accessed from other languages. This means the data wrangling skills you gain here will be transferable beyond Python.\nBut first, let’s load the necessary packages for this module. In Python, it’s common to use the alias pl for Polars. We’ll also use a submodule called polars.selectors, aliasing it as cs — don’t worry, we’ll cover selectors in more detail soon. We’ll also import everything from Plotnine for data visualization and bring in the main function from the great_tables package for generating beautifully looking tables.\nHere’s the setup code. Place it in its own cell in your notebook and run it:\n\nimport polars as pl\nimport polars.selectors as cs\nfrom plotnine import *\nfrom great_tables import GT\n\nNow, let’s explore our dataset. We’ll be looking at the WHO data about household pollution curated by the Our World In Data website.\nHousehold air pollution is primarily caused by burning polluting fuels like wood, animal dung, charcoal, agricultural waste, and kerosene in open fires or inefficient stoves. Globally, 2.1 billion people rely on these fuels for cooking, heating, and lighting. The poor combustion of these fuels leads to numerous health issues, such as pneumonia in children, and chronic diseases like obstructive pulmonary disease, lung cancer, stroke, and cardiovascular problems in adults.\nThis is a complex problem, and like many complex problems, it can be examined from different perspectives. We have three datasets to work with:\n\nCauses of death linked to household pollution\nTypes of fuel used for cooking in various countries\nProportion of the population with access to clean cooking fuels\n\nEach dataset offers a unique angle on this issue. Let’s dive in and start exploring!"
  },
  {
    "objectID": "wrangling.html#data",
    "href": "wrangling.html#data",
    "title": "Wrangling",
    "section": "",
    "text": "Welcome to the module on data wrangling! In the last lesson, we explored a small, but exciting gapminder dataset, and created quite a few visualizations with it. But in real-world scenarios, datasets are often much larger. This brings new challenges, like focusing on specific subsets of data—perhaps observations from a single time period or a selection of variables related to a particular phenomenon.\nIn this module, we’ll learn how to subset data and create meaningful summaries that provide a high-level overview of trends or differences between groups. Summarized data is often presented in tables, so we’ll also introduce a package for creating clear, professional-looking tables.\nMost importantly, we’ll dive into the blazingly fast Polars package for data manipulation. As I mentioned earlier, Polars is powered by Rust, a high-performance programming language. Its core functionality is exposed to Python but can also be accessed from other languages. This means the data wrangling skills you gain here will be transferable beyond Python.\nBut first, let’s load the necessary packages for this module. In Python, it’s common to use the alias pl for Polars. We’ll also use a submodule called polars.selectors, aliasing it as cs — don’t worry, we’ll cover selectors in more detail soon. We’ll also import everything from Plotnine for data visualization and bring in the main function from the great_tables package for generating beautifully looking tables.\nHere’s the setup code. Place it in its own cell in your notebook and run it:\n\nimport polars as pl\nimport polars.selectors as cs\nfrom plotnine import *\nfrom great_tables import GT\n\nNow, let’s explore our dataset. We’ll be looking at the WHO data about household pollution curated by the Our World In Data website.\nHousehold air pollution is primarily caused by burning polluting fuels like wood, animal dung, charcoal, agricultural waste, and kerosene in open fires or inefficient stoves. Globally, 2.1 billion people rely on these fuels for cooking, heating, and lighting. The poor combustion of these fuels leads to numerous health issues, such as pneumonia in children, and chronic diseases like obstructive pulmonary disease, lung cancer, stroke, and cardiovascular problems in adults.\nThis is a complex problem, and like many complex problems, it can be examined from different perspectives. We have three datasets to work with:\n\nCauses of death linked to household pollution\nTypes of fuel used for cooking in various countries\nProportion of the population with access to clean cooking fuels\n\nEach dataset offers a unique angle on this issue. Let’s dive in and start exploring!"
  },
  {
    "objectID": "wrangling.html#data-overview",
    "href": "wrangling.html#data-overview",
    "title": "Wrangling",
    "section": "Data Overview",
    "text": "Data Overview\nTo understand the data we’re working with, it’s helpful to preview it in a few ways. For instance, simply typing the name of a dataset—like clean_fuels—will display a preview of the first five and last five rows of the corresponding data frame.\nRows in a data frame are often referred to as observations or records, while columns are known as variables or features. If you want to see more rows than the default preview, you can use the .head() method and specify the number of rows to display:\n\n(clean_fuels  \n    .head(10))\n\n\nshape: (10, 6)\n\n\n\nregion\ncountry_code\ncountry\nyear\npop_clean_fuels_cooking_mln\nprop_clean_fuels_cooking_pct\n\n\nstr\nstr\nstr\ni64\nf64\nf64\n\n\n\n\n\"Africa\"\n\"SSD\"\n\"South Sudan\"\n2022\n0.0\n0.0\n\n\n\"Western Pacific\"\n\"NIU\"\n\"Niue\"\n2022\n0.002\n98.5\n\n\n\"Western Pacific\"\n\"TKL\"\n\"Tokelau\"\n2022\n0.0004\n28.3\n\n\n\"Western Pacific\"\n\"COK\"\n\"Cook Islands\"\n2022\n0.013\n72.7\n\n\n\"Western Pacific\"\n\"PLW\"\n\"Palau\"\n2022\n0.007\n29.45\n\n\n\"Africa\"\n\"STP\"\n\"Sao Tome and Principe\"\n2022\n0.009\n4.1\n\n\n\"Western Pacific\"\n\"FSM\"\n\"Micronesia (Federated States o…\n2022\n0.014\n13.2\n\n\n\"Africa\"\n\"BDI\"\n\"Burundi\"\n2022\n0.013\n0.1\n\n\n\"Western Pacific\"\n\"NRU\"\n\"Nauru\"\n2022\n0.011\n100.0\n\n\n\"Western Pacific\"\n\"TUV\"\n\"Tuvalu\"\n2022\n0.009\n75.2\n\n\n\n\n\n\nYou may have noticed the parentheses around the code block. This lets you write the code across multiple lines without worrying about indentation.\nPreviewing the first few rows gives you an initial sense of the dataset’s structure and content. If you’re curious about the last few rows, there’s also a .tail() method you can use in a similar way.\nFor a broader overview of your data, you can use the .describe() method:\n\n(clean_fuels  \n    .describe())\n\n\nshape: (9, 7)\n\n\n\nstatistic\nregion\ncountry_code\ncountry\nyear\npop_clean_fuels_cooking_mln\nprop_clean_fuels_cooking_pct\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\n\n\n\n\n\"count\"\n\"6402\"\n\"6402\"\n\"6402\"\n6402.0\n6402.0\n6402.0\n\n\n\"null_count\"\n\"0\"\n\"0\"\n\"0\"\n0.0\n0.0\n0.0\n\n\n\"mean\"\nnull\nnull\nnull\n2006.0\n19.29797\n61.080069\n\n\n\"std\"\nnull\nnull\nnull\n9.522648\n73.100935\n39.952764\n\n\n\"min\"\n\"Africa\"\n\"AFG\"\n\"Afghanistan\"\n1990.0\n0.0\n0.0\n\n\n\"25%\"\nnull\nnull\nnull\n1998.0\n0.23\n16.6\n\n\n\"50%\"\nnull\nnull\nnull\n2006.0\n2.24\n78.0\n\n\n\"75%\"\nnull\nnull\nnull\n2014.0\n10.24\n100.0\n\n\n\"max\"\n\"Western Pacific\"\n\"ZWE\"\n\"Zimbabwe\"\n2022.0\n1257.0\n100.0\n\n\n\n\n\n\nThe .describe() method provides a statistical summary of numerical columns, including metrics like the mean, standard deviation, minimum, maximum, and various quantiles. It’s especially useful for large datasets when you want to quickly understand key metrics.\nOne thing to note: if a column contains missing values, Polars will display these as null. Polars treats missing values as “contagious,” so any operation involving them will also result in missing values in the output. This behavior applies to all statistical operations. We’ll see more examples of this later.\nBoth .head() and .describe() return a data frame, which means you can chain these operations together. Method chaining is a powerful coding style that helps you write clean, readable, and maintainable code.\nHere’s the first challenge for you: combine the functions you learned so far to create a method chain:\n\n\n\n\n\n\nChallenge\n\n\n\n\nTake first 25 records of clean_fuels data frame and calculate summary statistical summary\nCompute statistical summary of the whole data and then present only quantile summaries of each column. The quantiles include minimum, maximum as well as the 25th, 50th and 75th quantile.\n\n\n\n\n(clean_fuels\n    .head(25)\n    .describe())\n\n(clean_fuels\n    .describe()\n    .tail(5))\n\n\nshape: (5, 7)\n\n\n\nstatistic\nregion\ncountry_code\ncountry\nyear\npop_clean_fuels_cooking_mln\nprop_clean_fuels_cooking_pct\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\n\n\n\n\n\"min\"\n\"Africa\"\n\"AFG\"\n\"Afghanistan\"\n1990.0\n0.0\n0.0\n\n\n\"25%\"\nnull\nnull\nnull\n1998.0\n0.23\n16.6\n\n\n\"50%\"\nnull\nnull\nnull\n2006.0\n2.24\n78.0\n\n\n\"75%\"\nnull\nnull\nnull\n2014.0\n10.24\n100.0\n\n\n\"max\"\n\"Western Pacific\"\n\"ZWE\"\n\"Zimbabwe\"\n2022.0\n1257.0\n100.0\n\n\n\n\n\n\nSometimes, datasets have many columns, making it difficult to gain a full overview using methods like head() or describe(). For these cases, Polars provides a particularly useful method called glimpse().\nWhen you use glimpse(), the dataset’s structure is displayed horizontally. Each variable is listed as a row, making it easier to scan through all columns, even if you’re working with a limited screen space. Here’s how it looks in action:\n\n(clean_fuels  \n    .glimpse())\n\nRows: 6402\nColumns: 6\n$ region                       &lt;str&gt; 'Africa', 'Western Pacific', 'Western Pacific', 'Western Pacific', 'Western Pacific', 'Africa', 'Western Pacific', 'Africa', 'Western Pacific', 'Western Pacific'\n$ country_code                 &lt;str&gt; 'SSD', 'NIU', 'TKL', 'COK', 'PLW', 'STP', 'FSM', 'BDI', 'NRU', 'TUV'\n$ country                      &lt;str&gt; 'South Sudan', 'Niue', 'Tokelau', 'Cook Islands', 'Palau', 'Sao Tome and Principe', 'Micronesia (Federated States of)', 'Burundi', 'Nauru', 'Tuvalu'\n$ year                         &lt;i64&gt; 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022\n$ pop_clean_fuels_cooking_mln  &lt;f64&gt; 0.0, 0.002, 0.0004, 0.013, 0.007, 0.009, 0.014, 0.013, 0.011, 0.009\n$ prop_clean_fuels_cooking_pct &lt;f64&gt; 0.0, 98.5, 28.3, 72.7, 29.45, 4.1, 13.2, 0.1, 100.0, 75.2\n\n\n\nNow, here’s a question for you what happens if you try to use glimpse() in a method chain?\n\n\n\n\n\n\nQuestion\n\n\n\nCan you chain the operation head() after calling glimpse()? What do you think the output will be?\n\n\nThe answer is: no, you cannot. glimpse(), does not return you a data frame. Instead, the output of glimpse() is the text printout meant solely for viewing. No further operations can be applied to it. If you attempt to chain additional methods, you’ll encounter an error. Give it a try if you want! Polars will throw an error saying that the head() method cannot be applied to a NoneType, which is the type of output glimpse() returns.\nBy understanding how to use head(), tail(), describe(), and glimpse(), you have powerful tools at your disposal to explore and familiarize yourself with any dataset before diving deeper into your analysis."
  },
  {
    "objectID": "wrangling.html#selectdrop",
    "href": "wrangling.html#selectdrop",
    "title": "Wrangling",
    "section": "select/drop",
    "text": "select/drop\nOne of the most common tasks in data analysis is selecting specific variables or columns from a dataset. Let’s start by pulling out the country information from the clean_fuels dataset. Pause the video for a moment and try running this code:\n\n(clean_fuels  \n    .select(pl.col(\"country\")))\n\n\nshape: (6_402, 1)\n\n\n\ncountry\n\n\nstr\n\n\n\n\n\"South Sudan\"\n\n\n\"Niue\"\n\n\n\"Tokelau\"\n\n\n\"Cook Islands\"\n\n\n\"Palau\"\n\n\n…\n\n\n\"Austria\"\n\n\n\"Germany\"\n\n\n\"Sweden\"\n\n\n\"Portugal\"\n\n\n\"India\"\n\n\n\n\n\n\nHere, we’re using the select() method to isolate a column. Notice how the column name is wrapped in the pl.col() function. This wrapper explicitly tells Polars that we’re referring to a column in the dataframe.\nBut here’s something cool—you can skip the pl.col() wrapper in certain cases! For example, this code:\n\n(clean_fuels  \n    .select(\"country_code\", \"country\"))  \n\n\nshape: (6_402, 2)\n\n\n\ncountry_code\ncountry\n\n\nstr\nstr\n\n\n\n\n\"SSD\"\n\"South Sudan\"\n\n\n\"NIU\"\n\"Niue\"\n\n\n\"TKL\"\n\"Tokelau\"\n\n\n\"COK\"\n\"Cook Islands\"\n\n\n\"PLW\"\n\"Palau\"\n\n\n…\n…\n\n\n\"AUT\"\n\"Austria\"\n\n\n\"DEU\"\n\"Germany\"\n\n\n\"SWE\"\n\"Sweden\"\n\n\n\"PRT\"\n\"Portugal\"\n\n\n\"IND\"\n\"India\"\n\n\n\n\n\n\n…does the exact same thing as this:\n\n(clean_fuels  \n    .select(pl.col(\"country_code\"), pl.col(\"country\")))  \n\n\nshape: (6_402, 2)\n\n\n\ncountry_code\ncountry\n\n\nstr\nstr\n\n\n\n\n\"SSD\"\n\"South Sudan\"\n\n\n\"NIU\"\n\"Niue\"\n\n\n\"TKL\"\n\"Tokelau\"\n\n\n\"COK\"\n\"Cook Islands\"\n\n\n\"PLW\"\n\"Palau\"\n\n\n…\n…\n\n\n\"AUT\"\n\"Austria\"\n\n\n\"DEU\"\n\"Germany\"\n\n\n\"SWE\"\n\"Sweden\"\n\n\n\"PRT\"\n\"Portugal\"\n\n\n\"IND\"\n\"India\"\n\n\n\n\n\n\nPretty neat, right? The select() method can directly interpret strings as column names, making your code a little cleaner and quicker to write.\nWhen you wrap a column name in pl.col(), you’re creating an expression. An expression is like an instruction—it doesn’t do anything on its own. For example, if you run this code:\n\npl.col(\"country_code\")\n\ncol(\"country_code\")\n\n\n…nothing happens. It just returns something called an “unevaluated expression”. But when you evaluate that expression in the context of a dataset, it turns into something powerful. For instance:\n\n(clean_fuels  \n    .select(pl.col(\"country_code\")))  \n\n\nshape: (6_402, 1)\n\n\n\ncountry_code\n\n\nstr\n\n\n\n\n\"SSD\"\n\n\n\"NIU\"\n\n\n\"TKL\"\n\n\n\"COK\"\n\n\n\"PLW\"\n\n\n…\n\n\n\"AUT\"\n\n\n\"DEU\"\n\n\n\"SWE\"\n\n\n\"PRT\"\n\n\n\"IND\"\n\n\n\n\n\n\nHere, the select() method acts as an evaluation environment, turning the pl.col() expression into actual data. select() is one of the several methods in Polars that can evaluate expressions. While select() is highly versatile and can do other things as well, for now, we’ll focus on its simplest use case: extracting columns from a data frame.\nThe pl.col() wrapper is super flexible, and it’s going to be central as we build more advanced expressions in Polars. For instance, you can use pl.col() to refer to multiple columns simultaneously:\n\n(clean_fuels  \n    .select(pl.col(\"country_code\", \"country\")))  \n\n\nshape: (6_402, 2)\n\n\n\ncountry_code\ncountry\n\n\nstr\nstr\n\n\n\n\n\"SSD\"\n\"South Sudan\"\n\n\n\"NIU\"\n\"Niue\"\n\n\n\"TKL\"\n\"Tokelau\"\n\n\n\"COK\"\n\"Cook Islands\"\n\n\n\"PLW\"\n\"Palau\"\n\n\n…\n…\n\n\n\"AUT\"\n\"Austria\"\n\n\n\"DEU\"\n\"Germany\"\n\n\n\"SWE\"\n\"Sweden\"\n\n\n\"PRT\"\n\"Portugal\"\n\n\n\"IND\"\n\"India\"\n\n\n\n\n\n\nSometimes, typing out long column names can feel like a chore, especially when you’re working with many columns. But don’t worry—Polars makes it easy to select columns by their position in the dataset. For example, this code selects the second and third columns by their numerical index:\n\n(clean_fuels  \n    .select(pl.nth(1,2)))  \n\n\nshape: (6_402, 2)\n\n\n\ncountry_code\ncountry\n\n\nstr\nstr\n\n\n\n\n\"SSD\"\n\"South Sudan\"\n\n\n\"NIU\"\n\"Niue\"\n\n\n\"TKL\"\n\"Tokelau\"\n\n\n\"COK\"\n\"Cook Islands\"\n\n\n\"PLW\"\n\"Palau\"\n\n\n…\n…\n\n\n\"AUT\"\n\"Austria\"\n\n\n\"DEU\"\n\"Germany\"\n\n\n\"SWE\"\n\"Sweden\"\n\n\n\"PRT\"\n\"Portugal\"\n\n\n\"IND\"\n\"India\"\n\n\n\n\n\n\nWatch out column indices in Polars are 0-based. That means the first column is index 0, the second column is index 1, and so on.\nWhat about negative numbers? They’re a handy shortcut for selecting columns from the end of the dataset. For instance, -1 refers to the last column, and this code will select the first and last columns:\n\n(clean_fuels  \n    .select(pl.nth(0, -1)))  \n\n\nshape: (6_402, 2)\n\n\n\nregion\nprop_clean_fuels_cooking_pct\n\n\nstr\nf64\n\n\n\n\n\"Africa\"\n0.0\n\n\n\"Western Pacific\"\n98.5\n\n\n\"Western Pacific\"\n28.3\n\n\n\"Western Pacific\"\n72.7\n\n\n\"Western Pacific\"\n29.45\n\n\n…\n…\n\n\n\"Europe\"\n100.0\n\n\n\"Europe\"\n100.0\n\n\n\"Europe\"\n100.0\n\n\n\"Europe\"\n100.0\n\n\n\"South-East Asia\"\n11.1\n\n\n\n\n\n\nA note of caution: selecting columns by the order of their occurence can be risky. If your dataset’s structure changes, you might accidentally select the wrong columns. So, use the nth() function sparingly.\nNow, let’s talk about the opposite of select()—the drop() method. The drop() method removes specific columns from your dataset, leaving everything else intact. For example:\n\n(clean_fuels  \n    .drop(pl.col(\"country\"), pl.col(\"region\")))  \n\n\nshape: (6_402, 4)\n\n\n\ncountry_code\nyear\npop_clean_fuels_cooking_mln\nprop_clean_fuels_cooking_pct\n\n\nstr\ni64\nf64\nf64\n\n\n\n\n\"SSD\"\n2022\n0.0\n0.0\n\n\n\"NIU\"\n2022\n0.002\n98.5\n\n\n\"TKL\"\n2022\n0.0004\n28.3\n\n\n\"COK\"\n2022\n0.013\n72.7\n\n\n\"PLW\"\n2022\n0.007\n29.45\n\n\n…\n…\n…\n…\n\n\n\"AUT\"\n1990\n7.72\n100.0\n\n\n\"DEU\"\n1990\n79.12\n100.0\n\n\n\"SWE\"\n1990\n8.57\n100.0\n\n\n\"PRT\"\n1990\n9.95\n100.0\n\n\n\"IND\"\n1990\n96.58\n11.1\n\n\n\n\n\n\nDopping is equivalent to selecting all columns except the ones you want to exclude. Here’s how could would write it using in terms of selection:\n\n(clean_fuels  \n    .select(pl.all().exclude(\"country\", \"region\")))  \n\n\nshape: (6_402, 4)\n\n\n\ncountry_code\nyear\npop_clean_fuels_cooking_mln\nprop_clean_fuels_cooking_pct\n\n\nstr\ni64\nf64\nf64\n\n\n\n\n\"SSD\"\n2022\n0.0\n0.0\n\n\n\"NIU\"\n2022\n0.002\n98.5\n\n\n\"TKL\"\n2022\n0.0004\n28.3\n\n\n\"COK\"\n2022\n0.013\n72.7\n\n\n\"PLW\"\n2022\n0.007\n29.45\n\n\n…\n…\n…\n…\n\n\n\"AUT\"\n1990\n7.72\n100.0\n\n\n\"DEU\"\n1990\n79.12\n100.0\n\n\n\"SWE\"\n1990\n8.57\n100.0\n\n\n\"PRT\"\n1990\n9.95\n100.0\n\n\n\"IND\"\n1990\n96.58\n11.1\n\n\n\n\n\n\nThe pl.all() function refers to all columns, and the exclude() method lets you refine the selection by removing specific ones.The pl.all() function refers to all columns, and the exclude() method lets you refine the selection by removing specific ones.\nA quick reminder—dropping columns doesn’t modify your original dataset. It only affects the result of that query. Unless you explicitly overwrite the original dataframe, everything stays the same. So feel free to experiment!\nNow it’s your turn. Select the columns related to population and the proportion of the population with access to clean fuels. Try using both selection by name and by index, as well as dropping the ones you dont need.\n\n\n\n\n\n\nChallenge\n\n\n\nSelect the columns related to population and the proportion of population with access to clean fuels from the clean_fuels dataset.\n\n\nPause the video and try couple of different ways of selecting these columns.\n\n(clean_fuels  \n    .select(pl.col(\"pop_clean_fuels_cooking_mln\"), pl.col(\"prop_clean_fuels_cooking_pct\"))) \n\n(clean_fuels  \n    .select(pl.nth(-2,-1))) \n\n(clean_fuels  \n    .drop(\"region\", \"country_code\", \"country\", \"year\"))\n\n\nshape: (6_402, 2)\n\n\n\npop_clean_fuels_cooking_mln\nprop_clean_fuels_cooking_pct\n\n\nf64\nf64\n\n\n\n\n0.0\n0.0\n\n\n0.002\n98.5\n\n\n0.0004\n28.3\n\n\n0.013\n72.7\n\n\n0.007\n29.45\n\n\n…\n…\n\n\n7.72\n100.0\n\n\n79.12\n100.0\n\n\n8.57\n100.0\n\n\n9.95\n100.0\n\n\n96.58\n11.1\n\n\n\n\n\n\nGot it? Great! Both approaches—selecting specific columns or dropping the ones you don’t need—give you the same result. Expressions like these make your analysis more dynamic and efficient, so you can quickly adapt to different datasets or scenarios."
  },
  {
    "objectID": "wrangling.html#selectors",
    "href": "wrangling.html#selectors",
    "title": "Wrangling",
    "section": "selectors",
    "text": "selectors\nSelecting columns is such a common task that Polars has a dedicated module for it—polars.selectors. This module provides a collection of methods specifically designed to simplify column selection. These are often aliased as cs for convenience. Have a look at the polars documentation for selectors. To get started, make sure you import the selectors module:\n\nimport polars.selectors as cs\n\nAmong the most useful selectors are, of course, selectors by name and column index (for which we might not really need selectors, because those can be picked up with pl.col() and pl.nth()).\n\n(clean_fuels\n    .select(cs.by_name(\"region\", \"country\"))\n)\n\n# note python is 0-based\n(clean_fuels\n    .select(cs.by_index(0,2,5))\n)\n\n\nshape: (6_402, 3)\n\n\n\nregion\ncountry\nprop_clean_fuels_cooking_pct\n\n\nstr\nstr\nf64\n\n\n\n\n\"Africa\"\n\"South Sudan\"\n0.0\n\n\n\"Western Pacific\"\n\"Niue\"\n98.5\n\n\n\"Western Pacific\"\n\"Tokelau\"\n28.3\n\n\n\"Western Pacific\"\n\"Cook Islands\"\n72.7\n\n\n\"Western Pacific\"\n\"Palau\"\n29.45\n\n\n…\n…\n…\n\n\n\"Europe\"\n\"Austria\"\n100.0\n\n\n\"Europe\"\n\"Germany\"\n100.0\n\n\n\"Europe\"\n\"Sweden\"\n100.0\n\n\n\"Europe\"\n\"Portugal\"\n100.0\n\n\n\"South-East Asia\"\n\"India\"\n11.1\n\n\n\n\n\n\nSelecting first and last columns are so common, there are useful shortthands cs.first() and cs.last(). To select all columns other than the one you specified, you can use the tilde ~ operator. Tilde operator works with all methods in cs. module and negates the selection. For example ~cs.last() refers to all columns other than the last one.\nSelectors can target columns based on their data types! For example, cs.numeric() picks all numeric columns. And if you want non-numeric columns, just negate it with ~.\nAnd now it is your turn. Try selecting first, everyhing other than the first, as well as all non-numeric columns. Use selector class for this. Pause the video and give it a try!\n\n\n\n\n\n\nChallenge\n\n\n\nUse polars.selectors aliased as cs to select - first column - everyhing other than first column - all non-numeric columns\n\n\n\n(clean_fuels\n    .select(cs.first())\n)\n\n# not first\n(clean_fuels\n    .select(~cs.first())\n)\n\n\n# not numeric\n(clean_fuels\n    .select(~cs.numeric())\n)\n\n\nshape: (6_402, 3)\n\n\n\nregion\ncountry_code\ncountry\n\n\nstr\nstr\nstr\n\n\n\n\n\"Africa\"\n\"SSD\"\n\"South Sudan\"\n\n\n\"Western Pacific\"\n\"NIU\"\n\"Niue\"\n\n\n\"Western Pacific\"\n\"TKL\"\n\"Tokelau\"\n\n\n\"Western Pacific\"\n\"COK\"\n\"Cook Islands\"\n\n\n\"Western Pacific\"\n\"PLW\"\n\"Palau\"\n\n\n…\n…\n…\n\n\n\"Europe\"\n\"AUT\"\n\"Austria\"\n\n\n\"Europe\"\n\"DEU\"\n\"Germany\"\n\n\n\"Europe\"\n\"SWE\"\n\"Sweden\"\n\n\n\"Europe\"\n\"PRT\"\n\"Portugal\"\n\n\n\"South-East Asia\"\n\"IND\"\n\"India\"\n\n\n\n\n\n\nFantastic work! With a wide menu of selector methods, plus column and index-based expressions like pl.col() and pl.nth(), Polars gives you incredible flexibility in working with your data. These tools will become invaluable as we move into crafting more complex expressions.\nStay tuned—there’s a lot more to explore!"
  },
  {
    "objectID": "wrangling.html#filter",
    "href": "wrangling.html#filter",
    "title": "Wrangling",
    "section": "Filter",
    "text": "Filter\nNow let’s talk about filtering data—an essential part of data analysis. In Polars, filtering allows you to subset your dataset based on logical conditions, and it’s powered by the magic of expressions. Logical operations are one of the simplest and most common use cases for expressions. For example, you can compare every value in the region column to the string “Europe”. If there’s a match, Polars returns True; otherwise, it returns False.\nLet’s see how this works in code:\n\n(clean_fuels\n    .filter(pl.col(\"region\")==\"Europe\")\n)\n\n\nshape: (1_749, 6)\n\n\n\nregion\ncountry_code\ncountry\nyear\npop_clean_fuels_cooking_mln\nprop_clean_fuels_cooking_pct\n\n\nstr\nstr\nstr\ni64\nf64\nf64\n\n\n\n\n\"Europe\"\n\"SMR\"\n\"San Marino\"\n2022\n0.034\n100.0\n\n\n\"Europe\"\n\"MCO\"\n\"Monaco\"\n2022\n0.04\n100.0\n\n\n\"Europe\"\n\"FRO\"\n\"Faroe Islands\"\n2022\n0.05\n100.0\n\n\n\"Europe\"\n\"AND\"\n\"Andorra\"\n2022\n0.077\n100.0\n\n\n\"Europe\"\n\"ISL\"\n\"Iceland\"\n2022\n0.35\n100.0\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"Europe\"\n\"BLR\"\n\"Belarus\"\n1990\n7.48\n73.2\n\n\n\"Europe\"\n\"AUT\"\n\"Austria\"\n1990\n7.72\n100.0\n\n\n\"Europe\"\n\"DEU\"\n\"Germany\"\n1990\n79.12\n100.0\n\n\n\"Europe\"\n\"SWE\"\n\"Sweden\"\n1990\n8.57\n100.0\n\n\n\"Europe\"\n\"PRT\"\n\"Portugal\"\n1990\n9.95\n100.0\n\n\n\n\n\n\nHere, the filter() method applies the logical condition, and only rows where the region is “Europe” are included in the result. Notice that for exact comparisons, we use the double equals sign ==. Similarly, for inequalities, we can use operators like &lt;=, &gt;=, &lt;, or &gt;. Not equal is spelled out as !=.\nBut filtering doesn’t stop there—you can combine multiple conditions to create more complex filters.\nHere’s a challenge for you. Can you find all the countries in Europe in the year 2022 where the majority of the population lacks access to clean fuels for cooking? Take a moment to write this expression. Pause the video if you need to.\n\n\n\n\n\n\nChallenge\n\n\n\nWere there any countries in Europe in the year 2022 where the majority of people lack access to clean fuel for cooking.\n\n\n\n(clean_fuels\n    .filter(\n        pl.col(\"region\")==\"Europe\",\n        pl.col(\"year\")==2022,\n        pl.col(\"prop_clean_fuels_cooking_pct\")&lt;50\n        )\n)\n\n\nshape: (1, 6)\n\n\n\nregion\ncountry_code\ncountry\nyear\npop_clean_fuels_cooking_mln\nprop_clean_fuels_cooking_pct\n\n\nstr\nstr\nstr\ni64\nf64\nf64\n\n\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n2022\n1.43\n41.1\n\n\n\n\n\n\nWhat did you get? Oh, wow! Over half the population of Bosnia still lacks access to clean fuels for cooking. That’s a powerful insight.\nNow let’s zoom in on Bosnia to better understand its data. Bosnia’s country code is “BIH”, but you can also filter by country name if you prefer.\n\n(\nclean_fuels\n    .filter(pl.col(\"country_code\")==\"BIH\")\n)\n\n\nshape: (33, 6)\n\n\n\nregion\ncountry_code\ncountry\nyear\npop_clean_fuels_cooking_mln\nprop_clean_fuels_cooking_pct\n\n\nstr\nstr\nstr\ni64\nf64\nf64\n\n\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n2022\n1.43\n41.1\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n2021\n1.43\n40.9\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n2020\n1.43\n40.85\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n2019\n1.47\n42.05\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n2018\n1.43\n40.8\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n1994\n2.29\n58.0\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n1993\n2.41\n58.9\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n1992\n2.47\n58.25\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n1991\n2.6\n59.5\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n1990\n2.64\n59.2\n\n\n\n\n\n\nWe are interested in tracking how the proportion of the population with access to clean fuels for cooking has changed over the years. To do this, we’ll place the year on the x-axis and the population proportion on the y-axis.\nIf you remember from the Plotnine module, the dataset goes into the first argument of the ggplot function.\nHere’s one way to do this:\n\n(\nggplot(\n    clean_fuels\n        .filter(pl.col(\"country_code\")==\"BIH\")\n    ) +\n    geom_line(mapping=aes(x=\"year\", y=\"prop_clean_fuels_cooking_pct\"))\n)\n\n\n\n\n\n\n\n\nThis works, but the code feels cluttered. It’s not immediately clear where the dataset comes from.\nLet’s clean this up using the .pipe() method.\n\n(clean_fuels\n    .filter(pl.col(\"country_code\")==\"BIH\")\n    .pipe(ggplot) +\n    geom_line(aes(x=\"year\", y=\"pop_clean_fuels_cooking_mln\"))\n)\n\n\n\n\n\n\n\n\nHere, the .pipe() method passes the filtered clean_fuels dataset into the ggplot function as its first argument. This keeps the code clean and modular. Everything after .pipe(ggplot) is Plotnine-specific code.\nNice!\nBut what if you’re not sure how a country’s name is spelled in the dataset? For example, is it “Czech Republic” or just “Czechia”?\nIn this case, you can use partial string matching to find it.\n\n(clean_fuels\n    .filter(pl.col(\"country\").str.starts_with(\"Cz\")))\n\n\nshape: (33, 6)\n\n\n\nregion\ncountry_code\ncountry\nyear\npop_clean_fuels_cooking_mln\nprop_clean_fuels_cooking_pct\n\n\nstr\nstr\nstr\ni64\nf64\nf64\n\n\n\n\n\"Europe\"\n\"CZE\"\n\"Czechia\"\n2022\n10.63\n100.0\n\n\n\"Europe\"\n\"CZE\"\n\"Czechia\"\n2021\n10.63\n100.0\n\n\n\"Europe\"\n\"CZE\"\n\"Czechia\"\n2020\n10.63\n100.0\n\n\n\"Europe\"\n\"CZE\"\n\"Czechia\"\n2019\n10.63\n100.0\n\n\n\"Europe\"\n\"CZE\"\n\"Czechia\"\n2018\n10.63\n100.0\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"Europe\"\n\"CZE\"\n\"Czechia\"\n1994\n10.36\n100.0\n\n\n\"Europe\"\n\"CZE\"\n\"Czechia\"\n1993\n10.36\n100.0\n\n\n\"Europe\"\n\"CZE\"\n\"Czechia\"\n1992\n10.35\n100.0\n\n\n\"Europe\"\n\"CZE\"\n\"Czechia\"\n1991\n10.35\n100.0\n\n\n\"Europe\"\n\"CZE\"\n\"Czechia\"\n1990\n10.34\n100.0\n\n\n\n\n\n\nHere, we use the str.starts_with() method, which checks if strings in the country column start with the letters “Cz.” Ah, there it is—“Czechia”! Polars offers several handy string operations, like:\n\nstr.starts_with()\nstr.ends_with()\nstr.contains()\n\nYou’ll see more of these as we progress, but these three are powerful enough to help you tackle the following challenge.\nFilter the data for your country and visualize the proportion of people with access to clean fuels. Once you’re happy with your data subset, use ggplot and everything you’ve learned about Plotnine to create a polished visualization.\n\n\n\n\n\n\nChallenge\n\n\n\nVisualize the proportion of people with access to clean fuels in your country\n\n\n\n(clean_fuels\n    .filter(pl.col(\"country\").str.starts_with(\"Ukr\"))\n    .pipe(ggplot)\n    + geom_line(aes(x=\"year\", y=\"prop_clean_fuels_cooking_pct\"))\n)\n\n\n\n\n\n\n\n\nThis looks fantastic! Great work visualizing your country’s data.\nIn the next section, we’ll explore adding more columns to our dataset and practice advanced subsetting and visualization techniques. Stay tuned!\nLet’s apply what we’ve learned about filtering to visualize the causes of death in some European countries.\n\nhhap_deaths\n\n\nshape: (10_800, 6)\n\n\n\nregion\ncountry_code\ncountry\nyear\ncause_of_death\ndeaths\n\n\nstr\nstr\nstr\ni64\nstr\nf64\n\n\n\n\n\"Americas\"\n\"ATG\"\n\"Antigua and Barbuda\"\n2010\n\"All causes\"\n0.0\n\n\n\"Americas\"\n\"ATG\"\n\"Antigua and Barbuda\"\n2010\n\"Acute lower respiratory infect…\n0.0\n\n\n\"Americas\"\n\"ATG\"\n\"Antigua and Barbuda\"\n2010\n\"Trachea, bronchus, lung cancer…\n0.0\n\n\n\"Americas\"\n\"ATG\"\n\"Antigua and Barbuda\"\n2010\n\"Ischaemic heart disease\"\n0.0\n\n\n\"Americas\"\n\"ATG\"\n\"Antigua and Barbuda\"\n2010\n\"Stroke\"\n0.0\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"Americas\"\n\"BOL\"\n\"Bolivia (Plurinational State o…\n2019\n\"Trachea, bronchus, lung cancer…\n98.72\n\n\n\"Africa\"\n\"GNB\"\n\"Guinea-Bissau\"\n2019\n\"Chronic obstructive pulmonary …\n98.88\n\n\n\"Africa\"\n\"CIV\"\n\"Cote d'Ivoire\"\n2019\n\"Chronic obstructive pulmonary …\n990.6\n\n\n\"Europe\"\n\"TUR\"\n\"Türkiye\"\n2019\n\"Chronic obstructive pulmonary …\n997.6\n\n\n\"Europe\"\n\"UZB\"\n\"Uzbekistan\"\n2019\n\"All causes\"\n9982.0\n\n\n\n\n\n\nThis dataset contains both summarized and detailed breakdowns of deaths for every country and year. Take a look at the column labeled cause_of_death. When this column says “All causes,” it represents the total deaths for that country and year—a sum of all the other rows.\nLet’s zoom in on Bosnia for a single year, say 2010, to understand this better.\n\n(hhap_deaths\n    .filter(pl.col(\"country_code\")==\"BIH\", \n            pl.col(\"year\")==2010)\n    )\n\n\nshape: (6, 6)\n\n\n\nregion\ncountry_code\ncountry\nyear\ncause_of_death\ndeaths\n\n\nstr\nstr\nstr\ni64\nstr\nf64\n\n\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n2010\n\"Acute lower respiratory infect…\n147.2\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n2010\n\"Stroke\"\n1685.0\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n2010\n\"Ischaemic heart disease\"\n2067.0\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n2010\n\"Chronic obstructive pulmonary …\n365.2\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n2010\n\"All causes\"\n4816.0\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n2010\n\"Trachea, bronchus, lung cancer…\n551.7\n\n\n\n\n\n\nOne of the rows is labeled “All causes” with 4,816 deaths. This total matches the sum of the individual causes of death. While it’s useful to have the total, it can lead to double counting if we include it in our analysis.\nNow let’s expand our view to include all European countries for which we have death data. We’ll exclude the totals and focus on trends for each specific cause of death. Faceting will help us visualize these trends country by country.\n\n(hhap_deaths\n    .filter(pl.col(\"region\")==\"Europe\",\n            pl.col(\"deaths\")&gt;0,\n            pl.col(\"cause_of_death\")!=\"All causes\")\n    .pipe(ggplot)+\n    aes(x=\"year\", y=\"deaths\", color=\"cause_of_death\", group=\"cause_of_death\")+\n    geom_smooth(method=\"lm\")+\n    facet_wrap(\"country\", scales=\"free_y\", nrow=2)+\n    theme(figure_size=(20,10), legend_position=\"bottom\")\n    )\n\n\n\n\n\n\n\n\nMost of the trends appear to be decreasing, which is good news. However, even with free y-axis scales for each country, the differences in scale make it hard to compare trends across countries. Look at Moldova! There’s a dramatic improvement in death cases here. Meanwhile, heart- and stroke-related deaths in neighboring Russia are on the rise.\nCan we calculate population numbers from our clean_fuels dataset?"
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "Welcome to the lesson on data visualization!\nData visualization is one of the most important skills in data analysis. Why? Because a well-made chart can reveal patterns, trends, and insights that might otherwise stay hidden in a spreadsheet. It’s like turning a jumble of numbers into a picture that tells a story.\nBut let’s be honest—visualizing data can sometimes feel overwhelming. There are so many types of charts to choose from and endless options for customizing them. Scatter plots, bar graphs, heatmaps—what should you use? And even after you pick a plot, there are all these parameters: axis, labels, color scales, gridlines… It’s easy to feel like you’re drowning in options!\nPoint-and-click tools for visualization, like those built into some software, can be helpful, but they come with their own challenges. They often overwhelm you with choices, and worse, they don’t always give you an easy way to reproduce or share your work.\nWhen you create visualizations using a script instead of a mouse, you unlock an entirely new level of power. Scripting your plots means they’re reproducible. You can tweak them, reuse them, and share the code with others. It’s like building a recipe that others can follow, modify, or inspect to understand exactly how the visualization was made.\nNow, let’s take a step back into history for a moment. In 1999, a statistician named Leland Wilkinson published a groundbreaking book called The Grammar of Graphics. Think of it like this: Just as grammar gives structure to language, Wilkinson’s framework gave structure to statistical graphics. He introduced a way to think about and construct plots systematically, rather than relying on intuition or tradition alone.\nHis ideas were revolutionary and influenced countless tools for making visualizations. One of the most famous examples is the R package ggplot2, created by Hadley Wickham. Wickham built on Wilkinson’s Grammar of Graphics and created what is now considered one of the most powerful and popular visualization tools in the world of data science.\nIn 2017 a passionate Python developer from Uganda by the name Hassan Kibridge ported ggplot2 into Python. His project became known as plotnine and it soon became a universal success. Here’s the story of plotnine in his own words:\n\nI discovered this “Grammar of Graphics” thing and found it elegant and powerful. So, I wanted to be able to use it in Python, my preferred programming language for data analysis. I read the key text on the subject by Leland Wilkinson and while I had a good grasp of it, translating it into a usable system would have been a huge undertaking. Hadley Wickham had done so for his doctorate and came up with ggplot2. While I was not confident enough to take this on, I felt that it was quite revolutionary and someone was going to implement it for Python.\nAnd it happened—a project came up that allowed people to seemingly make plots in Python using a grammar just like ggplot2. I started using it but soon discovered that it did not implement a grammar, it just faked one, and it broke down when you tried to make more complicated plots. Since this project was open source, I contributed to improving it. Yet to fix what was lacking grew into a complete overhaul, and this endeavor is what became Plotnine. The first release of which came out in July 2017 after about 3 years of on-and-off development.\nTo my surprise, Plotnine has been more successful than I imagined. For it, I have twice received the Google Open Source Peer Bonus Award—a recognition from Google employees towards open source software that is essential to their work. I have helped someone who was working on a COVID-19 vaccine trial solve a small problem they had run into with Plotnine. And, there is a book or two about it.\n\n\n\n\n\n\n\nTip\n\n\n\nHave a look at the full interview here as well as Hassan’s talk at the posit::conf(2023) in Chicago.\n\n\nWhile the syntax of plotnine might feel a bit different from typical Python code, don’t worry—there’s a reason for it! The goal is to keep the grammar intact, and that consistency makes it easy to learn and incredibly flexible to use.\nIn this lesson, we’ll dive into plotnine and explore how it allows you to create clear, beautiful, and insightful visualizations. We’ll guide you step by step so you can quickly become comfortable with its intuitive and expressive syntax.\nSo, join us on this journey into the wonderful world of data visualization. By the end, you’ll be creating plots that don’t just look good but also communicate your data’s story effectively. Let’s get started!"
  },
  {
    "objectID": "visualization.html#welcome",
    "href": "visualization.html#welcome",
    "title": "Visualization",
    "section": "",
    "text": "Welcome to the lesson on data visualization!\nData visualization is one of the most important skills in data analysis. Why? Because a well-made chart can reveal patterns, trends, and insights that might otherwise stay hidden in a spreadsheet. It’s like turning a jumble of numbers into a picture that tells a story.\nBut let’s be honest—visualizing data can sometimes feel overwhelming. There are so many types of charts to choose from and endless options for customizing them. Scatter plots, bar graphs, heatmaps—what should you use? And even after you pick a plot, there are all these parameters: axis, labels, color scales, gridlines… It’s easy to feel like you’re drowning in options!\nPoint-and-click tools for visualization, like those built into some software, can be helpful, but they come with their own challenges. They often overwhelm you with choices, and worse, they don’t always give you an easy way to reproduce or share your work.\nWhen you create visualizations using a script instead of a mouse, you unlock an entirely new level of power. Scripting your plots means they’re reproducible. You can tweak them, reuse them, and share the code with others. It’s like building a recipe that others can follow, modify, or inspect to understand exactly how the visualization was made.\nNow, let’s take a step back into history for a moment. In 1999, a statistician named Leland Wilkinson published a groundbreaking book called The Grammar of Graphics. Think of it like this: Just as grammar gives structure to language, Wilkinson’s framework gave structure to statistical graphics. He introduced a way to think about and construct plots systematically, rather than relying on intuition or tradition alone.\nHis ideas were revolutionary and influenced countless tools for making visualizations. One of the most famous examples is the R package ggplot2, created by Hadley Wickham. Wickham built on Wilkinson’s Grammar of Graphics and created what is now considered one of the most powerful and popular visualization tools in the world of data science.\nIn 2017 a passionate Python developer from Uganda by the name Hassan Kibridge ported ggplot2 into Python. His project became known as plotnine and it soon became a universal success. Here’s the story of plotnine in his own words:\n\nI discovered this “Grammar of Graphics” thing and found it elegant and powerful. So, I wanted to be able to use it in Python, my preferred programming language for data analysis. I read the key text on the subject by Leland Wilkinson and while I had a good grasp of it, translating it into a usable system would have been a huge undertaking. Hadley Wickham had done so for his doctorate and came up with ggplot2. While I was not confident enough to take this on, I felt that it was quite revolutionary and someone was going to implement it for Python.\nAnd it happened—a project came up that allowed people to seemingly make plots in Python using a grammar just like ggplot2. I started using it but soon discovered that it did not implement a grammar, it just faked one, and it broke down when you tried to make more complicated plots. Since this project was open source, I contributed to improving it. Yet to fix what was lacking grew into a complete overhaul, and this endeavor is what became Plotnine. The first release of which came out in July 2017 after about 3 years of on-and-off development.\nTo my surprise, Plotnine has been more successful than I imagined. For it, I have twice received the Google Open Source Peer Bonus Award—a recognition from Google employees towards open source software that is essential to their work. I have helped someone who was working on a COVID-19 vaccine trial solve a small problem they had run into with Plotnine. And, there is a book or two about it.\n\n\n\n\n\n\n\nTip\n\n\n\nHave a look at the full interview here as well as Hassan’s talk at the posit::conf(2023) in Chicago.\n\n\nWhile the syntax of plotnine might feel a bit different from typical Python code, don’t worry—there’s a reason for it! The goal is to keep the grammar intact, and that consistency makes it easy to learn and incredibly flexible to use.\nIn this lesson, we’ll dive into plotnine and explore how it allows you to create clear, beautiful, and insightful visualizations. We’ll guide you step by step so you can quickly become comfortable with its intuitive and expressive syntax.\nSo, join us on this journey into the wonderful world of data visualization. By the end, you’ll be creating plots that don’t just look good but also communicate your data’s story effectively. Let’s get started!"
  },
  {
    "objectID": "visualization.html#libraries",
    "href": "visualization.html#libraries",
    "title": "Visualization",
    "section": "Libraries",
    "text": "Libraries\nBefore we dive in, let’s talk about the tools and libraries we’ll be using in this lesson.\n\nimport polars as pl\nfrom plotnine import *\nfrom gapminder import gapminder\n\nFirst up is polars, the powerful data analysis library we’ll be relying on throughout the course. If you’re familiar with Python, you know it’s common practice to use shorthand or aliases when importing libraries. For polars, the standard alias is pl, so that’s what we’ll use here. Anytime we call a function from polars, it will be prefixed with pl. — simple and consistent.\nNow, when it comes to our visualization library, plotnine, we’ll take a slightly different approach. Instead of using a prefix, we’ll import all its functions directly into our workspace. This means we’ll use the from plotnine import * syntax, which essentially says, “Hey Python, bring in everything from plotnine!” Why? Because it makes our plotting code cleaner, easier to read, and more expressive.\nFinally, let’s talk about the dataset we’ll be exploring today. It comes from the gapminder package. If you’re not familiar with Gapminder, it’s a non-profit organization founded by Hans Rosling and his children back in 2005. Their mission? To promote a better understanding of global development through data—focusing on health, economics, and the environment.\nThe Gapminder Foundation maintains an incredible collection of statistics about the world, and this package is a small extract from their database. It’s packed with fascinating data on public health, economic development, and global welfare.\nHans Rosling himself is famous for his captivating TED Talk in 2007, where he used data to tell the story of global development. He spoke about life expectancy, GDP, and even the humble washing machine—and how it changed the world. If you haven’t watched that talk yet, I can’t recommend it enough. It’s a masterclass in how to make data come alive.\nSo, with our tools in hand and an inspiring dataset at our fingertips, we’re ready to start exploring and visualizing. Let’s get to it!"
  },
  {
    "objectID": "visualization.html#data",
    "href": "visualization.html#data",
    "title": "Visualization",
    "section": "Data",
    "text": "Data\nThe dataset has been conveniently imported for us by the gapminder package. To get started, we can simply type gapminder into our console and hit Enter. When we do that, we’ll see a preview of the data in the form of a table — what we call a DataFrame. In this table, each row represents an observation, and each column represents a variable.\n\ngapminder\n\n\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\n0\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.445314\n\n\n1\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.853030\n\n\n2\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.100710\n\n\n3\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.197138\n\n\n4\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.981106\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n1699\nZimbabwe\nAfrica\n1987\n62.351\n9216418\n706.157306\n\n\n1700\nZimbabwe\nAfrica\n1992\n60.377\n10704340\n693.420786\n\n\n1701\nZimbabwe\nAfrica\n1997\n46.809\n11404948\n792.449960\n\n\n1702\nZimbabwe\nAfrica\n2002\n39.989\n11926563\n672.038623\n\n\n1703\nZimbabwe\nAfrica\n2007\n43.487\n12311143\n469.709298\n\n\n\n\n1704 rows × 6 columns\n\n\n\nThis dataset has 1,704 rows and 6 columns, so it’s fairly compact but still rich with information. Let’s walk through what each of these columns means:\n\ncountry: This column lists the names of countries. If you take a look at the data, you’ll notice it starts with Afghanistan at the top and ends with Zimbabwe at the bottom. It seems the data is sorted alphabetically by country.\ncontinent: Here, we have the names of continents. For example, Afghanistan is listed under Asia, while Zimbabwe is under Africa. Makes perfect sense.\nyear: This column tells us the year of the observation. You’ll notice that each country has multiple rows because data was collected at different times. The dataset starts in 1952 and progresses in 5-year increments, which gives us a nice snapshot of changes over time.\nlifeExp: This column stands for life expectancy at birth. If we look at Afghanistan in 1952, for example, the life expectancy was just 28.8 years. Let that sink in for a moment — only 28 years! It’s a sobering reminder of the challenges some nations faced in the mid-20th century.\npop: This column shows the population of each country. Again, looking at Afghanistan in 1952, the population was just under 8.5 million people.\ngdpPercap: Finally, this column contains the GDP per capita, expressed in US dollars. From what I understand, these figures have been adjusted for inflation, so they should be comparable across countries and over time.\n\nAltogether, these six columns give us a fascinating lens through which to explore global trends in health, wealth, and population growth. The dataset might look simple at first glance, but it’s packed with stories waiting to be uncovered.\nNow that we know what we’re working with, let’s roll up our sleeves and start exploring!"
  },
  {
    "objectID": "visualization.html#first-plot",
    "href": "visualization.html#first-plot",
    "title": "Visualization",
    "section": "First plot",
    "text": "First plot\nNow, it’s time to create our very first plot! Here is a question we would like to answer using gapminder data:\n\n\n\n\n\n\nQuestion\n\n\n\nDo people in rich countries live longer than people in poor countries?\n\n\nThe answer may be quite intuitive, but we will continue our investigation further\n\n\n\n\n\n\nQuestion\n\n\n\n\nHow does the relationship between GDP per capita and Life expectancy look like? Is this relationship linear? Non-linear?\nAre there exceptions to the general rule (outliers)?\n\n\n\nIn order to answer these questions, we will create a plot from gapminder data. Here’s the code we’ll use. Take a moment to copy this code verbatim from your screen.\nWhen writing Python code with plotnine — and later when we use polars — you’ll notice that we often wrap our code in parentheses. This is a great habit to get into because it allows us to break our code into multiple lines without worrying about indentation.\n\n(\nggplot(gapminder)+\ngeom_point(mapping=aes(x='gdpPercap', y='lifeExp'))\n)\n\n\n\n\n\n\n\n\nLet’s walk through this step by step.\nInside the outer parentheses, the first thing we write is ggplot. This is the foundational function in plotnine, and it stands for Grammar of Graphics plot. Then, in parentheses again, we pass the dataset we want to use — in this case, gapminder.\nAfter that, we add a plus sign. The Grammar of Graphics, which plotnine is built on, thinks of plots as being made up of layers. The + sign we added tells Python that we’re adding more layers or components to our plot. Think of it as saying, “Wait, there’s more!”\nOn the next line, we write geom_point. This is the function that specifies the type of layer we’re adding to our plot. In this case, it’s a point plot, which means we’ll be drawing points on a graph. Without this layer, our plot would just be an empty canvas.\nInside the geom_point parentheses, we specify the argument: mapping followed by an = sign. This tells plotnine how we want to relate our data to the graph. AES stands for “aesthetics”. The inside of the aes function defines mapping of the variables in our data to certain aesthetical properies of our graph. We’re saying, “Take the GDP per capita (gdpPercap) and map it to the x-axis, and take life expectancy (lifeExp) and map it to the y-axis.” Notice that the column names are enclosed in quotes — that’s important!\nOnce you’ve written the code, go ahead and hit the Run button. If everything is correct, you should see your plot appear on the screen.\nLet’s take a moment to reflect on what we just did.\nIn our code, the first layer was the ggplot function, where we provided the dataset. The second layer was geom_point, which added points to our graph.\nThe result is a simple yet meaningful scatter plot. It shows a positive, non-linear relationship between GDP per capita on the x-axis and life expectancy on the y-axis. Does this align with what you initially expected? Or does it challenge your assumptions? Already, you can see how visualizing data helps uncover patterns and stories that might not be obvious at first glance.\nTake your time to review the code and compare it to the plot we created. Understanding this connection — how the code you write translates directly into what you see on the screen — is the key to mastering data visualization.\nIn fact, the structure of most plots in plotnine (and its R counterpart, ggplot2) can be summarized with a simple template:\nggplot(data = &lt;DATA&gt;) + \n  &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;))\nThis template is incredibly flexible and serves as the foundation for almost every visualization we’ll create.\nIn the remainder of this lesson, we’ll explore how to extend and customize this template to create a wide variety of visualizations. Each new element we add will open up even more possibilities.\nI’ll see you in the next one!"
  },
  {
    "objectID": "visualization.html#axis",
    "href": "visualization.html#axis",
    "title": "Visualization",
    "section": "Axis",
    "text": "Axis\nHello again! Ready for a challenge? I’ve got a question for you:\n\n\n\n\n\n\nChallenge\n\n\n\nHow has life expectancy changed over time?\n\n\nTake a moment to think about it. Better yet, try answering it by modifying the code we wrote in the last lesson.\nHere’s a quick hint before you pause the video: The gapminder dataset includes a column called year, which can go on the x-axis. Use that to tweak the code and see what you find. I’ll wait right here while you try it out!\nPause the video now and give it a shot. See you in a moment!\n\n\n(\nggplot(gapminder)+\ngeom_point(mapping = aes(x='year', y='lifeExp'))\n)\n\n\n\n\n\n\n\n\nDone? Excellent! Let’s take a look at what we’ve got. Nice work! You should see a scatter plot showing life expectancy over time.\nHmm… notice how some of the points are stacked on top of each other? That’s called overplotting, and it’s pretty common when you have a lot of data points at the same x or y values. Don’t worry—it’s easy to fix!\nInstead of geom_point, try using geom_jitter. This will add a tiny bit of random noise to spread out the points so they’re easier to see.\nHere’s how you do it:\n\n(\nggplot(gapminder)+\ngeom_jitter(mapping = aes(x='year', y='lifeExp'))\n)\n\n\n\n\n\n\n\n\nRun this code and check out the difference. Much better, right? Now we can see the points more clearly.\nLet’s keep going with this little game. Here’s your next challenge:\n\n\n\n\n\n\nChallenge\n\n\n\nCan you visualize life expectancy by continent?\n\n\nThink about which variable should go on the x-axis this time. Which continent do you think tends to have the highest life expectancy? Modify your code and give it a shot. Pause the video, try it out, and come back when you’re ready.\n\n\n(\nggplot(gapminder)+\ngeom_jitter(mapping = aes(x='continent', y='lifeExp'))\n)\n\n\n\n\n\n\n\n\nGreat job! What do we see here? Looks like life expectancy in Oceania is quite high, although there aren’t many points for that region. Europe is a close second. On the other hand, Africa seems to have the lowest life expectancy overall, judging by the density of points at the lower end of the y-axis.\nHere’s another question: Which continent has the widest spread in life expectancy values? That’s right—it’s Asia. There’s quite a bit of variation there, which is something we’ll dig into in more detail later in the course.\nFantastic work so far! Take a moment to review what you’ve done, and I’ll see you in the next section!"
  },
  {
    "objectID": "visualization.html#aestetical-mapping",
    "href": "visualization.html#aestetical-mapping",
    "title": "Visualization",
    "section": "Aestetical mapping",
    "text": "Aestetical mapping\n&lt;…Walks in, looking thoughtful….&gt;\nOh, hey there! You know, I’ve been thinking—what if we could combine the graphs from the last two challenges and show the relationship between not just two variables, but three?\nNow, don’t worry—we’re not diving into “three-dimensional” plots just yet. Instead, we can represent a third variable using color. Let me show you what I mean.\nHere’s modified code that maps the continent variable to the color aesthetic:\n\n(\nggplot(gapminder)+\ngeom_jitter(mapping = aes(x='year', y='lifeExp', color='continent'))\n)\n\n\n\n\n\n\n\n\nRun this code and take a look.\n\nWhat do you see? Now we can see more clearly how life expectancy has changed over time by continent. For example, the points representing Africa stay clustered near the lower end of the y-axis throughout the years, while Europe’s points are generally higher. Oceania is there too, but it’s barely noticeable because there are so few observations. Pretty cool, right?\nNow, I’ve got a question for you: What happens if we switch the mappings of continent and year? Give it a try!\n\n\n\n\n\n\nChallenge\n\n\n\nSwitch the mappings of continent and year in this sample code\n(\nggplot(gapminder)+\ngeom_jitter(mapping = aes(x='year', y='lifeExp', color='continent'))\n)\n\n\n\n# switch aesthetics\n(\nggplot(gapminder)+\ngeom_jitter(mapping = aes(x='continent', y='lifeExp', color='year'))\n)\n\n\n\n\n\n\n\n\n\nDone? Great! Do you still find this graph useful? Why or why not?\nNow let’s tweak it a bit more. What if, instead of mapping color to year, we mapped it to country? Give it a try!\n\n\n\n\n\n\nChallenge\n\n\n\nMap color to country in this sample code:\n(\nggplot(gapminder)+\ngeom_jitter(mapping = aes(x='continent', y='lifeExp', color='year'))\n)\n\n\n\n# color by country\n(\nggplot(gapminder)+\ngeom_jitter(mapping = aes(x='continent', y='lifeExp', color='country'))\n)\n\n\n\n\n\n\n\n\nWhat changed? How does mapping color to country differ from mapping it to year? Take a moment to think about it. What do you think is the main limitation of using the color aesthetic?\nAlright, here’s one last challenge for this section: Can you add a splash of color to our original graph of life expectancy by GDP per capita? Let’s color the points by continent.\n\n\n\n\n\n\nChallenge\n\n\n\nColor the points by continent in this sample code:\n(\nggplot(gapminder) +\ngeom_point(mapping = aes(x='gdpPercap', y='lifeExp'))\n)\n\n\nTake a moment to run your code and see what you get.\n\n(\nggplot(gapminder)+\ngeom_point(mapping = aes(x='gdpPercap', y='lifeExp', color='continent'))\n)\n\n\n\n\n\n\n\n\n\nAmazing! By adding color, we can now spot trends and patterns more easily. But did you notice something else? There are a few outliers in this plot. Can you tell which continent those points belong to?\nThe points look a little crowded. But you know, you can always transform GDP per capita to a logarithmic scale for better visualization. Just add scale_x_log10() as an additional layer to your graph, like that:\n\n# transform the scales\n(\nggplot(gapminder)+\ngeom_point(mapping = aes(x='gdpPercap', y='lifeExp', color='continent'))+\nscale_x_log10()\n)\n\n\n\n\n\n\n\n\nYou’re making fantastic progress! Well done! In the next section, we’ll explore even more aesthetics that can help us tell richer stories with our data. See you there!"
  },
  {
    "objectID": "visualization.html#more-aesthetics",
    "href": "visualization.html#more-aesthetics",
    "title": "Visualization",
    "section": "More aesthetics",
    "text": "More aesthetics\nHello again!\nSo far, we’ve explored some powerful ways to visualize data using the x, y, and color aesthetics. With these, we’ve been able to represent three variables in a single plot. Pretty amazing, right?\nNow, let’s quickly recap what we’ve learned about the color aesthetic. When we map a categorical variable like continent to color, plotnine automatically picks a distinct palette for each category. This works great when there are just a few categories, but as the number of categories grows, the colors start to blur together and lose their effectiveness.\nOn the other hand, when we map a continuous variable like year to color, we get a gradient. While individual values can be harder to pinpoint, the overall trends are beautifully highlighted by the gradient’s brightness.\nAlright, as promised, let me introduce you to another fantastic aesthetic: size.\nImagine we could vary the size of the points in our graph to represent something meaningful—like the population of a country. That would let us visualize not three, but four variables at the same time. Let’s give it a shot. Here’s the code:\n\n(\nggplot(gapminder)+\ngeom_point(mapping = aes(x='gdpPercap', y='lifeExp', color='continent', size='pop'))+\nscale_x_log10()\n)\n\n\n\n\n\n\n\n\nWRun this and take a moment to appreciate the result. Isn’t it gorgeous?\nNow we can see the journey of countries like China and India over time. Their points stand out because of their large populations. Under the logarithmic transformation of the x-axis, the relationship between GDP per capita and life expectancy starts to look more linear—but not quite!\nNotice the outliers on the far right? They all seem to be from Asian countries. Are these countries rich or poor? Rich, right? But their life expectancy doesn’t quite follow the trend we see in Europe or the Americas. Fascinating, isn’t it?\nNow, let me share one more aesthetic property with you: shape.\nShape can be a great tool for visualizing low-cardinality categorical variables, like continent. Instead of just using circles, we can use distinct shapes for each category. This lets us pack even more information into the same graph.\nReady for a challenge? Let’s push the limits and visualize five dimensions in a single plot. Modify the previous example to map year to color and continent to shape. Take a moment and try it. I’ll wait.\n\n\n\n\n\n\nChallenge\n\n\n\nMap year to color and continent to shape in this sample code\n(\nggplot(gapminder)+\ngeom_point(mapping = aes(x='gdpPercap', y='lifeExp', color='continent', size='pop'))+\nscale_x_log10()\n)\n\n\n\n\n# map year to color and continent to shape\n\n(\nggplot(gapminder)+\ngeom_point(mapping = aes(x='gdpPercap', y='lifeExp', color='year', size='pop', shape='continent'))+\nscale_x_log10()\n)\n\n\n\n\n\n\n\n\nWhat do you notice? Can you tell whether those Asian outliers come from small or large countries? Are they from earlier or later time periods?\nThese are the kinds of questions we can answer when we use multiple aesthetics thoughtfully. Isn’t it amazing how much information we can pack into a single visualization?\nFantastic work today! In the next lesson, we’ll continue exploring new tools and techniques to take your visualizations even further. See you soon!"
  },
  {
    "objectID": "visualization.html#non-data-linked-properties",
    "href": "visualization.html#non-data-linked-properties",
    "title": "Visualization",
    "section": "Non-data linked properties",
    "text": "Non-data linked properties\nWelcome back!\nSo far, we’ve packed a lot of information into single graphs using data-mapped aesthetics like color, size, and shape. While this approach is powerful, let’s face it—combining too many aesthetics can make a plot feel busy and overwhelming.\nSometimes, less is more. A clean and simple graph, highlighting just one or two aspects of the data, can be just as insightful—and a lot easier on the eyes.\nNow, the default style in plotnine is already quite nice, but there may come a time when you want to tweak things to better suit your storytelling. So, let’s look at how to customize graphs using non-data-linked properties—those that aren’t mapped to a variable but instead apply globally to all points in the graph.\nHere’s an example. What if we want all the points in our plot to be the same color, say blue? And what if we also want to adjust their size and transparency? Here’s the code to do that:\n\n(\nggplot(gapminder)+\ngeom_point(mapping = aes(x='gdpPercap', y='lifeExp'),\n            alpha=0.1, size=2, color='blue')\n)\n\n\n\n\n\n\n\n\nGo ahead, give this a try.\n\nBeautiful, isn’t it? All the points are now blue, with a larger size and a soft transparency that makes overlapping points blend together nicely. This transparency, or alpha, helps highlight areas where the data is dense—like shadows on a heatmap.\nNotice something? The color, size, and alpha settings aren’t part of the aes() function. That’s because these properties aren’t mapped to any variable in the data. Instead, they’re applied uniformly to every point in the plot.\nLet’s break it down:\n\ncolor=\"blue\": The color is set as a character string, wrapped in quotes. You can experiment with other colors too—try red, green, or even hex codes like “#FF5733”.\nsize=2: The size of the points is specified as a number, in millimeters. Increase the size to make the points larger or decrease it for smaller ones.\nalpha=0.1: Transparency is a decimal value between 0 and 1, where 0 is completely transparent and 1 is fully opaque.\n\nFinally, let’s talk about shapes. In plotnine, shapes are represented by numbers. For example:\n\n0 is a square,\n1 is a circle,\n2 is a triangle,\n20 is a small filled circle.\n\nHere’s a challenge for you\n\n\n\n\n\n\nChallenge\n\n\n\nChange the shape argument in the code to explore different shapes. Try values between 0 and 25, and see how your graph changes. You’ll find the full list of shapes in the plotnine documentation.\n\n\nSo, what do you think? With just a few tweaks, we’ve turned our scatter plot into a clean and stylish visual. Customizing non-data-linked properties like this is a great way to emphasize certain elements of your data without overwhelming your audience.\nIn the next lesson, we’ll explore even more ways to take your visualizations to the next level. See you there!"
  },
  {
    "objectID": "visualization.html#geometrical-objects",
    "href": "visualization.html#geometrical-objects",
    "title": "Visualization",
    "section": "Geometrical objects",
    "text": "Geometrical objects\nWelcome back! Let’s dive into another exciting aspect of creating visualizations in plotnine: geometrical objects, or geom_ functions.\nThese geom_ functions are the building blocks of your plots, allowing you to highlight different aspects of your data. By swapping or combining geom_ layers, you can tell entirely new stories with the same dataset.\nFor example, what if we wanted to show the development of life expectancy over time for each country? We could use geom_line() to connect individual data points belonging to the same country.\nHere’s the code to do just that:\n\n(\nggplot(gapminder)+\ngeom_line(mapping = aes(x='year', y='lifeExp',\n          group='country', color='continent'))\n)\n\n\n\n\n\n\n\n\nNote that we have a new aesthetics called group. It indicates which points need to be connected together to for a line. Here we are drawing one line per country. Take a moment to run this and see what you get.\n\nDo you see it? Each country now has its own line, colored by continent. It’s fascinating to watch life expectancy trends unfold over time. But look closely—you might notice some sharp, sudden drops for certain countries. What do you think caused these declines? Wars? Epidemics?\nWe’ll learn how to zoom in on these tragic moments and identify the affected countries later in the course, once we’ve mastered some data wrangling with Polars. For now, make a mental note of this question so you can return to it later.\nAnother powerful geometrical object is geom_boxplot(). This creates a “box-and-whisker” plot that illustrates the distribution of values within categories.\nFor example, let’s visualize how life expectancy varies by continent:\n\n(\nggplot(gapminder)+\ngeom_boxplot(mapping = aes(x='continent', y='lifeExp'))\n)\n\n\n\n\n\n\n\n\nRun the code and take a look.\n\nThe box represents the interquartile range—the middle 50% of data—while the line inside the box marks the median. The “whiskers” extend to show the 95% confidence interval, and any points outside this range are plotted as individual outliers.\nNow, wouldn’t it be great to combine this boxplot with our jittered points from earlier? This would help us see both the overall distribution and the outliers more clearly. Let’s layer them together:\n\n(\nggplot(gapminder)+\ngeom_jitter(mapping = aes(x='continent', y='lifeExp', color='continent'))+\ngeom_boxplot(mapping = aes(x='continent', y='lifeExp', color='continent'))\n)\n\n\n\n\n\n\n\n\n\nLooks great, doesn’t it? But notice something—there’s some duplication in our code. We had to repeat the same mappings for both geom_jitter and geom_boxplot. That’s fine for now, but it can become cumbersome as your visualizations grow more complex.\nHere’s a trick to make your code cleaner: you can move shared mappings to the parent ggplot() function. This way, every layer will “inherit” these mappings automatically:\n\n(\nggplot(gapminder, mapping = aes(x='continent', y='lifeExp', color='continent'))+\ngeom_jitter()+\ngeom_boxplot()\n)\n\n\n\n\n\n\n\n\nSee? No more repeating yourself! You can still add layer-specific settings or arguments within individual geom_ functions if needed.\n\n\n\n\n\n\nTip\n\n\n\nWhen building complex plots, start by adding one layer at a time. Once you’ve got the basic structure, move any common arguments up to the ggplot() function. This keeps your code tidy and easier to read.\n\n\nGreat job so far! In the next lesson, we’ll explore even more ways to enhance your visualizations. See you there!"
  },
  {
    "objectID": "visualization.html#trend-lines",
    "href": "visualization.html#trend-lines",
    "title": "Visualization",
    "section": "Trend lines",
    "text": "Trend lines\nWelcome back! Now, let’s take a closer look at the relationship between GDP per capita and life expectancy.\nAt first glance, life expectancy seems to improve as countries get richer. But is this relationship consistent across continents? Let’s find out by adding trend lines to our plot.\nTrends are essentially linear regression lines. You might remember them from school—they represent the best-fit line through your data. Here’s how we can add them to highlight differences in this relationship by continent:\n\n(\nggplot(gapminder, mapping = aes(x='gdpPercap', y='lifeExp', color='continent')) +\ngeom_point(alpha=0.5) +\ngeom_smooth(method='lm') +\nscale_x_log10()\n)\n\n\n\n\n\n\n\n\nTake a moment to run the code and see the result.\n\nWhat do you observe? By default, geom_smooth() creates a regression line for each continent, and plotnine even adds confidence intervals—those shaded gray areas around the lines. These intervals give us an idea of how well the model fits the data.\nWe also used the alpha argument to make our points semi-transparent. Why? It reduces visual clutter and lets the trend lines stand out more. Did you know that transparency can also be mapped to a variable? That’s right—just like color or size, you can use alpha as a mapping aesthetic to make transparency vary based on your data. Try experimenting with that later!\nHere’s a task for you: Modify the code we just used so that instead of creating separate regression lines for each continent, plotnine creates a single trend line for all data points.\nHere’s the code to start with:\n\n\n\n\n\n\nChallenge\n\n\n\nCreate a single regression line for all data points modifying this sample code\n(\nggplot(gapminder, mapping = aes(x='gdpPercap', y='lifeExp', color='continent'))+\ngeom_point(alpha=0.5)+\ngeom_smooth(method='lm')+\nscale_x_log10()\n)\n\n\nTake a moment to think about it. How can you combine the points colored by continent with a single global regression line?\nThere’s more than one way to solve this problem—see what you can come up with!\nDid you find this challenge hard? It’s ok! Let’s step through it together!\nIn our previous example, we declared all the mappings—x, y, and color—at the global level, in the ggplot() function. This means that every layer inherited these mappings. While this works well for most situations, it’s not what we need here.\nTo build a single trend line for all data points, we must ensure that the color aesthetic applies only to the points and not to the trend line. How do we do that? By moving the color mapping from the global level into the geom_point() function.\nHere’s how the updated code looks:\n\n(\nggplot(gapminder, mapping = aes(x='gdpPercap', y='lifeExp'))+\ngeom_point(aes(color='continent'), alpha=0.5)+\ngeom_smooth(method='lm')+\nscale_x_log10()\n)\n\n\n\n\n\n\n\n\nBy moving the color aesthetic into geom_point(), it now affects only the points layer. Notice that it’s still wrapped in the aes() function because it remains a data-linked property. Meanwhile, the trend line—added by geom_smooth()—inherits only the global mappings for x and y. This creates a single linear model across all continents, as we wanted.\nTake a moment to observe how this subtle adjustment changes the visualization and makes the trend line easier to interpret.\nSome of you might have come up with an alternative solution. Instead of changing the color aesthetic’s scope, we can override it directly within the geom_smooth() layer. In this case, the color aesthetic remains global, but we specify a non-data-linked property for the trend line, such as making it black:\n\n(\nggplot(gapminder, mapping = aes(x='gdpPercap', y='lifeExp', color='continent'))+\ngeom_point(alpha=0.5)+\ngeom_smooth(method='lm', color='black')+\nscale_x_log10()\n)\n\n\n\n\n\n\n\n\nHere, geom_smooth() ignores the global color aesthetic and instead applies the color black uniformly to the trend line. The result? A single black trend line stands out clearly, while the points remain color-coded by continent.\nBoth approaches work well, and the choice depends on how you want to structure your code and highlight different layers. Managing global and layer-specific mappings is a powerful feature in plotnine that gives you flexibility in creating clean, insightful plots."
  },
  {
    "objectID": "visualization.html#factors",
    "href": "visualization.html#factors",
    "title": "Visualization",
    "section": "Factors",
    "text": "Factors\nDo you want to learn a nifty trick that can improve your data visualization? This method can be useful when you want to visualize a continuous variable which has a limited number of distinct values.\nImagine we’re working with year, which is technically a continuous variable. But for some visualizations it might make more sense to treat each year as a separate category. How do we do that without modifying the data?\nSimple! Instead of referencing year as a string ('year'), wrap it in factor(), like this 'factor(year)'. This shorthand plotnine function converts a continuous variable into a categorical one on the fly, with each distinct value treated as its own category.\nLet’s put this into practice with a couple of challenges!\nCreate a boxplot of life expectancy over time, treating year as a categorical variable. Using this plot, can you detect when the interquartile range of life expectancy—the middle 50% of values—was the smallest?\nThen apply the same concept to gdpPercap.\nCreate a boxplot of GDP per capita by year, but this time keep it on a logarithmic scale. Remember that we used the scale_y_log10() function to make the data easier to interpret. Compare the interquartile range of GDP per capita in 2007 with that in 1952. Is the world today more or less diverse in terms of economic inequality?\n\n\n\n\n\n\nChallenge\n\n\n\n\nMake a boxplot of life expectancy by year. When was interquartile range of life expectancy the smallest?\nMake the same plot of gdpPercap (on a log scale) per year. Is the world today more or less diverse than in 1952?\n\n\n\nGo ahead and give it a try. Pause the video and come back once you have your answer!\n\n(\nggplot(gapminder)+\ngeom_boxplot(mapping = aes(x='factor(year)', y='lifeExp', group='year'))\n)\n\n\n\n\n\n\n\n\n\n(\nggplot(gapminder)+\ngeom_boxplot(mapping = aes(x='factor(year)', y='gdpPercap', group='year')) +\nscale_y_log10()\n)\n\n\n\n\n\n\n\n\nLooking at these two plots, you might notice a fascinating pattern.\nYou’re absolutely right: economic inequality has grown in the recent decades. In 1952, the world was much poorer, but there was a greater sense of uniformity across nations. By 2007, while the world is significantly wealthier on average, the disparities have widened.\nAnd those outliers? Intriguing, aren’t they? Three countries stand apart from the rest in the 1952. Which ones could they be? We’ll revisit these mysteries after diving into data wrangling techniques.\nGreat work on these challenges! These exercises show the power of visualizing data in different ways and how little tricks like factor() can make your plots much clearer. Next, we’ll explore other types of plots that can uncover even more insights. Stay tuned, and I’ll see you in the next lesson!"
  },
  {
    "objectID": "visualization.html#more-geoms",
    "href": "visualization.html#more-geoms",
    "title": "Visualization",
    "section": "More geoms",
    "text": "More geoms\nBy now, you’ve learned so much about using plotnine to create insightful visualizations. But we’ve barely scratched the surface!\nOne of the most exciting features of plotnine is the sheer variety of geoms—the building blocks for visualizing data. Start typing geom_ in your code editor, and you’ll see a list of options pop up. It’s like a treasure chest of possibilities, and each geom offers a unique perspective on your data.\nLet’s put your skills to the test with a few new challenges!\nHistograms are perfect for exploring the distribution of a single variable. Let’s start with life expectancy. Create a histogram and observe the shape of the distribution. How many peaks—or modes—does it have? Play around with the bins parameter. Adjusting the number of bins changes the granularity of your histogram, which can affect how you interpret the distribution. What value of bins seems reasonable to you?\n\n\n\n\n\n\nChallenge\n\n\n\nMake a histogram of life expectancy. What is the shape of the distribution? How many modes (peaks) does the distribution of life expectancy have? What value of the bins parameter look reasonable?\n\n\n\n(\nggplot(gapminder)+\ngeom_histogram(mapping = aes(x='lifeExp'), bins=100)\n)\n\n\n\n\n\n\n\n\nNext up: density plots. These are smoothed-out versions of histograms, showing the probability distribution of your data.\nCreate a simple density plot for life expectancy. You can do it! Start typing and you will find the function you need. Do you see it? What if you want to compare distributions across continents? Add a color aesthetic.\n\n\n\n\n\n\nChallenge\n\n\n\nBuild a density function. How would you compare density functions of different continents?\n\n\n\n(\nggplot(gapminder)+\ngeom_density(mapping = aes(x='lifeExp'))\n)\n\n\n\n\n\n\n\n\n\n(\nggplot(gapminder)+\ngeom_density(mapping = aes(x='lifeExp', color='continent'))\n)\n\n\n\n\n\n\n\n\nRight! You can split the data by continent by adding a color aesthetic and linking it to the variable continent. Or take it one step further! Use the fill aesthetic (in addition to color) to fill the areas under the curves. Add some transparency with alpha for a cleaner visualization like this:\n\n(\nggplot(gapminder)+\ngeom_density(mapping = aes(x='lifeExp', color='continent', fill='continent'), alpha=0.3)\n)\n\n\n\n\n\n\n\n\nThese plots help us see how life expectancy varies not just overall, but also within each continent. Notice any interesting patterns? What might explain the peaks—or modes—you see in the distributions?\nNow let’s level up with 2D density plots. These are excellent for visualizing relationships between two variables. Start by creating a density plot of log GDP per capita vs. life expectancy (use geom_density_2d() function):\n\n\n\n\n\n\nChallenge\n\n\n\nBuild a graph using geom_density2d() for log GDP per capita vs life expectancy. How many clusters of datapoints can you identify? What if you look at it by continent?\n\n\n\n(\nggplot(gapminder)+\ngeom_density_2d(mapping = aes(x='gdpPercap', y='lifeExp'))+\nscale_x_log10()\n)\n\n\n\n\n\n\n\n\nWhat do you see? Notice the two distinct clusters? One cluster represents countries that are poorer and have lower life expectancy, while the other includes those that are wealthier and healthier.\nNow let’s break it down by continent.\n\n\n\n\n\n\nChallenge\n\n\n\nAdd a color aesthetic to see how regions of the world are distributed\n\n\n\n(\nggplot(gapminder)+\ngeom_density_2d(mapping = aes(x='gdpPercap', y='lifeExp', color='continent'))+\nscale_x_log10()\n)\n\n\n\n\n\n\n\n\nIsn’t that fascinating? The lower cluster is primarily made up of African countries, while the higher cluster mostly includes Europe and Oceania. Asia? It’s scattered across both clusters, reflecting its diversity in economic and health outcomes. These exercises highlight the flexibility and power of plotnine. Whether it’s histograms, density plots, or advanced 2D density visualizations, each plot adds a new layer of understanding to your data."
  },
  {
    "objectID": "visualization.html#faceting",
    "href": "visualization.html#faceting",
    "title": "Visualization",
    "section": "Faceting",
    "text": "Faceting\nWhen your graph starts to feel a bit too crowded—perhaps with too many layers or overlapping aesthetics—there’s a simple solution: faceting. Faceting allows you to split your data into separate panels, creating multiple similar graphs for subsets of your data. This can make complex trends easier to spot and comparisons much clearer.\nIn plotnine, faceting is incredibly easy to use. Let’s revisit one of our earlier graphs and apply faceting to organize it by continent.\n\n(\nggplot(gapminder, mapping = aes(x = 'gdpPercap', y = 'lifeExp')) +\n  geom_point() +\n  geom_smooth(color=\"blue\") +\n  scale_x_log10() + \n  facet_wrap('continent')\n)\n\n\n\n\n\n\n\n\nHere’s what’s happening:\n\nfacet_wrap('continent') instructs plotnine to create a separate panel for each unique value in the continent column.\nPanels are arranged from left to right, and when they don’t fit on one row, they “wrap” onto the next line.\n\nThe result? A clean, organized set of charts where each panel highlights the GDP-per-capita and life expectancy trends for a specific continent. Faceting is especially helpful when the number of panels is manageable, and it lets us compare trends within each group side by side.\nLet’s take this idea further. What happens to the relationship between GDP per capita and life expectancy over time?\nTry faceting by year instead of continent.See if you can answer these questions\n\n\n\n\n\n\nQuestion\n\n\n\n\nDo the slopes of the trend lines change over the years?\nHow does the clustering of data points evolve as time progresses?\n\n\n\nThis exercise offers an incredible opportunity to see how historical events, global growth, and inequality have shaped the world over decades.\n\n\n\n\n\n\nChallenge\n\n\n\nFacet the following plot by year, keeping the linear smoother. You can edit this sample code\n(\nggplot(gapminder, mapping = aes(x = 'gdpPercap', y = 'lifeExp')) +\n  geom_point() +\n  geom_smooth(color='blue') +\n  scale_x_log10() + \n  facet_wrap('continent')\n)\n\n\n\n# facet by year\n(\nggplot(data = gapminder, mapping = aes(x = 'gdpPercap', y = 'lifeExp')) +\n  geom_point() +\n  geom_smooth(color='blue') +\n  scale_x_log10() + \n  facet_wrap('year')\n)\n\n\n\n\n\n\n\n\nWith everything we’ve learned so far, we can summarize the plotnine template as follows::\n(\nggplot(&lt;DATA&gt;) + \n  &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;)) + \n  &lt;SCALE_FUNCTION&gt; +\n  &lt;FACET_FUNCTION&gt;\n)\nFaceting is a powerful addition to your visualization toolkit, especially when your data has distinct groups or categories. Whether you’re analyzing trends over continents or time, faceting can make your insights clearer and more impactful.\nSo, go ahead—try faceting your own graphs. You’ll be amazed at what you uncover!"
  },
  {
    "objectID": "visualization.html#labeling-and-styling-the-chart",
    "href": "visualization.html#labeling-and-styling-the-chart",
    "title": "Visualization",
    "section": "Labeling and styling the chart",
    "text": "Labeling and styling the chart\nWe’ve built our chart layer by layer, and now it’s time to refine it for presentation—whether for your boss, a client, or publication. The final touches, like annotations and labels, can make all the difference in ensuring your audience understands your insights clearly.\nLet’s start with some practical data transformations. Instead of showing GDP per capita in raw numbers, wouldn’t it be better to express it in thousands of dollars? Similarly, population is easier to interpret when expressed in millions.\nWith plotnine, we don’t need to preprocess our data for this. You can specify transformations directly in your chart code. For example gdpPercap/1e3 divides GDP per capita by 1,000, and uses scientific notation (1e3) for convenience. Similarly, you can use pop/1e6 to show population in millions.\nTo make our chart clear and professional, we’ll use the labs() function. This function gathers all labels in one place, allowing us to customize:\n\nTitle and subtitle at the top,\nCaption at the bottom,\nLabels for x, y, and any mapped aesthetics, like color or size.\n\nHere’s an example of a polished, annotated chart:\n\n(\nggplot(data = gapminder) + \n  geom_point(mapping = aes(x = 'gdpPercap/1e3', y = 'lifeExp', size='pop/1e6', color='continent')) +\n  scale_x_log10() +\n  facet_wrap('year') + \n  labs(title=\"Life Expectancy vs GDP per capita over time\",\n        subtitle=\"In the past 50 years, life expectancy has improved in the world\",\n        caption=\"Source: Gapminder foundation, www.gapminder.org\",\n        x=\"GDP per capita, '000 USD\",\n        y=\"Life expectancy, years\",\n        color=\"Continent\",\n        size=\"Population, mln\")\n)\n\n\n\n\n\n\n\n\nEach label corresponds to the aesthetics used in the aes() mappings. Make sure all mapped aesthetics are labeled, even if they appear in just one layer.\nNow, let’s make your chart stand out! plotnine offers pre-selected themes that adjust the colors, fonts, and overall style of your plots.\nOne of my favorites is theme_minimal(). It simplifies the design, creating a clean and modern look:\n\n(\nggplot(data = gapminder) + \n  geom_point(mapping = aes(x = 'gdpPercap/1e3', y = 'lifeExp', size='pop/1e6', color='continent')) +\n  scale_x_log10() +\n  facet_wrap('year') + \n  labs(title=\"Life Expectancy vs GDP per capita over time\",\n        subtitle=\"In the past 50 years, life expectancy has improved in the world\",\n        caption=\"Source: Gapminder foundation, www.gapminder.org\",\n        x=\"GDP per capita, '000 USD\",\n        y=\"Life expectancy, years\",\n        color=\"Continent\",\n        size=\"Population, mln\") +\n  theme_minimal()\n)\n\n\n\n\n\n\n\n\nplotnine offers plenty of built-in themes to match your purpose:\n\ntheme_dark() for a sleek, high-contrast look.\ntheme_linedraw() for a simple, hand-drawn aesthetic.\ntheme_xkcd() for a playful, comic-style appearance.\ntheme_538() for a polished, professional newsroom feel.\n\nThemes contributed by the community can add even more variety. So, explore, experiment, and find the one that best suits your data story.\nCongratulations!\nYou’ve learned about aesthetics, scales, different types of geoms and now you also know how to annotate and apply themes to your visuals to make them more compelling. With these skills, you’re ready to create polished, professional-quality charts that truly stand out.\nGood luck, and we can’t wait to see the insights you’ll uncover!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Literacy with Python",
    "section": "",
    "text": "Welcome to Data Literacy with Python!\nLet me ask you something: Can you imagine living in today’s world but being unable to read? Think about it—street signs wouldn’t make sense, advertisements would just be noise, and most of the internet? Completely out of reach.\nNow, even with videos and voice assistants everywhere, written text is still the backbone of how we communicate and navigate life. Without it, you’d feel lost.\nBut here’s the thing: today’s world doesn’t just run on words. It runs on data.\nEvery day, we’re creating over 400 million terabytes of data. That’s every single day. And here’s a wild stat—90% of all the world’s data was created in just the last two years.\nThis explosion of information is transforming how we make decisions, whether it’s in business, science, or society as a whole. But here’s the catch: to keep up, you need to know how to make sense of it.\nData isn’t just numbers on a screen—it’s stories waiting to be uncovered. And understanding data has become just as important as being able to read or write.\nThat’s where this course comes in.\nWe’re going to teach you how to take raw, messy data and turn it into something meaningful. You’ll work with rectangular data—the kind you find in spreadsheets or databases.\nAnd don’t worry—this isn’t just about working with numbers. It’s about answering real-world questions, solving problems, and making decisions based on insights you uncover.\nBy the end of this course, you’ll have the skills to transform data into knowledge.\nLet’s talk about the tools you need to work with data.\nYou might be tempted by low-code or no-code solutions—those point-and-click interfaces that make everything seem so easy. And sure, they’re great for quick wins. But when it comes to serious data analysis, they have some big limitations.\nHere’s the thing: data analysis isn’t just about getting answers—it’s about getting credible answers.\nTo trust your insights, you need to leave a trail. Think about it—during analysis, you make dozens of tiny decisions:\nEvery decision shapes your results. And if you—or anyone else—can’t retrace those steps, how can you be sure your conclusions hold up?\nThat’s why scripting your analysis is so important.\nWith a script, every step is recorded. You can spot mistakes, refine your work, or pick up right where you left off—even months later. Low-code tools? They don’t give you that kind of transparency.\nSo, what’s the best language for scripting your data analysis?\nThe answer is Python.\nPython is the world’s most popular programming language, and for good reason. Created in 1990 by Guido van Rossum, Python has become the go-to language for everything from building websites to powering cutting-edge AI. It may not be the fastest language out there, but it’s arguably the most readable. And in today’s data-driven world, readability matters more than ever.\nThe Python ecosystem for data analysis is enormous. Whatever your question, there’s a good chance Python has a library—or ten—that can help.\nData analysis is unique—it’s less about traditional programming and more about crafting a story with your data. Your code should be clear and intuitive, not just for you, but for anyone who needs to understand your work.\nAnd that includes “future you”—because six months from now, you might not even recognize your own analysis without clear documentation!\nSo, as we dive into this course, we’ll emphasize simplicity, transparency, and readability. Because great analysis isn’t just about crunching numbers—it’s about telling a story that stands the test of time.\nData analysis is evolving.\nToday, some of the most cutting-edge tools are built on high-performance programming languages like Rust, Java, or C++. Why? Because these languages are fast—lightning fast. But here’s the best part: you don’t need to write in these languages to enjoy their benefits.\nModern tools now separate the user interface from the engine. That means the algorithms working behind the scenes are the same, no matter which scripting language you use.\nInitiatives like Apache Arrow go even further—they create standardized data formats, making it easy to move between tools and platforms without losing performance or compatibility.\nIn this course, we’re diving into tools built on Rust—one of the fastest, most efficient programming languages out there. Specifically, we’ll use uv for managing packages and environments and polars for data wrangling.\nThese tools are not just fast—they’re scalable.\nThe examples we’ll explore together are small—easy to follow and understand. But don’t let that fool you. The same tools we use here can scale effortlessly to handle datasets with billions of rows, processed across dozens of parallel machines.\nWhat’s even better? The interface doesn’t change.\nSo whether you’re working on a personal project, academic research, or a large-scale business application, the skills you gain here will translate directly to the real world.\nThe datasets may be small, but the questions and challenges we tackle are universal. By the end of this course, you’ll be equipped to uncover meaningful insights from your own data, no matter its size or complexity.\nLet’s get started on this exciting journey into the world of data literacy!"
  },
  {
    "objectID": "index.html#shell",
    "href": "index.html#shell",
    "title": "Data Literacy with Python",
    "section": "Shell",
    "text": "Shell\nBefore we dive into the thrilling world of data analysis, let’s pause and talk about something fundamental: your computer. That’s right, the device you’re watching this on is more than just a tool—it’s the backbone of your data analysis journey. Set it up well, and it will help you create reliable, reproducible, and stunning analytic projects.\nBut to unlock its full potential, we need to talk about something that might not look exciting at first glance but is incredibly powerful: the command line—also known as the shell or terminal.\nEvery operating system has some version of a terminal. Whether you’re on Windows, macOS, or Linux, this is your gateway to greater control over your computer. In the terminal, you don’t rely on menus, buttons, or icons. Instead, you type commands, giving you direct access to your system’s capabilities.\nNow, if this is new to you, don’t worry! Below this video, you’ll find instructions on how to locate and open the terminal on your operating system. Take a moment to familiarize yourself with launching it, and then meet me back here.\nOnce you’re ready, we’ll discuss folders—yes, the simple yet crucial task of organizing your files. If you already know how to navigate your operating system, create new folders, and understand the user directory, feel free to skip ahead to the next video. Otherwise, stick with me, and I’ll walk you through everything you need to know.\n\n\n\n\n\n\nChallenge\n\n\n\nFind and open the terminal on your computer. Take that first step toward becoming a power user!"
  },
  {
    "objectID": "index.html#on-files-and-folders",
    "href": "index.html#on-files-and-folders",
    "title": "Data Literacy with Python",
    "section": "On files and folders",
    "text": "On files and folders\nNow that you’ve found your terminal, it’s time to talk about something fundamental to every data analysis project: files and folders.\nThink about how your computer organizes everything you create or download—whether it’s a Word document, an Excel spreadsheet, or a photo. These files are stored in folders, and if you’ve ever saved something important, you know how critical it is to remember where it’s located.\nWhen working with data analysis, you’ll generate a lot of files—scripts, datasets, reports, visualizations—the list goes on. Keeping all of these files organized and in one place is essential to staying productive. You don’t want to waste time hunting through your computer every time you need something.\nHere’s the good news: your operating system already provides a great tool for organizing files—folders. Folders, or as they are sometimes called directories, are like containers, and they can even hold other folders. Picture this as a tree, with your main folder as the trunk and subfolders branching out.\nLet’s take a moment to explore this on your computer.\n\nOn Windows, you can open File Explorer by clicking the icon on your taskbar or pressing the Windows key + E. Once it’s open, you’ll see something like this:\n\n\n\nOn Mac, you’ll use an app called Finder. Just click its icon in the Dock, and you’ll see a view like this:\n\n\n\nOn Linux, it’s often just called Files, and the interface may vary depending on your distribution. On my Ubuntu setup, it looks like this:\n\n\nInside these interfaces, folders are easy to spot with their distinct icons. Click on one, and you’ll dive inside to see its contents—maybe more files, maybe more folders.\nNow, every operating system has a home base for your personal files. This is typically called your Home folder and contains directories like Music, Pictures, Videos, and Downloads. A special folder called Desktop displays its contents right on your screen.\nWhile it’s tempting to store everything on your Desktop for easy access, this isn’t the best long-term solution. A cluttered Desktop can make it harder to stay organized, and let’s be honest—it doesn’t look great either.\n\nInstead, consider creating a dedicated folder for your data analysis projects. For example, you could use your Documents folder or create a new folder called Projects. If your Documents folder is synced to the cloud—like with OneDrive or iCloud—think carefully about whether that’s the right place for large datasets. Cloud storage is precious, and you might want to save that space for smaller files like presentations or text documents.\nDon’t worry about backups just yet—we’ll cover that later when we dive into version control using Git and GitHub. For now, focus on picking a location that’s tidy, accessible, and works for you.\nOnce you’ve chosen your perfect home base, we’ll move on to the next step: learning how to navigate your files and folders using the command line. Trust me, it’s easier than it sounds, and it will make your workflow so much more efficient!"
  },
  {
    "objectID": "index.html#navigating-in-the-command-line",
    "href": "index.html#navigating-in-the-command-line",
    "title": "Data Literacy with Python",
    "section": "Navigating in the command line",
    "text": "Navigating in the command line\nAlright, let’s open up your terminal! Don’t worry—this isn’t as intimidating as it might look. I’m going to guide you through some simple steps that will help you make sense of this dark text window, and these skills will serve you every time you use the command line.\nFirst, take a look at your screen. See that little blinking line? That’s called the cursor, and it’s where you’ll type commands. To the left of the cursor, you’ll notice something else—this is called the system prompt. It’s a little symbol or combination of symbols, like $, &gt;, or %. Think of it as your computer saying, “I’m ready for your command!”\nBut before you start typing anything, let’s ask an important question: “Where am I?”\nWhen you open your terminal, it starts in a specific location in your computer’s file system. This location is called a directory, and it’s similar to a folder in your file explorer. But which folder or directory are you in right now?\nHere’s your first and perhaps most important command: pwd. It stands for “print working directory.” This command tells you the exact path of the folder your terminal is currently in.\nIf you’re using Windows, the equivalent command is cd—which we’ll use a bit differently later on, but for now, you can think of it as “current directory.”\nGo ahead and type pwd (or cd on Windows) and hit Enter. Your terminal will respond with something called a path. This is a series of folder names separated by forward slashes /, and on Windows, you might also see a drive letter like C: or D:.\nFor example, my terminal tells me this:\nDmytro@dell-xps-9570: $ pwd  \n/home/Dmytro  \nThis means my terminal has opened in my user directory, /home/Dmytro/. That’s the folder where my personal files are stored.\nNow that we know where we are, the next logical question is: “What’s here?”\nTo find out, use the command ls. This stands for “list,” and it will show you everything inside the current directory—both files and other directories.\nHere’s what my terminal shows when I type ls:\nDmytro@dell-xps-9570:~$ ls  \n 'Calibre Library'  Pictures           Projects  \n Desktop            Public             Videos  \n Documents          Downloads          Templates  \n Zotero             Music              Scanned_Document_House.pdf  \nAs you can see, I have a mix of directories and one lonely PDF document I prepared for the tax authorities. I recognize all of these files—they’re the same ones I’d see if I opened my Files app and navigated to my user directory.\nSo, in just two simple commands—pwd to figure out where you are and ls to see what’s inside—you’ve already gained some control over the terminal. Pretty cool, right?\nNow that you know how to figure out where you are on your computer, we’ll take it a step further and learn how to move around the file system using the terminal. Stay with me—you’re doing great!\nThe command you’ll need is cd, which stands for “change directory.” This is how you “step into” a folder or directory from the terminal.\nLet’s say you see a folder named Pictures in your current directory, and you want to navigate into it. All you have to do is type:\ncd Pictures\nHit Enter, and you’ll notice that the prefix—the text before the system prompt—changes. This tells you that you’re now inside the Pictures directory.\nWant to double-check where you are? Use the pwd command again, and it will confirm your new location. If you want to see what’s inside the Pictures folder, just type ls.\nBut what if you want to go back to where you were before? Easy! Just type:\ncd ..\nThe two dots (..) mean “go up one level.” Hit Enter, and you’ll be back in the directory you started from.\nLet me show you an example of a full roundtrip:\nDmytro@dell-xps-9570:~$ cd Pictures  \nDmytro@dell-xps-9570:~/Pictures$ pwd  \n/home/Dmytro/Pictures  \nDmytro@dell-xps-9570:~/Pictures$ cd ..  \nDmytro@dell-xps-9570:~$ pwd  \n/home/Dmytro  \nSee how simple that was? Using just cd and cd .., you can navigate up and down the directory tree.\nNow it’s your turn! Play around with these commands. Practice navigating into folders and back out again. There are faster ways to move around your file system, but for now, these basic steps are more than enough to get you started. Once you’re comfortable with these movements, please, take an effort to navigate to the directory, where you decided to store your project files.\n\n\n\n\n\n\nChallenge\n\n\n\nUsing cd, cd .. and ls, navigate to the directory, where you decided to store your project files.\n\n\nWhen you have done that, join me in the next section, where we’ll begin installing some software to set up your data analysis environment. I’ll see you there!"
  },
  {
    "objectID": "index.html#installing-uv",
    "href": "index.html#installing-uv",
    "title": "Data Literacy with Python",
    "section": "Installing uv",
    "text": "Installing uv\nAlright, now it’s time to roll up our sleeves and install one of the most exciting tools in modern Python development—uv!\nuv is an incredibly fast Python installation and environment manager, written in the high-performance language Rust. This tool burst onto the Python scene earlier this year, and it’s already winning the hearts of Python developers worldwide.\nWhen I say “fast,” I mean lightning fast—orders of magnitude faster than its popular counterparts! Just look at this chart:\n\nNot only is uv fast, but it’s also environmentally friendly. Yes, it’s saving the planet! By reducing the time and energy it takes for data centers to rebuild their test and development environments daily, uv contributes to a smaller carbon footprint.\nIn this course, we’ll only scratch the surface of what uv can do. But I promise, as you start using it in your real-life projects, your appreciation for this tool—and the Astral team behind it—will grow exponentially\nNow let’s get started with the installation. Open your terminal and type the following command. Hit Enter, and the installation should begin.\n\n\n\n\n\n\nChallenge\n\n\n\nOn Mac or Linux, use the following command after the system prompt:\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nOn Windown, type the following:\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n\n\nIf you already have Python installed, you can also install uv using one of Python’s existing package managers. On Mac, it’s even possible to install it through Homebrew. I’ll include a link to the official documentation with detailed instructions below this video.\n\n\n\n\n\n\nChallenge\n\n\n\nCheck out uv installation instructions at https://docs.astral.sh/uv/getting-started/installation/\n\n\nOnce the installation is complete, let’s check if everything worked. In your terminal, type uv. You should see output that looks something like this:\nuv\n\n#&gt; An extremely fast Python package manager.\n#&gt;\n#&gt; Usage: uv [OPTIONS] &lt;COMMAND&gt;\n#&gt; \n#&gt; Commands:\n#&gt;  run      Run a command or script\n#&gt;  init     Create a new project\n#&gt;  ...\nIf you see that, congratulations! You’ve successfully installed uv.\nFrom now on, we’ll use uv to manage everything, including installing Python itself. Stay with me as we continue setting up your computing environment. I’ll see you in the next section!"
  },
  {
    "objectID": "index.html#managing-python-installation",
    "href": "index.html#managing-python-installation",
    "title": "Data Literacy with Python",
    "section": "Managing python installation",
    "text": "Managing python installation\nuv isn’t just a package and environment manager—it also handles Python installations effortlessly. Let’s start by checking if uv can detect an existing Python installation on your system.\nIn your terminal, run:\nuv python find\nIf uv detects a Python installation, it will return the path to it. This path will also likely give you a clue about the Python version. But if you’re unsure about the version, you can use this command:\nuv python list --only-installed\nNow comes the decision point: Do you want to use your existing Python installation, or would you prefer a fresh installation managed entirely by uv?\nHere’s my advice: While you can use the Python installation that uv detects, it’s often a good idea to have a separate Python installation that’s fully under uv’s control. Why? Because Python is often deeply integrated into modern operating systems, and having an isolated installation ensures you don’t accidentally disrupt system-level functionality.\nPython environment management has historically been a mess! XKCD even made a famous comic about it.\n\nYeah, it can get pretty bad. But don’t worry—with uv, that nightmare is a thing of the past. If you’re ready to let uv manage your Python installation, run this command:\nuv python install\nThis will install the latest and greatest version of Python, which will now be entirely supervised by uv. If the installation completes without errors, you’re officially set up with Python! To verify, you can run the uv python find command again, and it should show the new Python installation.\nWith Python sorted, you’re ready for the next step: setting up your project environment.\nAs we discussed earlier, every data analysis project should live in its own folder. This keeps things tidy and avoids any cross-contamination between projects. Now, naming your project folder can feel like a challenge—it’s almost an art! Developers joke that naming things is one of the hardest parts of computer science. Why? Because you want a name that’s memorable, descriptive, and easy to type.\nSo, pick a short, meaningful name without spaces. I’ve decided to call my project data-literacy-project. Once you’ve chosen a name, make sure you’re in the directory where you want to create your project. You can double-check by running pwd (cd on Windows). Now, let’s initialize your project. Type the following, replacing data-literacy-project with your project’s name:\nuv init data-literacy-project\ncd data-literacy-project\nNote that the second command navigates you into the folder (remember, we learned cd foldername). That is because uv init has created a new directory for you with the name you picked. There’s one command I did not tell you about, which allows you to create directories manually. It is called mkdir which stands for “make directory”, followed by the directory name.\nIf you prefer, you could create the folder yourself and then initialize it with uv. Here’s how that would look:\n$ mkdir data-literacy-project\n$ cd data-literacy-project\n$ uv init\nEither way, once you’re inside the folder, it’s ready to go! You can explore what uv has created for you by running ls. One of the files you’ll see is pyproject.toml. This is your project’s manifest—a file where uv tracks metadata about your project, including its dependencies (the libraries it needs).\nSpeaking of dependencies, let’s add some libraries. Run the following commands:\n$ uv add gapminder plotnine polars pyarrow great_tables\n$ uv add --dev setuptools jupyter ipykernel pyyaml nbformat nbclient jupyterlab-quarto\nThese commands install several Python libraries we’ll use throughout the course. Here we separated the main packages we need for analysis and visualization (listed here in the first line), from the auxilliary packages used to set up a computing environment, the tools we are going to use. The idea is that your Python code relying on main dependencies may stay the same, while the computing setup may look completely different. In this course we will be using Jupyter notebooks, but the same analysis can be performed in Python script files in an IDE, such as VS Studio Code or Positron.\nAs you run these commands, you will notice that uv installs more packages than we requested. It pulls in the packages that these specified libraries rely on — it handles all the messy details of software dependecies for you! And thanks to uv, the installation will be lightning-fast.\nCurious about what just happened? I want to leave you with a fun little command: it’s called cat. Yes, like the furry little animal! The cat command is a handy way to view the contents of small files directly in your terminal. For larger files, it might take a while to load, but for our purposes, cat is perfect.\nLet’s use it to check out the contents of our project manifest file. In your terminal, type:\ncat pyproject.toml\nWhen you hit enter, you’ll see something like this:\n[project]\nname = \"data-literacy-project\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.10\"\ndependencies = [\n    \"gapminder&gt;=0.1\",\n    \"great-tables&gt;=0.15.0\",\n    \"plotnine&gt;=0.14.3\",\n    \"polars&gt;=1.17.1\",\n    \"pyarrow&gt;=18.1.0\",\n]\n\n[tool.uv]\ndev-dependencies = [\n    \"ipykernel&gt;=6.29.5\",\n    \"jupyter&gt;=1.1.1\",\n    \"jupyterlab-quarto&gt;=0.3.5\",\n    \"nbclient&gt;=0.10.1\",\n    \"nbformat&gt;=5.10.4\",\n    \"pyyaml&gt;=6.0.2\",\n    \"setuptools&gt;=75.6.0\",\n]\nThis file, pyproject.toml, serves as a “manifest” for your project. It keeps track of metadata like your project name, version, and—most importantly—dependencies. Each time you add a new library with uv add, it’s recorded here.\nNow, here’s something happening behind the scenes: uv created a hidden folder called .venv. This stands for “virtual environment,” and it’s where all your project’s libraries and dependencies live. Hidden files and folders don’t show up when you run ls— not unless you add the -a flag to the command, so ls -a will, in fact, show that you have this .venv folder in your project directory. But don’t worry that this folder stays hidden, you’ll rarely, if ever, need to open it. uv manages this folder for you, so you can focus on your work without getting bogged down in the details.\nHere’s the most important thing to know: the .venv folder is what makes your project self-contained. All your dependencies are stored there, safe and separate from anything else on your system. This is especially useful if you want to share your work later—a topic we’ll cover in the final module of this course."
  },
  {
    "objectID": "index.html#launch-jupyter",
    "href": "index.html#launch-jupyter",
    "title": "Data Literacy with Python",
    "section": "Launch Jupyter",
    "text": "Launch Jupyter\nFinally, let’s talk about Jupyter, one of the most transformative tools for interactive data analysis.\nJupyter is more than just a tool—it’s a revolutionary concept. It provides a cross-platform environment for literate programming, where code, text, and results coexist seamlessly. Oh, and here’s a fun fact: the name “Jupyter” stands for Julia-Python-R, highlighting its roots as a language-agnostic platform for fast prototyping, experimentation, and sharing results. Today, the Jupyter ecosystem has grown immensely, with thousands of educators, practitioners, and enthusiasts gathering annually at JupyterCon, a conference dedicated to this amazing technology.\nAt the heart of Jupyter is the Jupyter Notebook, a special format that allows you to write, execute, and save code, alongside text and rich results like visualizations. Behind the scenes, Jupyter maintains a live connection to the execution language—called a kernel. In our case, that’s Python.\nLuckily, the uv tool integrates Jupyter as one of its supported engines, making it simple to launch Jupyter Lab, the interactive environment where we’ll be working. To launch Jupyter Lab, type this command in your terminal:\nuv run --with jupyter jupyter lab\nWhen you run this command, your terminal will remain busy while Jupyter Lab is running. It’s a good idea to minimize the terminal and let it work in the background. Shortly after, your default web browser will open, displaying the Jupyter Lab interface.\nYou’ll see a screen like this:\n\nClick the large blue button in the top left corner—or press Ctrl-Shift-L (or Cmd-Shift-L on a Mac)—to open a Launcher.\n\nIn the Launcher, click the “Python 3 (ipykernel)” button under the “Notebook” section. This will open a new tab in Jupyter Lab, where you’ll see an empty document that looks like this:\n\nNow, let’s take a closer look at the Jupyter Notebook interface. A notebook is made up of cells, which can be either code cells or text cells. By default, the first cell is created for you, and it’s set to “code.” The cursor will already be placed in the cell, ready for you to start typing Python code.\nLet’s try a quick example!\n\n\n\n\n\n\nChallenge\n\n\n\nType the following code into the first cell:\nfrom datetime import date\nprint('Today is', date.today())\nPress Ctrl-Enter (or Cmd-Enter on a Mac) to execute the code.\n\n\nYou’ll see the output—today’s date—appear below the cell. Notice how the cell turns grey, indicating you’ve exited “edit mode” and are now in command mode.\nFrom command mode, you can do all sorts of things:\n\nPress B to insert a new cell below.\nPress A to insert a cell above.\n\nWhen you create a new cell, it won’t immediately be in edit mode. It’ll still be greyed out, waiting for further instructions.\nLet’s say we want to explain what the code does. For that, we can turn the new cell into a text cell. To change the cell type, press M (for “Markdown”). Markdown is a simple formatting language that lets you write text with styling.\nNow, place your cursor in the cell to start typing your thoughts. You can even add styling! For example:\n\nSurround text with single stars * text * for italics.\nUse double stars ** text ** to make text bold.\n\nTry it out!\nAdd a brief description of what the code does, and play around with text formatting. When you’re done editing text in a Markdown cell, press Ctrl-Enter (or Cmd-Enter on a Mac) to render it as rich text. Your formatting—like italics, bold text, or headings—will all be applied instantly.\n\n\n\n\n\n\nChallenge\n\n\n\nAdd a description of what the code does, use text formatting. Conclude with Ctrl-Enter (or Cmd-Enter on a Mac) to render it.\n\n\nIf you want to switch a Markdown cell back to code mode, it’s simple: press Y while in command mode. Now, the cell will behave like a code cell. Be cautious, though! If you type something invalid, the notebook will produce an error when you try to execute it.\nTo exit edit mode and return to command mode, press the Esc key on your keyboard. Once in command mode, you can navigate through cells using the arrow keys. This is helpful when you want to scroll through your notebook, locate a specific section, or insert new cells either above (A) or below (B).\nTo help you work more efficiently, here’s a pro tip: Jupyter Notebooks come with a host of useful shortcuts. Don’t worry, we’ll leave a list for you to refer to later.\nIn the next module, you’ll use this environment to create stunning data visualizations. While it’s technically possible to write all your code in a single cell, it’s not recommended. You’ll soon learn that breaking your code into smaller, modular sections makes debugging much easier. Plus, combining code cells with text cells lets you document your thoughts, findings, and learning process.\nThink of your notebook as exactly that—a notebook. Use it as a space for both experimentation and documentation. Interleave your code and text, and add section titles as you move through new chapters. In the final module, we’ll explore how to turn your notebook into a polished, professional report. So start structuring your work now, aiming for clarity and organization from the beginning.\nI encourage you to play around with Jupyter Notebook! Try simple arithmetic operations in code cells and add comments in Markdown cells below them. Experiment with switching cell types, adding formatting, and organizing your notebook.\n\n\n\n\n\n\nChallenge\n\n\n\n\nAdd a code cell, write Python code to perform simple arithmetic operations.\nAdd comments in a Markdown cell. Adding formatting\nAdd structure to your notebook using headers of different levels.\n\n\n\nWhen you’re ready to save your work, press Ctrl-S (or Cmd-S on Mac). Be sure to give your notebook a meaningful name and keep the file extension as .ipynb. This tells your system that the file is a Jupyter Notebook. I hope you have fun exploring this amazing tool! Jupyter notebooks are powerful, versatile, and central part of modern data analysis toolset.\nAnd that’s it! The boring setup part is officially behind us. From here on, it’s all about diving into the exciting world of data analysis. Onward and upward!"
  },
  {
    "objectID": "pivotjoin.html",
    "href": "pivotjoin.html",
    "title": "Pivots and joins",
    "section": "",
    "text": "import polars as pl\nimport polars.selectors as cs\nfrom plotnine import *\nfrom gapminder import gapminder\nfrom great_tables import GT\ntheme_set(theme_linedraw())"
  },
  {
    "objectID": "pivotjoin.html#pivot",
    "href": "pivotjoin.html#pivot",
    "title": "Pivots and joins",
    "section": "Pivot",
    "text": "Pivot\nEarlier we mentioned that data frame consists of observations organized in rows and variable organized in columns. But not all data comes as well-organized as wel would want it to be. Today we are going to look at the sample of the data collected by the Break from Plastics environmental campaign. Here’s the description of the data:\n\nIn 2020, thanks to our members and allies, Break Free From Plastic engaged 14,734 volunteers in 55 countries to conduct 575 brand audits. These volunteers collected 346,494 pieces of plastic waste, 63% of which was marked with a clear consumer brand. Despite the challenges of organizing during a global pandemic, our volunteers safely coordinated more brand audit events in more countries this year than in the previous two years. As a special activity during the pandemic, we also worked with over 300 waste pickers to highlight their roles as essential workers. Participants catalogued over 5,000 brands in this year’s global audit. Our analysis reveals the following as the 2020 Top 10 Global Polluters: The Coca-Cola Company; PepsiCo; Nestlé; Unilever; Mondelez International; Mars, Inc.; Procter & Gamble; Philip Morris International; Colgate-Palmolive; and Perfetti Van Melle.\n\nWe are going to investigate this dataset and summarize it using Great Tables. Great Tables is a python package which implements “grammar of tables”, so in many respects it is similar in spirit to plotnine, in a sense that it attempts to develop a consistent API around production of tables making it very easy to produce nice looking summaries form data in table form. The main function in Great Tables is GT(). It takes data argument and many other useful options. We can use .pipe to pass the data into it.\nIn the code example provided to you in the notebook you can see the code for importing the data, as well as the dataframe containing the data dictionary for this dataset. The data frame is specified via a dictionary denoted with curly braces {}. Dictionaries are widely used in Python for passing “key-value” pairs.\n\nplastics_df = pl.read_csv(\"bffp/BFFplastics.csv\")\n\nplastics_docs = pl.DataFrame({\n    \"Variable\": [\"region\", \"country_code\" , \"country\", \"year\", \"parent_company\", \"empty\", \"hdpe\", \"ldpe\", \"o\", \"pet\", \"pp\", \"ps\", \"pvc\", \"grand_total\", \"num_events\", \"volunteers\"],\n    \"Class\": [\"character\",\"character\",\"character\", \"double\", \"character\", \"double\", \"double\", \"double\", \"double\", \"double\", \"double\", \"double\", \"double\", \"double\", \"double\", \"double\"],\n    \"Description\": [\"Region\", \"Alpha 3 ISO 3166 code\",\"Country of cleanup\", \"Year (2019 or 2020)\", \"Source of plastic\", \"Category left empty count\", \"High density polyethylene count (Plastic milk containers, plastic bags, bottle caps, trash cans, oil cans, plastic lumber, toolboxes, supplement containers)\", \"Low density polyethylene count (Plastic bags, Ziploc bags, buckets, squeeze bottles, plastic tubes, chopping boards)\", \"Category marked other count\", \"Polyester plastic count (Polyester fibers, soft drink bottles, food containers (also see plastic bottles)\", \"Polypropylene count (Flower pots, bumpers, car interior trim, industrial fibers, carry-out beverage cups, microwavable food containers, DVD keep cases)\", \"Polystyrene count (Toys, video cassettes, ashtrays, trunks, beverage/food coolers, beer cups, wine and champagne cups, carry-out food containers, Styrofoam)\", \"PVC plastic count (Window frames, bottles for chemicals, flooring, plumbing pipes)\", \"Grand total count (all types of plastic)\", \"Number of counting events\", \"Number of volunteers\"]\n})\n\n(plastics_docs\n    .pipe(GT)\n)\n\n(plastics_df.describe())\n\n\nshape: (9, 17)\n\n\n\nstatistic\nregion\ncountry_code\ncountry\nyear\nparent_company\nempty\nhdpe\nldpe\no\npet\npp\nps\npvc\ngrand_total\nnum_events\nvolunteers\n\n\nstr\nstr\nstr\nstr\nf64\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"count\"\n\"12034\"\n\"12034\"\n\"12034\"\n13380.0\n\"13330\"\n10137.0\n11734.0\n11303.0\n13113.0\n13166.0\n11884.0\n11408.0\n9052.0\n13366.0\n13380.0\n13273.0\n\n\n\"null_count\"\n\"1346\"\n\"1346\"\n\"1346\"\n0.0\n\"50\"\n3243.0\n1646.0\n2077.0\n267.0\n214.0\n1496.0\n1972.0\n4328.0\n14.0\n0.0\n107.0\n\n\n\"mean\"\nnull\nnull\nnull\n2019.305232\nnull\n0.411759\n3.04602\n10.319384\n49.61359\n20.940301\n8.220801\n1.862114\n0.350088\n90.15083\n33.369806\n1117.645295\n\n\n\"std\"\nnull\nnull\nnull\n0.460523\nnull\n22.586066\n66.123044\n194.644067\n1601.989534\n428.157766\n141.805081\n39.737064\n7.894296\n1873.68134\n44.708642\n1812.402748\n\n\n\"min\"\n\"Africa\"\n\"ARE\"\n\"Argentina\"\n2019.0\n\"\"ESE\"\"\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n\"25%\"\nnull\nnull\nnull\n2019.0\nnull\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n4.0\n114.0\n\n\n\"50%\"\nnull\nnull\nnull\n2019.0\nnull\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n15.0\n400.0\n\n\n\"75%\"\nnull\nnull\nnull\n2020.0\nnull\n0.0\n0.0\n0.0\n2.0\n0.0\n0.0\n0.0\n0.0\n6.0\n42.0\n1416.0\n\n\n\"max\"\n\"Western Pacific\"\n\"ZAF\"\n\"Viet Nam\"\n2020.0\n\"脆司令/Cui Siling\"\n2208.0\n3728.0\n11700.0\n120646.0\n36226.0\n6046.0\n2101.0\n622.0\n120646.0\n145.0\n31318.0\n\n\n\n\n\n\nThere are many missing values in this data. The year column covers 2019 and 2020. The different plastics types exibit different degrees of missingness.\nFirst have a look at the data\n\nplastics_df\n\nplastics_df.filter(pl.col(\"country\").is_null())\n\n\nshape: (1_346, 16)\n\n\n\nregion\ncountry_code\ncountry\nyear\nparent_company\nempty\nhdpe\nldpe\no\npet\npp\nps\npvc\ngrand_total\nnum_events\nvolunteers\n\n\nstr\nstr\nstr\ni64\nstr\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\n\n\n\n\nnull\nnull\nnull\n2019\n\"Grand Total\"\nnull\n1535\n6443\n30181\n11087\n5420\n2101\n188\n56955\n145\n1416\n\n\nnull\nnull\nnull\n2019\n\"Unbranded\"\nnull\n631\n3176\n17432\n4265\n2417\n1545\n20\n29486\n145\n1416\n\n\nnull\nnull\nnull\n2019\n\"The Coca-Cola Company\"\nnull\n130\n4\n157\n1154\n210\n1\n51\n1707\n145\n1416\n\n\nnull\nnull\nnull\n2019\n\"Philip Morris\"\nnull\n13\n37\n1579\n1\n0\n4\n0\n1634\n145\n1416\n\n\nnull\nnull\nnull\n2019\n\"Sahakari Jal\"\nnull\n0\n0\n0\n563\n975\n0\n0\n1538\n145\n1416\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\nnull\nnull\nnull\n2019\n\"Schulte\"\nnull\n0\n0\n1\n0\n0\n0\n0\n1\n145\n1416\n\n\nnull\nnull\nnull\n2019\n\"Schwartau\"\nnull\n0\n1\n0\n0\n0\n0\n0\n1\n145\n1416\n\n\nnull\nnull\nnull\n2019\n\"SchÃ¶fferhofer\"\nnull\n0\n0\n1\n0\n0\n0\n0\n1\n145\n1416\n\n\nnull\nnull\nnull\n2019\n\"Scotts Black Mulch\"\nnull\n0\n1\n0\n0\n0\n0\n0\n1\n145\n1416\n\n\nnull\nnull\nnull\n2019\n\"Cherry Valley marketplace\"\nnull\n0\n0\n1\n0\n0\n0\n0\n1\n145\n1416\n\n\n\n\n\n\nLook at the very first row. The parent_company is Grand Total. It seems like this line contains the totals for Argentinian records for 2019.\nHowever, for year 2020, the rows with totals per country are not marked with “Grand Total”, but rather are populated with the missing value\n\n(plastics_df\n    .filter(pl.col(\"year\")==2020))\n\n\nshape: (4_084, 16)\n\n\n\nregion\ncountry_code\ncountry\nyear\nparent_company\nempty\nhdpe\nldpe\no\npet\npp\nps\npvc\ngrand_total\nnum_events\nvolunteers\n\n\nstr\nstr\nstr\ni64\nstr\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\n\n\n\n\n\"Americas\"\n\"ARG\"\n\"Argentina\"\n2020\nnull\n0\n12\n9\n70\n9\n7\n2\n0\n109\n24\n9\n\n\n\"Americas\"\n\"ARG\"\n\"Argentina\"\n2020\n\"Aceitera Martinez S.A\"\n0\n0\n0\n0\n1\n0\n0\n0\n1\n24\n9\n\n\n\"Americas\"\n\"ARG\"\n\"Argentina\"\n2020\n\"AGD\"\n0\n0\n0\n3\n1\n0\n0\n0\n4\n24\n9\n\n\n\"Americas\"\n\"ARG\"\n\"Argentina\"\n2020\n\"Alfredo Willliner S.A\"\n0\n0\n0\n0\n0\n0\n2\n0\n2\n24\n9\n\n\n\"Americas\"\n\"ARG\"\n\"Argentina\"\n2020\n\"Alicorp Argentina\"\n0\n0\n0\n0\n0\n1\n0\n0\n1\n24\n9\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Western Pacific\"\n\"VNM\"\n\"Viet Nam\"\n2020\n\"Vinamilk\"\n0\n0\n0\n91\n1\n0\n0\n0\n92\n6\n27\n\n\n\"Western Pacific\"\n\"VNM\"\n\"Viet Nam\"\n2020\n\"VINH HAO CO.\"\n0\n0\n0\n0\n4\n0\n0\n0\n4\n6\n27\n\n\n\"Western Pacific\"\n\"VNM\"\n\"Viet Nam\"\n2020\n\"Vital\"\n0\n0\n0\n0\n4\n0\n0\n0\n4\n6\n27\n\n\n\"Western Pacific\"\n\"VNM\"\n\"Viet Nam\"\n2020\n\"VM Group\"\n0\n2\n0\n0\n0\n0\n0\n0\n2\n6\n27\n\n\n\"Western Pacific\"\n\"VNM\"\n\"Viet Nam\"\n2020\n\"Yakult\"\n0\n0\n0\n0\n0\n2\n0\n0\n2\n6\n27\n\n\n\n\n\n\nBefore we dive deeper into analyzing top contributors to plastic waste, lets look at the totals per country. We now have two options. We could populate the rows containing the totals with the phrase “Grand Total” and then use it to filter. This assumes that totals are calculated properly for all countries. We will basically disregard the majority of the dataset and rely on few rows that have been pre-computed for us.\nAn alternative strategy could be to drop all rows with invalid company name (marked with either “Grand Total” or left missing “null”) and recompute totals by hand for each of the count columns.\n\nplastics_totals = (plastics_df\n    .drop(\"grand_total\", \"num_events\", \"volunteers\")\n    .filter((pl.col(\"parent_company\")!=\"Grand Total\"),\n            (pl.col(\"parent_company\").is_not_null()))\n    .group_by(\"country\", \"year\")\n    .agg(pl.exclude(\"country\", \"year\", \"parent_company\").drop_nulls().sum())\n    )\n\n\n# LEFT AS AN EXERCISE FOR THE USER\n# fill the missing values in the parent_company column with the phrase \"Grand Total\". \n# Since the parent company column is no longer informative, we can drop it.\nplastics_totals_1 = (plastics_df\n    .drop(\"grand_total\", \"num_events\", \"volunteers\")\n    .with_columns(pl.col(\"parent_company\").fill_null(\"Grand Total\"))\n    .filter(pl.col(\"parent_company\")==\"Grand Total\")\n    .drop(\"parent_company\")\n    )\n# What did we miss?\n(plastics_totals_1\n    .join(plastics_totals, on=[\"country\", \"year\"], how=\"anti\")\n    )\n\n(plastics_totals\n    .join(plastics_totals_1, on=[\"country\", \"year\"], how=\"anti\")\n    )\n\n# does not have row for the total\ntmp1_df = (plastics_df\n    .filter(pl.col(\"country\")==\"Slovenia\",\n            pl.col(\"year\")==2020)\n    )\n\n# does not have any records other than the total\ntmp2_df = (plastics_df\n    .filter(pl.col(\"country\")==\"United Arab Emirates\",\n            pl.col(\"year\")==2020)\n    )\n\ntmp2_df = (plastics_totals\n    .filter(pl.col(\"country\").is_null())\n    )\n\nWe do have grand_total column which seems to be a sum of columns empty:pvc describing counts for different type of plastic. Let’s say our goal is to make a chart describing the composition of trash by plastic type in different countries. In order to calculate proportions as we did previously, we need to have multiple observations for each country, one for each plastic type. Then we will calculate proportion over the country (and year).\n\nplastics_long_df = (plastics_df\n    .drop(\"grand_total\", \"num_events\", \"volunteers\")\n    .with_columns(pl.col(\"parent_company\").fill_null(\"Grand Total\"))\n    .filter(pl.col(\"parent_company\")==\"Grand Total\")\n    .drop(\"parent_company\")\n    #.unpivot(index=[\"country\", \"year\"], variable_name=\"plastic_type\", value_name=\"quantity\")\n    .unpivot(index=cs.by_name(\"country\", \"year\"), variable_name=\"plastic_type\", value_name=\"quantity\")\n    .with_columns(pl.col(\"quantity\").fill_null(0))\n    )\n# In `.unpivot()` the `on` argument is optional. Everything other than index\n\nPivot is making the data wider\n\n(plastics_long_df\n    .pivot(on=\"year\", values=\"quantity\")\n    )\n# In `.pivot()` the `index` is optional. All remaining columns not specified in `on` and `values` will be used.\n# either index or values need to be specified\n\n\nshape: (650, 4)\n\n\n\ncountry\nplastic_type\n2019\n2020\n\n\nstr\nstr\nstr\nstr\n\n\n\n\n\"Argentina\"\n\"region\"\n\"Americas\"\n\"Americas\"\n\n\n\"Australia\"\n\"region\"\n\"Western Pacific\"\n\"Western Pacific\"\n\n\n\"Bangladesh\"\n\"region\"\n\"South-East Asia\"\nnull\n\n\n\"Benin\"\n\"region\"\n\"Africa\"\n\"Africa\"\n\n\n\"Bhutan\"\n\"region\"\n\"South-East Asia\"\nnull\n\n\n…\n…\n…\n…\n\n\n\"Peru\"\n\"pvc\"\nnull\n\"1\"\n\n\n\"Romania\"\n\"pvc\"\nnull\n\"0\"\n\n\n\"Serbia\"\n\"pvc\"\nnull\n\"0\"\n\n\n\"Singapore\"\n\"pvc\"\nnull\n\"0\"\n\n\n\"Togo\"\n\"pvc\"\nnull\n\"0\"\n\n\n\n\n\n\n\n(plastics_long_df\n    .pivot(on=\"year\", index=\"country\", values=\"quantity\", aggregate_function='sum')\n    )\n\nplastics_campaigns_df = (plastics_long_df\n    .pivot(on=\"year\", index=[\"country\", \"plastic_type\"], values=\"quantity\")\n    .group_by(\"country\")\n    .agg(pl.all().exclude(\"country\", \"plastic_type\").fill_null(0).sum())\n)\n\n(plastics_campaigns_df    \n    .filter(pl.col(\"country\").is_in([\"Germany\", \"Netherlands\", \"Switzerland\", \"France\", \"Spain\", \"Italy\"]))\n    .pipe(GT)\n    .cols_label(\n        country=\"Country\")\n    .tab_spanner(\"Year\", columns=[\"2019\", \"2020\"])\n    .tab_header(\n        title = \"Break Free From Plastics campaigns\",\n        subtitle = \"Brand plastic counts in some European countries\")\n    .tab_source_note(\n        source_note=\"Source: BFFP data via TidyTuesday 2021-01-26\"\n    )\n)\n\n\n\n\n\n\n\nBreak Free From Plastics campaigns\n\n\nBrand plastic counts in some European countries\n\n\nCountry\nYear\n\n\n2019\n2020\n\n\n\n\nSpain\nNone\nNone\n\n\nGermany\nNone\nNone\n\n\nSwitzerland\nNone\nNone\n\n\nFrance\nNone\nNone\n\n\nItaly\nNone\nNone\n\n\n\nSource: BFFP data via TidyTuesday 2021-01-26"
  },
  {
    "objectID": "pivotjoin.html#stacking",
    "href": "pivotjoin.html#stacking",
    "title": "Pivots and joins",
    "section": "Stacking",
    "text": "Stacking\n\n# indoor air pollution\nhhap_deaths = pl.read_csv(\"hhap/hhap_deaths.csv\")\nclean_fuels = pl.read_csv(\"hhap/clean_fuels_cooking.csv\")\nfuel_types = pl.read_csv(\"hhap/cooking_by_fuel_type.csv\")\n\nAnother common type of operation is stacking two identical datasets together (vertically). This is possible to do when the meaning of the columns in the datasets is the same and we are interested in combining two parts of identical data into a new and larger dataset.\nRecall that in our household air pollution case study we had three files: - hhap_deaths - containing death cases, associated with air pollution - fuel_types - describing information about the fuels used by population in different countries for household needs - clean_fuels - containing the fraction of population in each country with access to clean fulels for cooking\nAll three of these datasets contain three identical columns describing the country of observation: region, country_code and country. The countries listed in each of the datasets is largely similar, but not completely overlapping. Let’s see if we can compile a single master set of all countries with the codes and the regions they belong to. Because the data is recorded over many years each of the datasets contains many duplicates entries. This problem will be even larger when we stack the data from several datasets together, so we will need to ensure the records in our final (combined) dataset are unique.\n\nidcols=cs.by_name(\"region\", \"country_code\", \"country\")\ncountry_regions = (hhap_deaths.select(idcols)\n    .vstack(fuel_types.select(idcols))\n    .vstack(clean_fuels.select(idcols))\n    .unique()\n)\n\nNote, that here we created a temporary object idcols, which will store only selector object for the three columns we are interested in. Polar selectors are independent entities which can live both inside the querying contexts as well as in the global environment, i.e. in memory accessible\nLets compare our country codes with the full list of codes issued by ISO. Here’s a file with all Alpha 2 and Alpha 3 codes issued to nation states and territories.\n\niso_df = pl.read_csv(\"hhap/CountryCodes_Alpha2_Alpha3.csv\")\n\n(country_regions\n    .join(iso_df, left_on=\"country_code\", right_on=\"alpha3\", how=\"anti\"))\n\n(country_regions\n    .join(iso_df, left_on=\"country_code\", right_on=\"alpha3\", how=\"left\"))\n\n# How many countries are not present in the combined household air pollution dataset? \n# What proportion of those countries have the world \"Island\" in their name?\n\ntmp_df1 = (iso_df\n    .join(country_regions, left_on=\"alpha3\", right_on=\"country_code\", how=\"anti\"))\n\n(iso_df\n    .join(country_regions, left_on=\"alpha3\", right_on=\"country_code\", how=\"anti\")\n    .select(pl.col(\"country\").str.contains(\"Island\").mean())\n    )\n\n(iso_df\n    .join(country_regions, left_on=\"alpha3\", right_on=\"country_code\", how=\"anti\")\n    .group_by(pl.col(\"country\").str.contains(\"Island\").alias(\"island\"))\n    .len()\n    .with_columns(pl.col(\"len\")/pl.sum(\"len\"))\n    .filter(\"island\")\n    )\n\n\nshape: (1, 2)\n\n\n\nisland\nlen\n\n\nbool\nf64\n\n\n\n\ntrue\n0.259259\n\n\n\n\n\n\nHorizontal stacking is possible, but you probably want to do a join instead, because horizontal stacking assumes that row order is the same and observations are identical. This is better ensured with unique IDs which could be used for join."
  }
]