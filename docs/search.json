[
  {
    "objectID": "wrangling.html",
    "href": "wrangling.html",
    "title": "Wrangling",
    "section": "",
    "text": "Welcome to the module on data wrangling! In the last lesson, we explored a small, but exciting gapminder dataset, and created quite a few visualizations with it. But in real-world scenarios, datasets are often much larger. This brings new challenges, like focusing on specific subsets of data—perhaps observations from a single time period or a selection of variables related to a particular phenomenon.\nIn this module, we’ll learn how to subset data and create meaningful summaries that provide a high-level overview of trends or differences between groups. Summarized data is often presented in tables, so we’ll also introduce a package for creating clear, professional-looking tables.\nMost importantly, we’ll dive into the blazingly fast Polars package for data manipulation. As I mentioned earlier, Polars is powered by Rust, a high-performance programming language. Its core functionality is exposed to Python but can also be accessed from other languages. This means the data wrangling skills you gain here will be transferable beyond Python.\nBut first, let’s load the necessary packages for this module. In Python, it’s common to use the alias pl for Polars. We’ll also use a submodule called polars.selectors, aliasing it as cs — don’t worry, we’ll cover selectors in more detail soon. We’ll also import everything from Plotnine for data visualization and bring in the main function from the great_tables package for generating beautifully looking tables.\nHere’s the setup code. Place it in its own cell in your notebook and run it:\n\nimport polars as pl\nimport polars.selectors as cs\nfrom plotnine import *\nfrom great_tables import GT\n\nNow, let’s explore our dataset. We’ll be looking at the WHO data about household pollution curated by the Our World In Data website.\nHousehold air pollution is primarily caused by burning polluting fuels like wood, animal dung, charcoal, agricultural waste, and kerosene in open fires or inefficient stoves. Globally, 2.1 billion people rely on these fuels for cooking, heating, and lighting. The poor combustion of these fuels leads to numerous health issues, such as pneumonia in children, and chronic diseases like obstructive pulmonary disease, lung cancer, stroke, and cardiovascular problems in adults.\nThis is a complex problem, and like many complex problems, it can be examined from different perspectives. We have three datasets to work with:\n\nCauses of death linked to household pollution\nTypes of fuel used for cooking in various countries\nProportion of the population with access to clean cooking fuels\n\nEach dataset offers a unique angle on this issue. Let’s dive in and start exploring!"
  },
  {
    "objectID": "wrangling.html#data",
    "href": "wrangling.html#data",
    "title": "Wrangling",
    "section": "",
    "text": "Welcome to the module on data wrangling! In the last lesson, we explored a small, but exciting gapminder dataset, and created quite a few visualizations with it. But in real-world scenarios, datasets are often much larger. This brings new challenges, like focusing on specific subsets of data—perhaps observations from a single time period or a selection of variables related to a particular phenomenon.\nIn this module, we’ll learn how to subset data and create meaningful summaries that provide a high-level overview of trends or differences between groups. Summarized data is often presented in tables, so we’ll also introduce a package for creating clear, professional-looking tables.\nMost importantly, we’ll dive into the blazingly fast Polars package for data manipulation. As I mentioned earlier, Polars is powered by Rust, a high-performance programming language. Its core functionality is exposed to Python but can also be accessed from other languages. This means the data wrangling skills you gain here will be transferable beyond Python.\nBut first, let’s load the necessary packages for this module. In Python, it’s common to use the alias pl for Polars. We’ll also use a submodule called polars.selectors, aliasing it as cs — don’t worry, we’ll cover selectors in more detail soon. We’ll also import everything from Plotnine for data visualization and bring in the main function from the great_tables package for generating beautifully looking tables.\nHere’s the setup code. Place it in its own cell in your notebook and run it:\n\nimport polars as pl\nimport polars.selectors as cs\nfrom plotnine import *\nfrom great_tables import GT\n\nNow, let’s explore our dataset. We’ll be looking at the WHO data about household pollution curated by the Our World In Data website.\nHousehold air pollution is primarily caused by burning polluting fuels like wood, animal dung, charcoal, agricultural waste, and kerosene in open fires or inefficient stoves. Globally, 2.1 billion people rely on these fuels for cooking, heating, and lighting. The poor combustion of these fuels leads to numerous health issues, such as pneumonia in children, and chronic diseases like obstructive pulmonary disease, lung cancer, stroke, and cardiovascular problems in adults.\nThis is a complex problem, and like many complex problems, it can be examined from different perspectives. We have three datasets to work with:\n\nCauses of death linked to household pollution\nTypes of fuel used for cooking in various countries\nProportion of the population with access to clean cooking fuels\n\nEach dataset offers a unique angle on this issue. Let’s dive in and start exploring!"
  },
  {
    "objectID": "wrangling.html#data-overview",
    "href": "wrangling.html#data-overview",
    "title": "Wrangling",
    "section": "Data Overview",
    "text": "Data Overview\nTo understand the data we’re working with, it’s helpful to preview it in a few ways. For instance, simply typing the name of a dataset—like clean_fuels—will display a preview of the first five and last five rows of the corresponding data frame.\nRows in a data frame are often referred to as observations or records, while columns are known as variables or features. If you want to see more rows than the default preview, you can use the .head() method and specify the number of rows to display:\n\n(clean_fuels  \n    .head(10))\n\n\nshape: (10, 6)\n\n\n\nregion\ncountry_code\ncountry\nyear\npop_clean_fuels_cooking_mln\nprop_clean_fuels_cooking_pct\n\n\nstr\nstr\nstr\ni64\nf64\nf64\n\n\n\n\n\"Africa\"\n\"SSD\"\n\"South Sudan\"\n2022\n0.0\n0.0\n\n\n\"Western Pacific\"\n\"NIU\"\n\"Niue\"\n2022\n0.002\n98.5\n\n\n\"Western Pacific\"\n\"TKL\"\n\"Tokelau\"\n2022\n0.0004\n28.3\n\n\n\"Western Pacific\"\n\"COK\"\n\"Cook Islands\"\n2022\n0.013\n72.7\n\n\n\"Western Pacific\"\n\"PLW\"\n\"Palau\"\n2022\n0.007\n29.45\n\n\n\"Africa\"\n\"STP\"\n\"Sao Tome and Principe\"\n2022\n0.009\n4.1\n\n\n\"Western Pacific\"\n\"FSM\"\n\"Micronesia (Federated States o…\n2022\n0.014\n13.2\n\n\n\"Africa\"\n\"BDI\"\n\"Burundi\"\n2022\n0.013\n0.1\n\n\n\"Western Pacific\"\n\"NRU\"\n\"Nauru\"\n2022\n0.011\n100.0\n\n\n\"Western Pacific\"\n\"TUV\"\n\"Tuvalu\"\n2022\n0.009\n75.2\n\n\n\n\n\n\nYou may have noticed the parentheses around the code block. This lets you write the code across multiple lines without worrying about indentation.\nPreviewing the first few rows gives you an initial sense of the dataset’s structure and content. If you’re curious about the last few rows, there’s also a .tail() method you can use in a similar way.\nFor a broader overview of your data, you can use the .describe() method:\n\n(clean_fuels  \n    .describe())\n\n\nshape: (9, 7)\n\n\n\nstatistic\nregion\ncountry_code\ncountry\nyear\npop_clean_fuels_cooking_mln\nprop_clean_fuels_cooking_pct\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\n\n\n\n\n\"count\"\n\"6402\"\n\"6402\"\n\"6402\"\n6402.0\n6402.0\n6402.0\n\n\n\"null_count\"\n\"0\"\n\"0\"\n\"0\"\n0.0\n0.0\n0.0\n\n\n\"mean\"\nnull\nnull\nnull\n2006.0\n19.29797\n61.080069\n\n\n\"std\"\nnull\nnull\nnull\n9.522648\n73.100935\n39.952764\n\n\n\"min\"\n\"Africa\"\n\"AFG\"\n\"Afghanistan\"\n1990.0\n0.0\n0.0\n\n\n\"25%\"\nnull\nnull\nnull\n1998.0\n0.23\n16.6\n\n\n\"50%\"\nnull\nnull\nnull\n2006.0\n2.24\n78.0\n\n\n\"75%\"\nnull\nnull\nnull\n2014.0\n10.24\n100.0\n\n\n\"max\"\n\"Western Pacific\"\n\"ZWE\"\n\"Zimbabwe\"\n2022.0\n1257.0\n100.0\n\n\n\n\n\n\nThe .describe() method provides a statistical summary of numerical columns, including metrics like the mean, standard deviation, minimum, maximum, and various quantiles. It’s especially useful for large datasets when you want to quickly understand key metrics.\nOne thing to note: if a column contains missing values, Polars will display these as null. Polars treats missing values as “contagious,” so any operation involving them will also result in missing values in the output. This behavior applies to all statistical operations. We’ll see more examples of this later.\nBoth .head() and .describe() return a data frame, which means you can chain these operations together. Method chaining is a powerful coding style that helps you write clean, readable, and maintainable code.\nHere are a couple of challenges for you to practice combining the functions into a method chain:\n\n\n\n\n\n\nChallenge\n\n\n\n\nTake first 25 records of clean_fuels data frame and then calculate statistical summary\nCompute statistical summary of the whole data and then present only quantile summaries of each column. The quantiles include minimum, maximum, as well as the 25th, 50th and 75th quantile.\n\n\n\n\n(clean_fuels\n    .head(25)\n    .describe())\n\n(clean_fuels\n    .describe()\n    .tail(5))\n\n\nshape: (5, 7)\n\n\n\nstatistic\nregion\ncountry_code\ncountry\nyear\npop_clean_fuels_cooking_mln\nprop_clean_fuels_cooking_pct\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\n\n\n\n\n\"min\"\n\"Africa\"\n\"AFG\"\n\"Afghanistan\"\n1990.0\n0.0\n0.0\n\n\n\"25%\"\nnull\nnull\nnull\n1998.0\n0.23\n16.6\n\n\n\"50%\"\nnull\nnull\nnull\n2006.0\n2.24\n78.0\n\n\n\"75%\"\nnull\nnull\nnull\n2014.0\n10.24\n100.0\n\n\n\"max\"\n\"Western Pacific\"\n\"ZWE\"\n\"Zimbabwe\"\n2022.0\n1257.0\n100.0\n\n\n\n\n\n\nSometimes, datasets have many columns, making it difficult to gain a full overview using methods like head() or describe(). For these cases, Polars provides a particularly useful method called glimpse().\nWhen you glimpse(), the dataset’s structure is displayed horizontally. Each variable is listed as a row, making it easier to scan through all columns, even if you’re working with a limited screen space. Here’s how it looks in action:\n\n(clean_fuels  \n    .glimpse())\n\nRows: 6402\nColumns: 6\n$ region                       &lt;str&gt; 'Africa', 'Western Pacific', 'Western Pacific', 'Western Pacific', 'Western Pacific', 'Africa', 'Western Pacific', 'Africa', 'Western Pacific', 'Western Pacific'\n$ country_code                 &lt;str&gt; 'SSD', 'NIU', 'TKL', 'COK', 'PLW', 'STP', 'FSM', 'BDI', 'NRU', 'TUV'\n$ country                      &lt;str&gt; 'South Sudan', 'Niue', 'Tokelau', 'Cook Islands', 'Palau', 'Sao Tome and Principe', 'Micronesia (Federated States of)', 'Burundi', 'Nauru', 'Tuvalu'\n$ year                         &lt;i64&gt; 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022\n$ pop_clean_fuels_cooking_mln  &lt;f64&gt; 0.0, 0.002, 0.0004, 0.013, 0.007, 0.009, 0.014, 0.013, 0.011, 0.009\n$ prop_clean_fuels_cooking_pct &lt;f64&gt; 0.0, 98.5, 28.3, 72.7, 29.45, 4.1, 13.2, 0.1, 100.0, 75.2\n\n\n\nNow, here’s a question for you: what happens if you try to use glimpse() in a method chain?\n\n\n\n\n\n\nQuestion\n\n\n\nCan you chain the operation head() after calling glimpse()? What do you think the output will be?\n\n\nThe answer is: no, you cannot. glimpse(), does not return you a data frame. Instead, the output of glimpse() is the text printout meant solely for viewing. No further operations can be applied to it. If you attempt to chain additional methods, you’ll encounter an error. Give it a try if you want! Polars will throw an error saying that the head() method cannot be applied to a NoneType, which is the type of output glimpse() returns.\nIf you understand how to use head(), tail(), describe(), and glimpse(), you have powerful tools at your disposal to explore and familiarize yourself with any dataset before diving deeper into your analysis."
  },
  {
    "objectID": "wrangling.html#selectdrop",
    "href": "wrangling.html#selectdrop",
    "title": "Wrangling",
    "section": "select/drop",
    "text": "select/drop\nOne of the most common tasks in data analysis is selecting specific variables or columns from a dataset. Let’s start by pulling out the country information from the clean_fuels data. Pause the video for a moment and try running this code:\n\n(clean_fuels  \n    .select(pl.col(\"country\")))\n\n\nshape: (6_402, 1)\n\n\n\ncountry\n\n\nstr\n\n\n\n\n\"South Sudan\"\n\n\n\"Niue\"\n\n\n\"Tokelau\"\n\n\n\"Cook Islands\"\n\n\n\"Palau\"\n\n\n…\n\n\n\"Austria\"\n\n\n\"Germany\"\n\n\n\"Sweden\"\n\n\n\"Portugal\"\n\n\n\"India\"\n\n\n\n\n\n\nHere, we’re using the select() method to isolate a column. Notice how the column name is wrapped in the pl.col() function. This wrapper explicitly tells Polars that we’re referring to a column in the dataframe.\nBut here’s something cool—you can skip the pl.col() wrapper in certain cases! For example, this code:\n\n(clean_fuels  \n    .select(\"country_code\", \"country\"))  \n\n\nshape: (6_402, 2)\n\n\n\ncountry_code\ncountry\n\n\nstr\nstr\n\n\n\n\n\"SSD\"\n\"South Sudan\"\n\n\n\"NIU\"\n\"Niue\"\n\n\n\"TKL\"\n\"Tokelau\"\n\n\n\"COK\"\n\"Cook Islands\"\n\n\n\"PLW\"\n\"Palau\"\n\n\n…\n…\n\n\n\"AUT\"\n\"Austria\"\n\n\n\"DEU\"\n\"Germany\"\n\n\n\"SWE\"\n\"Sweden\"\n\n\n\"PRT\"\n\"Portugal\"\n\n\n\"IND\"\n\"India\"\n\n\n\n\n\n\n…does the exact same thing as this:\n\n(clean_fuels  \n    .select(pl.col(\"country_code\"), pl.col(\"country\")))  \n\n\nshape: (6_402, 2)\n\n\n\ncountry_code\ncountry\n\n\nstr\nstr\n\n\n\n\n\"SSD\"\n\"South Sudan\"\n\n\n\"NIU\"\n\"Niue\"\n\n\n\"TKL\"\n\"Tokelau\"\n\n\n\"COK\"\n\"Cook Islands\"\n\n\n\"PLW\"\n\"Palau\"\n\n\n…\n…\n\n\n\"AUT\"\n\"Austria\"\n\n\n\"DEU\"\n\"Germany\"\n\n\n\"SWE\"\n\"Sweden\"\n\n\n\"PRT\"\n\"Portugal\"\n\n\n\"IND\"\n\"India\"\n\n\n\n\n\n\nPretty neat, right? The select() method can directly interpret strings as column names, making your code a little cleaner and quicker to write.\nWhen you wrap a column name in pl.col(), you’re creating an expression. An expression is like a recepie — it doesn’t do anything on its own. For example, if you run this code:\n\npl.col(\"country_code\")\n\ncol(\"country_code\")\n\n\n…nothing happens. It just returns something called an “unevaluated expression”. But when you evaluate that expression in the context of a dataset, it returns something useful. For instance:\n\n(clean_fuels  \n    .select(pl.col(\"country_code\")))  \n\n\nshape: (6_402, 1)\n\n\n\ncountry_code\n\n\nstr\n\n\n\n\n\"SSD\"\n\n\n\"NIU\"\n\n\n\"TKL\"\n\n\n\"COK\"\n\n\n\"PLW\"\n\n\n…\n\n\n\"AUT\"\n\n\n\"DEU\"\n\n\n\"SWE\"\n\n\n\"PRT\"\n\n\n\"IND\"\n\n\n\n\n\n\nHere, the select() method acts as an evaluation environment, turning the pl.col() expression into actual data.\n\n\n\n\n\n\nTip\n\n\n\nselect() is one of the several methods in Polars that can evaluate expressions. While select() is highly versatile and can do other things as well, for now, we’ll focus on its simplest use case: extracting columns from a data frame.\n\n\nThe pl.col() wrapper is super flexible, and it’s going to be central as we build more advanced expressions in Polars. For instance, you can use pl.col() to refer to multiple columns simultaneously:\n\n(clean_fuels  \n    .select(pl.col(\"country_code\", \"country\")))  \n\n\nshape: (6_402, 2)\n\n\n\ncountry_code\ncountry\n\n\nstr\nstr\n\n\n\n\n\"SSD\"\n\"South Sudan\"\n\n\n\"NIU\"\n\"Niue\"\n\n\n\"TKL\"\n\"Tokelau\"\n\n\n\"COK\"\n\"Cook Islands\"\n\n\n\"PLW\"\n\"Palau\"\n\n\n…\n…\n\n\n\"AUT\"\n\"Austria\"\n\n\n\"DEU\"\n\"Germany\"\n\n\n\"SWE\"\n\"Sweden\"\n\n\n\"PRT\"\n\"Portugal\"\n\n\n\"IND\"\n\"India\"\n\n\n\n\n\n\nSometimes, typing out long column names can feel like a chore, especially when you’re working with many columns. But don’t worry — Polars makes it easy to select columns by their position in the dataset. For example, this code selects the second and third columns by their numerical index:\n\n(clean_fuels  \n    .select(pl.nth(1,2)))  \n\n\nshape: (6_402, 2)\n\n\n\ncountry_code\ncountry\n\n\nstr\nstr\n\n\n\n\n\"SSD\"\n\"South Sudan\"\n\n\n\"NIU\"\n\"Niue\"\n\n\n\"TKL\"\n\"Tokelau\"\n\n\n\"COK\"\n\"Cook Islands\"\n\n\n\"PLW\"\n\"Palau\"\n\n\n…\n…\n\n\n\"AUT\"\n\"Austria\"\n\n\n\"DEU\"\n\"Germany\"\n\n\n\"SWE\"\n\"Sweden\"\n\n\n\"PRT\"\n\"Portugal\"\n\n\n\"IND\"\n\"India\"\n\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nColumn indices in Polars are 0-based. That means the first column is index 0, the second column is index 1, and so on.\n\n\nWhat about negative numbers? They’re a handy shortcut for selecting columns from the back of the dataset. For instance, -1 refers to the last column, and this code will select the first and last columns:\n\n(clean_fuels  \n    .select(pl.nth(0, -1)))  \n\n\nshape: (6_402, 2)\n\n\n\nregion\nprop_clean_fuels_cooking_pct\n\n\nstr\nf64\n\n\n\n\n\"Africa\"\n0.0\n\n\n\"Western Pacific\"\n98.5\n\n\n\"Western Pacific\"\n28.3\n\n\n\"Western Pacific\"\n72.7\n\n\n\"Western Pacific\"\n29.45\n\n\n…\n…\n\n\n\"Europe\"\n100.0\n\n\n\"Europe\"\n100.0\n\n\n\"Europe\"\n100.0\n\n\n\"Europe\"\n100.0\n\n\n\"South-East Asia\"\n11.1\n\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nA note of caution: Selecting columns by the order of their occurence can be risky. If your dataset’s structure changes, you might accidentally select the wrong columns. So, use the nth() function sparingly.\n\n\nNow, let’s talk about the opposite of select() — the drop() method. The drop() method removes specific columns from your dataset, leaving everything else intact. For example:\n\n(clean_fuels  \n    .drop(pl.col(\"country\"), pl.col(\"region\")))  \n\n\nshape: (6_402, 4)\n\n\n\ncountry_code\nyear\npop_clean_fuels_cooking_mln\nprop_clean_fuels_cooking_pct\n\n\nstr\ni64\nf64\nf64\n\n\n\n\n\"SSD\"\n2022\n0.0\n0.0\n\n\n\"NIU\"\n2022\n0.002\n98.5\n\n\n\"TKL\"\n2022\n0.0004\n28.3\n\n\n\"COK\"\n2022\n0.013\n72.7\n\n\n\"PLW\"\n2022\n0.007\n29.45\n\n\n…\n…\n…\n…\n\n\n\"AUT\"\n1990\n7.72\n100.0\n\n\n\"DEU\"\n1990\n79.12\n100.0\n\n\n\"SWE\"\n1990\n8.57\n100.0\n\n\n\"PRT\"\n1990\n9.95\n100.0\n\n\n\"IND\"\n1990\n96.58\n11.1\n\n\n\n\n\n\nDopping is equivalent to selecting all columns except the ones you want to exclude. Here’s how could would write it using in terms of selection:\n\n(clean_fuels  \n    .select(pl.all().exclude(\"country\", \"region\")))  \n\n\nshape: (6_402, 4)\n\n\n\ncountry_code\nyear\npop_clean_fuels_cooking_mln\nprop_clean_fuels_cooking_pct\n\n\nstr\ni64\nf64\nf64\n\n\n\n\n\"SSD\"\n2022\n0.0\n0.0\n\n\n\"NIU\"\n2022\n0.002\n98.5\n\n\n\"TKL\"\n2022\n0.0004\n28.3\n\n\n\"COK\"\n2022\n0.013\n72.7\n\n\n\"PLW\"\n2022\n0.007\n29.45\n\n\n…\n…\n…\n…\n\n\n\"AUT\"\n1990\n7.72\n100.0\n\n\n\"DEU\"\n1990\n79.12\n100.0\n\n\n\"SWE\"\n1990\n8.57\n100.0\n\n\n\"PRT\"\n1990\n9.95\n100.0\n\n\n\"IND\"\n1990\n96.58\n11.1\n\n\n\n\n\n\nThe pl.all() function refers to all columns, and the exclude() method lets you refine the selection by removing specific ones.\n\n\n\n\n\n\nNote\n\n\n\nA quick reminder: dropping columns doesn’t modify your original dataset. It only affects the result of that query. Unless you explicitly overwrite the original dataframe, everything stays the same. So feel free to experiment!\n\n\nNow it’s your turn. Select the columns related to the number and the proportion of the people with access to clean fuels. Try using both selection by name, by index, and dropping the columns you dont need.\n\n\n\n\n\n\nChallenge\n\n\n\nSelect the columns related to the number and the proportion of people with access to clean fuel for cooking from the clean_fuels dataset.\n\n\nPlease, pause the video and try couple of different ways of selecting these columns.\n\n(clean_fuels  \n    .select(pl.col(\"pop_clean_fuels_cooking_mln\"), pl.col(\"prop_clean_fuels_cooking_pct\"))) \n\n(clean_fuels  \n    .select(pl.nth(-2,-1))) \n\n(clean_fuels  \n    .drop(\"region\", \"country_code\", \"country\", \"year\"))\n\n\nshape: (6_402, 2)\n\n\n\npop_clean_fuels_cooking_mln\nprop_clean_fuels_cooking_pct\n\n\nf64\nf64\n\n\n\n\n0.0\n0.0\n\n\n0.002\n98.5\n\n\n0.0004\n28.3\n\n\n0.013\n72.7\n\n\n0.007\n29.45\n\n\n…\n…\n\n\n7.72\n100.0\n\n\n79.12\n100.0\n\n\n8.57\n100.0\n\n\n9.95\n100.0\n\n\n96.58\n11.1\n\n\n\n\n\n\nGot it? Great! Both approaches (selecting specific columns or dropping the ones you don’t need) give you the same result. Expressions like these make your analysis more dynamic and efficient, so you can quickly adapt to different datasets or scenarios."
  },
  {
    "objectID": "wrangling.html#selectors",
    "href": "wrangling.html#selectors",
    "title": "Wrangling",
    "section": "selectors",
    "text": "selectors\nSelecting columns is such a common task that Polars has a dedicated module for it. It is called polars.selectors. This module provides a collection of methods specifically designed to simplify picking columns from a data frame. polars.selectors is often aliased as cs for convenience.\n\n\n\n\n\n\nNote\n\n\n\nHave a look at the documentation for selectors.\n\n\nLet’s make sure we import the selectors module:\n\nimport polars.selectors as cs\n\nAmong the most useful selectors are, of course, selectors by name and by column index (for which we might not really need selectors, because those can be picked out with pl.col() and pl.nth()).\n\n(clean_fuels\n    .select(cs.by_name(\"region\", \"country\"))\n)\n\n# note python is 0-based\n(clean_fuels\n    .select(cs.by_index(0,2,5))\n)\n\n\nshape: (6_402, 3)\n\n\n\nregion\ncountry\nprop_clean_fuels_cooking_pct\n\n\nstr\nstr\nf64\n\n\n\n\n\"Africa\"\n\"South Sudan\"\n0.0\n\n\n\"Western Pacific\"\n\"Niue\"\n98.5\n\n\n\"Western Pacific\"\n\"Tokelau\"\n28.3\n\n\n\"Western Pacific\"\n\"Cook Islands\"\n72.7\n\n\n\"Western Pacific\"\n\"Palau\"\n29.45\n\n\n…\n…\n…\n\n\n\"Europe\"\n\"Austria\"\n100.0\n\n\n\"Europe\"\n\"Germany\"\n100.0\n\n\n\"Europe\"\n\"Sweden\"\n100.0\n\n\n\"Europe\"\n\"Portugal\"\n100.0\n\n\n\"South-East Asia\"\n\"India\"\n11.1\n\n\n\n\n\n\nSelecting first and last columns are so common, there are useful shortthands cs.first() and cs.last(). To select all columns other than the one you specified, you can use the tilde ~ operator. Tilde operator works with all methods in cs. module and negates the selection. For example ~cs.last() refers to all columns other than the last one.\nSelectors can target columns based on their data types! For example, cs.numeric() picks all numeric columns. And if you want non-numeric columns, you can just negate it with ~.\nAnd now it is your turn! Practice selecting first, everyhing other than the first, as well as all non-numeric columns. Use selector class for this. Pause the video and give it a try!\n\n\n\n\n\n\nChallenge\n\n\n\nUse polars.selectors aliased as cs. to select - first column - everyhing other than the first column - all non-numeric columns\n\n\n\n(clean_fuels\n    .select(cs.first())\n)\n\n# not first\n(clean_fuels\n    .select(~cs.first())\n)\n\n\n# not numeric\n(clean_fuels\n    .select(~cs.numeric())\n)\n\n\nshape: (6_402, 3)\n\n\n\nregion\ncountry_code\ncountry\n\n\nstr\nstr\nstr\n\n\n\n\n\"Africa\"\n\"SSD\"\n\"South Sudan\"\n\n\n\"Western Pacific\"\n\"NIU\"\n\"Niue\"\n\n\n\"Western Pacific\"\n\"TKL\"\n\"Tokelau\"\n\n\n\"Western Pacific\"\n\"COK\"\n\"Cook Islands\"\n\n\n\"Western Pacific\"\n\"PLW\"\n\"Palau\"\n\n\n…\n…\n…\n\n\n\"Europe\"\n\"AUT\"\n\"Austria\"\n\n\n\"Europe\"\n\"DEU\"\n\"Germany\"\n\n\n\"Europe\"\n\"SWE\"\n\"Sweden\"\n\n\n\"Europe\"\n\"PRT\"\n\"Portugal\"\n\n\n\"South-East Asia\"\n\"IND\"\n\"India\"\n\n\n\n\n\n\nFantastic work! With a wide menu of selector methods, plus column and index-based expressions like pl.col() and pl.nth(), Polars gives you incredible flexibility in working with your data. These tools will become invaluable as we move into crafting more complex expressions.\nStay tuned — there’s a lot more to explore!"
  },
  {
    "objectID": "wrangling.html#filter",
    "href": "wrangling.html#filter",
    "title": "Wrangling",
    "section": "Filter",
    "text": "Filter\nNow let’s talk about filtering — an essential part of data analysis. In Polars, you can use filtering to subset your dataset based on logical conditions, using the magic of expressions. Logical operations are one of the simplest and most common use cases for expressions. For example, you can compare every value in the region column to the string “Europe”. If there’s a match, Polars returns True; otherwise, it returns False.\nLet’s see how this works in code:\n\n(clean_fuels\n    .filter(pl.col(\"region\")==\"Europe\")\n)\n\n\nshape: (1_749, 6)\n\n\n\nregion\ncountry_code\ncountry\nyear\npop_clean_fuels_cooking_mln\nprop_clean_fuels_cooking_pct\n\n\nstr\nstr\nstr\ni64\nf64\nf64\n\n\n\n\n\"Europe\"\n\"SMR\"\n\"San Marino\"\n2022\n0.034\n100.0\n\n\n\"Europe\"\n\"MCO\"\n\"Monaco\"\n2022\n0.04\n100.0\n\n\n\"Europe\"\n\"FRO\"\n\"Faroe Islands\"\n2022\n0.05\n100.0\n\n\n\"Europe\"\n\"AND\"\n\"Andorra\"\n2022\n0.077\n100.0\n\n\n\"Europe\"\n\"ISL\"\n\"Iceland\"\n2022\n0.35\n100.0\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"Europe\"\n\"BLR\"\n\"Belarus\"\n1990\n7.48\n73.2\n\n\n\"Europe\"\n\"AUT\"\n\"Austria\"\n1990\n7.72\n100.0\n\n\n\"Europe\"\n\"DEU\"\n\"Germany\"\n1990\n79.12\n100.0\n\n\n\"Europe\"\n\"SWE\"\n\"Sweden\"\n1990\n8.57\n100.0\n\n\n\"Europe\"\n\"PRT\"\n\"Portugal\"\n1990\n9.95\n100.0\n\n\n\n\n\n\nHere, the filter() method applies the logical condition, and only rows where the region is “Europe” are included in the result. Notice that for exact comparisons, we use the double equals sign ==. Similarly, for inequalities, we can use operators like &lt;=, &gt;=, &lt;, or &gt;. Not equal is spelled out as !=.\nBut filtering doesn’t stop there — you can combine multiple conditions to create more complex filters.\nHere’s a challenge for you. Can you find all the countries in Europe where the majority of the population lacked access to clean fuels for cooking at the end of 2022? Take a moment to write this expression. Pause the video if you need to.\n\n\n\n\n\n\nChallenge\n\n\n\nWere there any countries in Europe in 2022 where the majority of people lacked access to clean fuel for cooking?\n\n\n\n(clean_fuels\n    .filter(\n        pl.col(\"region\")==\"Europe\",\n        pl.col(\"year\")==2022,\n        pl.col(\"prop_clean_fuels_cooking_pct\")&lt;50\n        )\n)\n\n\nshape: (1, 6)\n\n\n\nregion\ncountry_code\ncountry\nyear\npop_clean_fuels_cooking_mln\nprop_clean_fuels_cooking_pct\n\n\nstr\nstr\nstr\ni64\nf64\nf64\n\n\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n2022\n1.43\n41.1\n\n\n\n\n\n\nWhat did you get? Oh, wow! Over half the population of Bosnia still lacks access to clean fuels for cooking. That’s a powerful insight!\nLet’s zoom in on Bosnia to better understand the data. Bosnia’s country code is “BIH”, but you can also use country name to filter, if you prefer.\n\n(\nclean_fuels\n    .filter(pl.col(\"country_code\")==\"BIH\")\n)\n\n\nshape: (33, 6)\n\n\n\nregion\ncountry_code\ncountry\nyear\npop_clean_fuels_cooking_mln\nprop_clean_fuels_cooking_pct\n\n\nstr\nstr\nstr\ni64\nf64\nf64\n\n\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n2022\n1.43\n41.1\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n2021\n1.43\n40.9\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n2020\n1.43\n40.85\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n2019\n1.47\n42.05\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n2018\n1.43\n40.8\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n1994\n2.29\n58.0\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n1993\n2.41\n58.9\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n1992\n2.47\n58.25\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n1991\n2.6\n59.5\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n1990\n2.64\n59.2\n\n\n\n\n\n\nWe are interested in tracking how the proportion of the population with access to clean fuels for cooking has changed over the years. Let’s create a plot, with the year on the x-axis and the proportion of population on the y-axis.\nIf you remember from the Plotnine module, the dataset goes into the first argument of the ggplot function.\nHere’s one way to do this:\n\n(\nggplot(clean_fuels.filter(pl.col(\"country_code\")==\"BIH\"))\n    +geom_line(mapping=aes(x=\"year\", y=\"prop_clean_fuels_cooking_pct\"))\n)\n\n\n\n\n\n\n\n\nThis works, but the code feels a little cluttered. It’s not immediately clear where the dataset comes from.\nLet’s clean this up using the .pipe() method. The .pipe() method hands the data to the ggplot() function, placing it as the first argument.\n(data\n    .some_operation()\n    .other_operation()\n    .pipe(ggplot)\n    + ...)\n\n# the same as\ndf = data.some_operation().other_operation()\nggplot(df)\n+ ...\n\nThis keeps the code clean and modular. Everything after .pipe(ggplot) is Plotnine-specific code.\n\n(clean_fuels\n    .filter(pl.col(\"country_code\")==\"BIH\")\n    .pipe(ggplot)\n    +geom_line(aes(x=\"year\", y=\"pop_clean_fuels_cooking_mln\"))\n)\n\n\n\n\n\n\n\n\nNice!\nWhat if you’re not sure how a country’s name is spelled in the dataset? For example, is it “Czech Republic” or just “Czechia”?\nIn this case, you can use partial string matching to find it.\n\n(clean_fuels\n    .filter(pl.col(\"country\").str.starts_with(\"Cz\")))\n\n\nshape: (33, 6)\n\n\n\nregion\ncountry_code\ncountry\nyear\npop_clean_fuels_cooking_mln\nprop_clean_fuels_cooking_pct\n\n\nstr\nstr\nstr\ni64\nf64\nf64\n\n\n\n\n\"Europe\"\n\"CZE\"\n\"Czechia\"\n2022\n10.63\n100.0\n\n\n\"Europe\"\n\"CZE\"\n\"Czechia\"\n2021\n10.63\n100.0\n\n\n\"Europe\"\n\"CZE\"\n\"Czechia\"\n2020\n10.63\n100.0\n\n\n\"Europe\"\n\"CZE\"\n\"Czechia\"\n2019\n10.63\n100.0\n\n\n\"Europe\"\n\"CZE\"\n\"Czechia\"\n2018\n10.63\n100.0\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"Europe\"\n\"CZE\"\n\"Czechia\"\n1994\n10.36\n100.0\n\n\n\"Europe\"\n\"CZE\"\n\"Czechia\"\n1993\n10.36\n100.0\n\n\n\"Europe\"\n\"CZE\"\n\"Czechia\"\n1992\n10.35\n100.0\n\n\n\"Europe\"\n\"CZE\"\n\"Czechia\"\n1991\n10.35\n100.0\n\n\n\"Europe\"\n\"CZE\"\n\"Czechia\"\n1990\n10.34\n100.0\n\n\n\n\n\n\nHere, we use the str.starts_with() method, which checks if strings in the country column start with the letters “Cz.” Ah, there it is — “Czechia”!\nPolars offers several handy string operations. For example:\n\nstr.starts_with()\nstr.ends_with()\nstr.contains()\n\nYou’ll see more of these as we progress, but these three are powerful enough to help you tackle the following challenge.\nFilter the data for your own country and visualize the proportion of people with access to clean fuels. Once you’re happy with the subset of your data, use ggplot and everything you’ve learned about Plotnine to create a polished visualization.\n\n\n\n\n\n\nChallenge\n\n\n\nVisualize the proportion of people with access to clean fuels in your own country\n\n\n\n(clean_fuels\n    .filter(pl.col(\"country\").str.starts_with(\"Ukr\"))\n    .pipe(ggplot)\n    +geom_line(aes(x=\"year\", y=\"prop_clean_fuels_cooking_pct\"))\n    +theme_seaborn()\n    +labs(title=\"Share of population with access to clean fuels in Ukraine\",\n            x=\"Year\", y=\"Proportion, %\")\n)\n\n\n\n\n\n\n\n\nThis looks fantastic! Great work visualizing your country’s data.\nIn the next section, we’ll explore adding more columns to our dataset and practice advanced subsetting and visualization techniques. Stay tuned!"
  },
  {
    "objectID": "wrangling.html#more-practice-with-filtering",
    "href": "wrangling.html#more-practice-with-filtering",
    "title": "Wrangling",
    "section": "More practice with filtering",
    "text": "More practice with filtering\nLet’s apply what we’ve learned about filtering to visualize the causes of death in some European countries.\n\nhhap_deaths\n\n\nshape: (10_800, 6)\n\n\n\nregion\ncountry_code\ncountry\nyear\ncause_of_death\ndeaths\n\n\nstr\nstr\nstr\ni64\nstr\nf64\n\n\n\n\n\"Americas\"\n\"ATG\"\n\"Antigua and Barbuda\"\n2010\n\"All causes\"\n0.0\n\n\n\"Americas\"\n\"ATG\"\n\"Antigua and Barbuda\"\n2010\n\"Acute lower respiratory infect…\n0.0\n\n\n\"Americas\"\n\"ATG\"\n\"Antigua and Barbuda\"\n2010\n\"Trachea, bronchus, lung cancer…\n0.0\n\n\n\"Americas\"\n\"ATG\"\n\"Antigua and Barbuda\"\n2010\n\"Ischaemic heart disease\"\n0.0\n\n\n\"Americas\"\n\"ATG\"\n\"Antigua and Barbuda\"\n2010\n\"Stroke\"\n0.0\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"Americas\"\n\"BOL\"\n\"Bolivia (Plurinational State o…\n2019\n\"Trachea, bronchus, lung cancer…\n98.72\n\n\n\"Africa\"\n\"GNB\"\n\"Guinea-Bissau\"\n2019\n\"Chronic obstructive pulmonary …\n98.88\n\n\n\"Africa\"\n\"CIV\"\n\"Cote d'Ivoire\"\n2019\n\"Chronic obstructive pulmonary …\n990.6\n\n\n\"Europe\"\n\"TUR\"\n\"Türkiye\"\n2019\n\"Chronic obstructive pulmonary …\n997.6\n\n\n\"Europe\"\n\"UZB\"\n\"Uzbekistan\"\n2019\n\"All causes\"\n9982.0\n\n\n\n\n\n\nThis dataset contains both summarized and detailed breakdowns of deaths for every country and year. Take a look at the column labeled cause_of_death. When this column says “All causes,” it represents the total deaths for that country and year—a sum of all the other rows.\nLet’s zoom in on Bosnia for a single year, say 2010, to understand this better.\n\n(hhap_deaths\n    .filter(pl.col(\"country_code\")==\"BIH\", \n            pl.col(\"year\")==2010)\n    )\n\n\nshape: (6, 6)\n\n\n\nregion\ncountry_code\ncountry\nyear\ncause_of_death\ndeaths\n\n\nstr\nstr\nstr\ni64\nstr\nf64\n\n\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n2010\n\"Acute lower respiratory infect…\n147.2\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n2010\n\"Stroke\"\n1685.0\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n2010\n\"Ischaemic heart disease\"\n2067.0\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n2010\n\"Chronic obstructive pulmonary …\n365.2\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n2010\n\"All causes\"\n4816.0\n\n\n\"Europe\"\n\"BIH\"\n\"Bosnia and Herzegovina\"\n2010\n\"Trachea, bronchus, lung cancer…\n551.7\n\n\n\n\n\n\nOne of the rows is labeled “All causes” with 4,816 deaths. This total matches the sum of the individual causes of death. While it’s useful to have the total, it can lead to double counting if we include it in our analysis.\nNow let’s expand our view to include all European countries for which we have death data. We’ll exclude the totals and focus on trends for each specific cause of death. Faceting will help us visualize these trends country by country.\n\n(hhap_deaths\n    .filter(pl.col(\"region\")==\"Europe\",\n            pl.col(\"deaths\")&gt;0,\n            pl.col(\"cause_of_death\")!=\"All causes\")\n    .pipe(ggplot, aes(x=\"year\", y=\"deaths\", color=\"cause_of_death\", group=\"cause_of_death\"))\n    +geom_smooth(method=\"lm\")\n    +facet_wrap(\"country\", scales=\"free_y\", nrow=3)\n    +theme_seaborn()\n    +theme(figure_size=(20,10), legend_position=\"bottom\")\n    )\n\n\n\n\n\n\n\n\nMost of the trends appear to be decreasing, which is good news. However, even with free y-axis scales for each country, the differences in scale make it hard to compare trends across countries. Look at Moldova! There’s a dramatic improvement in death cases here. Meanwhile, heart- and stroke-related deaths in neighboring Russia are on the rise.\nIt would be good to put these numbers in perspective using population of these countries. Is this something we could calculate from our clean_fuels dataset?"
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "Welcome to the lesson on data visualization!\nData visualization is one of the most important skills in data analysis. Why? Because a well-made chart can reveal patterns, trends, and insights that might otherwise stay hidden in a spreadsheet. It’s like turning a jumble of numbers into a picture that tells a story.\nBut let’s be honest—visualizing data can sometimes feel overwhelming. There are so many types of charts to choose from and endless options for customizing them. Scatter plots, bar graphs, heatmaps—what should you use? And even after you pick a plot, there are all these parameters: axis, labels, color scales, gridlines… It’s easy to feel like you’re drowning in options!\nPoint-and-click tools for visualization, like those built into some software, can be helpful, but they come with their own challenges. They often overwhelm you with choices, and worse, they don’t always give you an easy way to reproduce or share your work.\nWhen you create visualizations using a script instead of a mouse, you unlock an entirely new level of power. Scripting your plots means they’re reproducible. You can tweak them, reuse them, and share the code with others. It’s like building a recipe that others can follow, modify, or inspect to understand exactly how the visualization was made.\nNow, let’s take a step back into history for a moment. In 1999, a statistician named Leland Wilkinson published a groundbreaking book called The Grammar of Graphics. Think of it like this: Just as grammar gives structure to language, Wilkinson’s framework gave structure to statistical graphics. He introduced a way to think about and construct plots systematically, rather than relying on intuition or tradition alone.\nHis ideas were revolutionary and influenced countless tools for making visualizations. One of the most famous examples is the R package ggplot2, created by Hadley Wickham. Wickham built on Wilkinson’s Grammar of Graphics and created what is now considered one of the most powerful and popular visualization tools in the world of data science.\nIn 2017 a passionate Python developer from Uganda by the name Hassan Kibridge ported ggplot2 into Python. His project became known as plotnine and it soon became a universal success. Here’s the story of plotnine in his own words:\n\nI discovered this “Grammar of Graphics” thing and found it elegant and powerful. So, I wanted to be able to use it in Python, my preferred programming language for data analysis. I read the key text on the subject by Leland Wilkinson and while I had a good grasp of it, translating it into a usable system would have been a huge undertaking. Hadley Wickham had done so for his doctorate and came up with ggplot2. While I was not confident enough to take this on, I felt that it was quite revolutionary and someone was going to implement it for Python.\nAnd it happened—a project came up that allowed people to seemingly make plots in Python using a grammar just like ggplot2. I started using it but soon discovered that it did not implement a grammar, it just faked one, and it broke down when you tried to make more complicated plots. Since this project was open source, I contributed to improving it. Yet to fix what was lacking grew into a complete overhaul, and this endeavor is what became Plotnine. The first release of which came out in July 2017 after about 3 years of on-and-off development.\nTo my surprise, Plotnine has been more successful than I imagined. For it, I have twice received the Google Open Source Peer Bonus Award—a recognition from Google employees towards open source software that is essential to their work. I have helped someone who was working on a COVID-19 vaccine trial solve a small problem they had run into with Plotnine. And, there is a book or two about it.\n\n\n\n\n\n\n\nTip\n\n\n\nHave a look at the full interview here as well as Hassan’s talk at the posit::conf(2023) in Chicago.\n\n\nWhile the syntax of plotnine might feel a bit different from typical Python code, don’t worry—there’s a reason for it! The goal is to keep the grammar intact, and that consistency makes it easy to learn and incredibly flexible to use.\nIn this lesson, we’ll dive into plotnine and explore how it allows you to create clear, beautiful, and insightful visualizations. We’ll guide you step by step so you can quickly become comfortable with its intuitive and expressive syntax.\nSo, join us on this journey into the wonderful world of data visualization. By the end, you’ll be creating plots that don’t just look good but also communicate your data’s story effectively. Let’s get started!"
  },
  {
    "objectID": "visualization.html#welcome",
    "href": "visualization.html#welcome",
    "title": "Visualization",
    "section": "",
    "text": "Welcome to the lesson on data visualization!\nData visualization is one of the most important skills in data analysis. Why? Because a well-made chart can reveal patterns, trends, and insights that might otherwise stay hidden in a spreadsheet. It’s like turning a jumble of numbers into a picture that tells a story.\nBut let’s be honest—visualizing data can sometimes feel overwhelming. There are so many types of charts to choose from and endless options for customizing them. Scatter plots, bar graphs, heatmaps—what should you use? And even after you pick a plot, there are all these parameters: axis, labels, color scales, gridlines… It’s easy to feel like you’re drowning in options!\nPoint-and-click tools for visualization, like those built into some software, can be helpful, but they come with their own challenges. They often overwhelm you with choices, and worse, they don’t always give you an easy way to reproduce or share your work.\nWhen you create visualizations using a script instead of a mouse, you unlock an entirely new level of power. Scripting your plots means they’re reproducible. You can tweak them, reuse them, and share the code with others. It’s like building a recipe that others can follow, modify, or inspect to understand exactly how the visualization was made.\nNow, let’s take a step back into history for a moment. In 1999, a statistician named Leland Wilkinson published a groundbreaking book called The Grammar of Graphics. Think of it like this: Just as grammar gives structure to language, Wilkinson’s framework gave structure to statistical graphics. He introduced a way to think about and construct plots systematically, rather than relying on intuition or tradition alone.\nHis ideas were revolutionary and influenced countless tools for making visualizations. One of the most famous examples is the R package ggplot2, created by Hadley Wickham. Wickham built on Wilkinson’s Grammar of Graphics and created what is now considered one of the most powerful and popular visualization tools in the world of data science.\nIn 2017 a passionate Python developer from Uganda by the name Hassan Kibridge ported ggplot2 into Python. His project became known as plotnine and it soon became a universal success. Here’s the story of plotnine in his own words:\n\nI discovered this “Grammar of Graphics” thing and found it elegant and powerful. So, I wanted to be able to use it in Python, my preferred programming language for data analysis. I read the key text on the subject by Leland Wilkinson and while I had a good grasp of it, translating it into a usable system would have been a huge undertaking. Hadley Wickham had done so for his doctorate and came up with ggplot2. While I was not confident enough to take this on, I felt that it was quite revolutionary and someone was going to implement it for Python.\nAnd it happened—a project came up that allowed people to seemingly make plots in Python using a grammar just like ggplot2. I started using it but soon discovered that it did not implement a grammar, it just faked one, and it broke down when you tried to make more complicated plots. Since this project was open source, I contributed to improving it. Yet to fix what was lacking grew into a complete overhaul, and this endeavor is what became Plotnine. The first release of which came out in July 2017 after about 3 years of on-and-off development.\nTo my surprise, Plotnine has been more successful than I imagined. For it, I have twice received the Google Open Source Peer Bonus Award—a recognition from Google employees towards open source software that is essential to their work. I have helped someone who was working on a COVID-19 vaccine trial solve a small problem they had run into with Plotnine. And, there is a book or two about it.\n\n\n\n\n\n\n\nTip\n\n\n\nHave a look at the full interview here as well as Hassan’s talk at the posit::conf(2023) in Chicago.\n\n\nWhile the syntax of plotnine might feel a bit different from typical Python code, don’t worry—there’s a reason for it! The goal is to keep the grammar intact, and that consistency makes it easy to learn and incredibly flexible to use.\nIn this lesson, we’ll dive into plotnine and explore how it allows you to create clear, beautiful, and insightful visualizations. We’ll guide you step by step so you can quickly become comfortable with its intuitive and expressive syntax.\nSo, join us on this journey into the wonderful world of data visualization. By the end, you’ll be creating plots that don’t just look good but also communicate your data’s story effectively. Let’s get started!"
  },
  {
    "objectID": "visualization.html#libraries",
    "href": "visualization.html#libraries",
    "title": "Visualization",
    "section": "Libraries",
    "text": "Libraries\nBefore we dive in, let’s talk about the tools and libraries we’ll be using in this lesson.\n\nimport polars as pl\nfrom plotnine import *\nfrom gapminder import gapminder\n\nFirst up is polars, the powerful data analysis library we’ll be relying on throughout the course. If you’re familiar with Python, you know it’s common practice to use shorthand or aliases when importing libraries. For polars, the standard alias is pl, so that’s what we’ll use here. Anytime we call a function from polars, it will be prefixed with pl. — simple and consistent.\nNow, when it comes to our visualization library, plotnine, we’ll take a slightly different approach. Instead of using a prefix, we’ll import all its functions directly into our workspace. This means we’ll use the from plotnine import * syntax, which essentially says, “Hey Python, bring in everything from plotnine!” Why? Because it makes our plotting code cleaner, easier to read, and more expressive.\nFinally, let’s talk about the dataset we’ll be exploring today. It comes from the gapminder package. If you’re not familiar with Gapminder, it’s a non-profit organization founded by Hans Rosling and his children back in 2005. Their mission? To promote a better understanding of global development through data—focusing on health, economics, and the environment.\nThe Gapminder Foundation maintains an incredible collection of statistics about the world, and this package is a small extract from their database. It’s packed with fascinating data on public health, economic development, and global welfare.\nHans Rosling himself is famous for his captivating TED Talk in 2007, where he used data to tell the story of global development. He spoke about life expectancy, GDP, and even the humble washing machine—and how it changed the world. If you haven’t watched that talk yet, I can’t recommend it enough. It’s a masterclass in how to make data come alive.\nSo, with our tools in hand and an inspiring dataset at our fingertips, we’re ready to start exploring and visualizing. Let’s get to it!"
  },
  {
    "objectID": "visualization.html#data",
    "href": "visualization.html#data",
    "title": "Visualization",
    "section": "Data",
    "text": "Data\nThe dataset has been conveniently imported for us by the gapminder package. To get started, we can simply type gapminder into our console and hit Enter. When we do that, we’ll see a preview of the data in the form of a table — what we call a DataFrame. In this table, each row represents an observation, and each column represents a variable.\n\ngapminder\n\n\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\n0\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.445314\n\n\n1\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.853030\n\n\n2\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.100710\n\n\n3\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.197138\n\n\n4\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.981106\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n1699\nZimbabwe\nAfrica\n1987\n62.351\n9216418\n706.157306\n\n\n1700\nZimbabwe\nAfrica\n1992\n60.377\n10704340\n693.420786\n\n\n1701\nZimbabwe\nAfrica\n1997\n46.809\n11404948\n792.449960\n\n\n1702\nZimbabwe\nAfrica\n2002\n39.989\n11926563\n672.038623\n\n\n1703\nZimbabwe\nAfrica\n2007\n43.487\n12311143\n469.709298\n\n\n\n\n1704 rows × 6 columns\n\n\n\nThis dataset has 1,704 rows and 6 columns, so it’s fairly compact but still rich with information. Let’s walk through what each of these columns means:\n\ncountry: This column lists the names of countries. If you take a look at the data, you’ll notice it starts with Afghanistan at the top and ends with Zimbabwe at the bottom. It seems the data is sorted alphabetically by country.\ncontinent: Here, we have the names of continents. For example, Afghanistan is listed under Asia, while Zimbabwe is under Africa. Makes perfect sense.\nyear: This column tells us the year of the observation. You’ll notice that each country has multiple rows because data was collected at different times. The dataset starts in 1952 and progresses in 5-year increments, which gives us a nice snapshot of changes over time.\nlifeExp: This column stands for life expectancy at birth. If we look at Afghanistan in 1952, for example, the life expectancy was just 28.8 years. Let that sink in for a moment — only 28 years! It’s a sobering reminder of the challenges some nations faced in the mid-20th century.\npop: This column shows the population of each country. Again, looking at Afghanistan in 1952, the population was just under 8.5 million people.\ngdpPercap: Finally, this column contains the GDP per capita, expressed in US dollars. From what I understand, these figures have been adjusted for inflation, so they should be comparable across countries and over time.\n\nAltogether, these six columns give us a fascinating lens through which to explore global trends in health, wealth, and population growth. The dataset might look simple at first glance, but it’s packed with stories waiting to be uncovered.\nNow that we know what we’re working with, let’s roll up our sleeves and start exploring!"
  },
  {
    "objectID": "visualization.html#first-plot",
    "href": "visualization.html#first-plot",
    "title": "Visualization",
    "section": "First plot",
    "text": "First plot\nNow, it’s time to create our very first plot! Here is a question we would like to answer using gapminder data:\n\n\n\n\n\n\nQuestion\n\n\n\nDo people in rich countries live longer than people in poor countries?\n\n\nThe answer may be quite intuitive, but we will continue our investigation further\n\n\n\n\n\n\nQuestion\n\n\n\n\nHow does the relationship between GDP per capita and Life expectancy look like? Is this relationship linear? Non-linear?\nAre there exceptions to the general rule (outliers)?\n\n\n\nIn order to answer these questions, we will create a plot from gapminder data. Here’s the code we’ll use. Take a moment to copy this code verbatim from your screen.\nWhen writing Python code with plotnine — and later when we use polars — you’ll notice that we often wrap our code in parentheses. This is a great habit to get into because it allows us to break our code into multiple lines without worrying about indentation.\n\n(\nggplot(gapminder)+\ngeom_point(mapping=aes(x='gdpPercap', y='lifeExp'))\n)\n\n\n\n\n\n\n\n\nLet’s walk through this step by step.\nInside the outer parentheses, the first thing we write is ggplot. This is the foundational function in plotnine, and it stands for Grammar of Graphics plot. Then, in parentheses again, we pass the dataset we want to use — in this case, gapminder.\nAfter that, we add a plus sign. The Grammar of Graphics, which plotnine is built on, thinks of plots as being made up of layers. The + sign we added tells Python that we’re adding more layers or components to our plot. Think of it as saying, “Wait, there’s more!”\nOn the next line, we write geom_point. This is the function that specifies the type of layer we’re adding to our plot. In this case, it’s a point plot, which means we’ll be drawing points on a graph. Without this layer, our plot would just be an empty canvas.\nInside the geom_point parentheses, we specify the argument: mapping followed by an = sign. This tells plotnine how we want to relate our data to the graph. AES stands for “aesthetics”. The inside of the aes function defines mapping of the variables in our data to certain aesthetical properies of our graph. We’re saying, “Take the GDP per capita (gdpPercap) and map it to the x-axis, and take life expectancy (lifeExp) and map it to the y-axis.” Notice that the column names are enclosed in quotes — that’s important!\nOnce you’ve written the code, go ahead and hit the Run button. If everything is correct, you should see your plot appear on the screen.\nLet’s take a moment to reflect on what we just did.\nIn our code, the first layer was the ggplot function, where we provided the dataset. The second layer was geom_point, which added points to our graph.\nThe result is a simple yet meaningful scatter plot. It shows a positive, non-linear relationship between GDP per capita on the x-axis and life expectancy on the y-axis. Does this align with what you initially expected? Or does it challenge your assumptions? Already, you can see how visualizing data helps uncover patterns and stories that might not be obvious at first glance.\nTake your time to review the code and compare it to the plot we created. Understanding this connection — how the code you write translates directly into what you see on the screen — is the key to mastering data visualization.\nIn fact, the structure of most plots in plotnine (and its R counterpart, ggplot2) can be summarized with a simple template:\nggplot(data = &lt;DATA&gt;) + \n  &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;))\nThis template is incredibly flexible and serves as the foundation for almost every visualization we’ll create.\nIn the remainder of this lesson, we’ll explore how to extend and customize this template to create a wide variety of visualizations. Each new element we add will open up even more possibilities.\nI’ll see you in the next one!"
  },
  {
    "objectID": "visualization.html#axis",
    "href": "visualization.html#axis",
    "title": "Visualization",
    "section": "Axis",
    "text": "Axis\nHello again! Ready for a challenge? I’ve got a question for you:\n\n\n\n\n\n\nChallenge\n\n\n\nHow has life expectancy changed over time?\n\n\nTake a moment to think about it. Better yet, try answering it by modifying the code we wrote in the last lesson.\nHere’s a quick hint before you pause the video: The gapminder dataset includes a column called year, which can go on the x-axis. Use that to tweak the code and see what you find. I’ll wait right here while you try it out!\nPause the video now and give it a shot. See you in a moment!\n\n\n(\nggplot(gapminder)+\ngeom_point(mapping = aes(x='year', y='lifeExp'))\n)\n\n\n\n\n\n\n\n\nDone? Excellent! Let’s take a look at what we’ve got. Nice work! You should see a scatter plot showing life expectancy over time.\nHmm… notice how some of the points are stacked on top of each other? That’s called overplotting, and it’s pretty common when you have a lot of data points at the same x or y values. Don’t worry—it’s easy to fix!\nInstead of geom_point, try using geom_jitter. This will add a tiny bit of random noise to spread out the points so they’re easier to see.\nHere’s how you do it:\n\n(\nggplot(gapminder)+\ngeom_jitter(mapping = aes(x='year', y='lifeExp'))\n)\n\n\n\n\n\n\n\n\nRun this code and check out the difference. Much better, right? Now we can see the points more clearly.\nLet’s keep going with this little game. Here’s your next challenge:\n\n\n\n\n\n\nChallenge\n\n\n\nCan you visualize life expectancy by continent?\n\n\nThink about which variable should go on the x-axis this time. Which continent do you think tends to have the highest life expectancy? Modify your code and give it a shot. Pause the video, try it out, and come back when you’re ready.\n\n\n(\nggplot(gapminder)+\ngeom_jitter(mapping = aes(x='continent', y='lifeExp'))\n)\n\n\n\n\n\n\n\n\nGreat job! What do we see here? Looks like life expectancy in Oceania is quite high, although there aren’t many points for that region. Europe is a close second. On the other hand, Africa seems to have the lowest life expectancy overall, judging by the density of points at the lower end of the y-axis.\nHere’s another question: Which continent has the widest spread in life expectancy values? That’s right—it’s Asia. There’s quite a bit of variation there, which is something we’ll dig into in more detail later in the course.\nFantastic work so far! Take a moment to review what you’ve done, and I’ll see you in the next section!"
  },
  {
    "objectID": "visualization.html#aestetical-mapping",
    "href": "visualization.html#aestetical-mapping",
    "title": "Visualization",
    "section": "Aestetical mapping",
    "text": "Aestetical mapping\n&lt;…Walks in, looking thoughtful….&gt;\nOh, hey there! You know, I’ve been thinking—what if we could combine the graphs from the last two challenges and show the relationship between not just two variables, but three?\nNow, don’t worry—we’re not diving into “three-dimensional” plots just yet. Instead, we can represent a third variable using color. Let me show you what I mean.\nHere’s modified code that maps the continent variable to the color aesthetic:\n\n(\nggplot(gapminder)+\ngeom_jitter(mapping = aes(x='year', y='lifeExp', color='continent'))\n)\n\n\n\n\n\n\n\n\nRun this code and take a look.\n\nWhat do you see? Now we can see more clearly how life expectancy has changed over time by continent. For example, the points representing Africa stay clustered near the lower end of the y-axis throughout the years, while Europe’s points are generally higher. Oceania is there too, but it’s barely noticeable because there are so few observations. Pretty cool, right?\nNow, I’ve got a question for you: What happens if we switch the mappings of continent and year? Give it a try!\n\n\n\n\n\n\nChallenge\n\n\n\nSwitch the mappings of continent and year in this sample code\n(\nggplot(gapminder)+\ngeom_jitter(mapping = aes(x='year', y='lifeExp', color='continent'))\n)\n\n\n\n# switch aesthetics\n(\nggplot(gapminder)+\ngeom_jitter(mapping = aes(x='continent', y='lifeExp', color='year'))\n)\n\n\n\n\n\n\n\n\n\nDone? Great! Do you still find this graph useful? Why or why not?\nNow let’s tweak it a bit more. What if, instead of mapping color to year, we mapped it to country? Give it a try!\n\n\n\n\n\n\nChallenge\n\n\n\nMap color to country in this sample code:\n(\nggplot(gapminder)+\ngeom_jitter(mapping = aes(x='continent', y='lifeExp', color='year'))\n)\n\n\n\n# color by country\n(\nggplot(gapminder)+\ngeom_jitter(mapping = aes(x='continent', y='lifeExp', color='country'))\n)\n\n\n\n\n\n\n\n\nWhat changed? How does mapping color to country differ from mapping it to year? Take a moment to think about it. What do you think is the main limitation of using the color aesthetic?\nAlright, here’s one last challenge for this section: Can you add a splash of color to our original graph of life expectancy by GDP per capita? Let’s color the points by continent.\n\n\n\n\n\n\nChallenge\n\n\n\nColor the points by continent in this sample code:\n(\nggplot(gapminder) +\ngeom_point(mapping = aes(x='gdpPercap', y='lifeExp'))\n)\n\n\nTake a moment to run your code and see what you get.\n\n(\nggplot(gapminder)+\ngeom_point(mapping = aes(x='gdpPercap', y='lifeExp', color='continent'))\n)\n\n\n\n\n\n\n\n\n\nAmazing! By adding color, we can now spot trends and patterns more easily. But did you notice something else? There are a few outliers in this plot. Can you tell which continent those points belong to?\nThe points look a little crowded. But you know, you can always transform GDP per capita to a logarithmic scale for better visualization. Just add scale_x_log10() as an additional layer to your graph, like that:\n\n# transform the scales\n(\nggplot(gapminder)+\ngeom_point(mapping = aes(x='gdpPercap', y='lifeExp', color='continent'))+\nscale_x_log10()\n)\n\n\n\n\n\n\n\n\nYou’re making fantastic progress! Well done! In the next section, we’ll explore even more aesthetics that can help us tell richer stories with our data. See you there!"
  },
  {
    "objectID": "visualization.html#more-aesthetics",
    "href": "visualization.html#more-aesthetics",
    "title": "Visualization",
    "section": "More aesthetics",
    "text": "More aesthetics\nHello again!\nSo far, we’ve explored some powerful ways to visualize data using the x, y, and color aesthetics. With these, we’ve been able to represent three variables in a single plot. Pretty amazing, right?\nNow, let’s quickly recap what we’ve learned about the color aesthetic. When we map a categorical variable like continent to color, plotnine automatically picks a distinct palette for each category. This works great when there are just a few categories, but as the number of categories grows, the colors start to blur together and lose their effectiveness.\nOn the other hand, when we map a continuous variable like year to color, we get a gradient. While individual values can be harder to pinpoint, the overall trends are beautifully highlighted by the gradient’s brightness.\nAlright, as promised, let me introduce you to another fantastic aesthetic: size.\nImagine we could vary the size of the points in our graph to represent something meaningful—like the population of a country. That would let us visualize not three, but four variables at the same time. Let’s give it a shot. Here’s the code:\n\n(\nggplot(gapminder)+\ngeom_point(mapping = aes(x='gdpPercap', y='lifeExp', color='continent', size='pop'))+\nscale_x_log10()\n)\n\n\n\n\n\n\n\n\nWRun this and take a moment to appreciate the result. Isn’t it gorgeous?\nNow we can see the journey of countries like China and India over time. Their points stand out because of their large populations. Under the logarithmic transformation of the x-axis, the relationship between GDP per capita and life expectancy starts to look more linear—but not quite!\nNotice the outliers on the far right? They all seem to be from Asian countries. Are these countries rich or poor? Rich, right? But their life expectancy doesn’t quite follow the trend we see in Europe or the Americas. Fascinating, isn’t it?\nNow, let me share one more aesthetic property with you: shape.\nShape can be a great tool for visualizing low-cardinality categorical variables, like continent. Instead of just using circles, we can use distinct shapes for each category. This lets us pack even more information into the same graph.\nReady for a challenge? Let’s push the limits and visualize five dimensions in a single plot. Modify the previous example to map year to color and continent to shape. Take a moment and try it. I’ll wait.\n\n\n\n\n\n\nChallenge\n\n\n\nMap year to color and continent to shape in this sample code\n(\nggplot(gapminder)+\ngeom_point(mapping = aes(x='gdpPercap', y='lifeExp', color='continent', size='pop'))+\nscale_x_log10()\n)\n\n\n\n\n# map year to color and continent to shape\n\n(\nggplot(gapminder)+\ngeom_point(mapping = aes(x='gdpPercap', y='lifeExp', color='year', size='pop', shape='continent'))+\nscale_x_log10()\n)\n\n\n\n\n\n\n\n\nWhat do you notice? Can you tell whether those Asian outliers come from small or large countries? Are they from earlier or later time periods?\nThese are the kinds of questions we can answer when we use multiple aesthetics thoughtfully. Isn’t it amazing how much information we can pack into a single visualization?\nFantastic work today! In the next lesson, we’ll continue exploring new tools and techniques to take your visualizations even further. See you soon!"
  },
  {
    "objectID": "visualization.html#non-data-linked-properties",
    "href": "visualization.html#non-data-linked-properties",
    "title": "Visualization",
    "section": "Non-data linked properties",
    "text": "Non-data linked properties\nWelcome back!\nSo far, we’ve packed a lot of information into single graphs using data-mapped aesthetics like color, size, and shape. While this approach is powerful, let’s face it—combining too many aesthetics can make a plot feel busy and overwhelming.\nSometimes, less is more. A clean and simple graph, highlighting just one or two aspects of the data, can be just as insightful—and a lot easier on the eyes.\nNow, the default style in plotnine is already quite nice, but there may come a time when you want to tweak things to better suit your storytelling. So, let’s look at how to customize graphs using non-data-linked properties—those that aren’t mapped to a variable but instead apply globally to all points in the graph.\nHere’s an example. What if we want all the points in our plot to be the same color, say blue? And what if we also want to adjust their size and transparency? Here’s the code to do that:\n\n(\nggplot(gapminder)+\ngeom_point(mapping = aes(x='gdpPercap', y='lifeExp'),\n            alpha=0.1, size=2, color='blue')\n)\n\n\n\n\n\n\n\n\nGo ahead, give this a try.\n\nBeautiful, isn’t it? All the points are now blue, with a larger size and a soft transparency that makes overlapping points blend together nicely. This transparency, or alpha, helps highlight areas where the data is dense—like shadows on a heatmap.\nNotice something? The color, size, and alpha settings aren’t part of the aes() function. That’s because these properties aren’t mapped to any variable in the data. Instead, they’re applied uniformly to every point in the plot.\nLet’s break it down:\n\ncolor=\"blue\": The color is set as a character string, wrapped in quotes. You can experiment with other colors too—try red, green, or even hex codes like “#FF5733”.\nsize=2: The size of the points is specified as a number, in millimeters. Increase the size to make the points larger or decrease it for smaller ones.\nalpha=0.1: Transparency is a decimal value between 0 and 1, where 0 is completely transparent and 1 is fully opaque.\n\nFinally, let’s talk about shapes. In plotnine, shapes are represented by numbers. For example:\n\n0 is a square,\n1 is a circle,\n2 is a triangle,\n20 is a small filled circle.\n\nHere’s a challenge for you\n\n\n\n\n\n\nChallenge\n\n\n\nChange the shape argument in the code to explore different shapes. Try values between 0 and 25, and see how your graph changes. You’ll find the full list of shapes in the plotnine documentation.\n\n\nSo, what do you think? With just a few tweaks, we’ve turned our scatter plot into a clean and stylish visual. Customizing non-data-linked properties like this is a great way to emphasize certain elements of your data without overwhelming your audience.\nIn the next lesson, we’ll explore even more ways to take your visualizations to the next level. See you there!"
  },
  {
    "objectID": "visualization.html#geometrical-objects",
    "href": "visualization.html#geometrical-objects",
    "title": "Visualization",
    "section": "Geometrical objects",
    "text": "Geometrical objects\nWelcome back! Let’s dive into another exciting aspect of creating visualizations in plotnine: geometrical objects, or geom_ functions.\nThese geom_ functions are the building blocks of your plots, allowing you to highlight different aspects of your data. By swapping or combining geom_ layers, you can tell entirely new stories with the same dataset.\nFor example, what if we wanted to show the development of life expectancy over time for each country? We could use geom_line() to connect individual data points belonging to the same country.\nHere’s the code to do just that:\n\n(\nggplot(gapminder)+\ngeom_line(mapping = aes(x='year', y='lifeExp',\n          group='country', color='continent'))\n)\n\n\n\n\n\n\n\n\nNote that we have a new aesthetics called group. It indicates which points need to be connected together to for a line. Here we are drawing one line per country. Take a moment to run this and see what you get.\n\nDo you see it? Each country now has its own line, colored by continent. It’s fascinating to watch life expectancy trends unfold over time. But look closely—you might notice some sharp, sudden drops for certain countries. What do you think caused these declines? Wars? Epidemics?\nWe’ll learn how to zoom in on these tragic moments and identify the affected countries later in the course, once we’ve mastered some data wrangling with Polars. For now, make a mental note of this question so you can return to it later.\nAnother powerful geometrical object is geom_boxplot(). This creates a “box-and-whisker” plot that illustrates the distribution of values within categories.\nFor example, let’s visualize how life expectancy varies by continent:\n\n(\nggplot(gapminder)+\ngeom_boxplot(mapping = aes(x='continent', y='lifeExp'))\n)\n\n\n\n\n\n\n\n\nRun the code and take a look.\n\nThe box represents the interquartile range—the middle 50% of data—while the line inside the box marks the median. The “whiskers” extend to show the 95% confidence interval, and any points outside this range are plotted as individual outliers.\nNow, wouldn’t it be great to combine this boxplot with our jittered points from earlier? This would help us see both the overall distribution and the outliers more clearly. Let’s layer them together:\n\n(\nggplot(gapminder)+\ngeom_jitter(mapping = aes(x='continent', y='lifeExp', color='continent'))+\ngeom_boxplot(mapping = aes(x='continent', y='lifeExp', color='continent'))\n)\n\n\n\n\n\n\n\n\n\nLooks great, doesn’t it? But notice something—there’s some duplication in our code. We had to repeat the same mappings for both geom_jitter and geom_boxplot. That’s fine for now, but it can become cumbersome as your visualizations grow more complex.\nHere’s a trick to make your code cleaner: you can move shared mappings to the parent ggplot() function. This way, every layer will “inherit” these mappings automatically:\n\n(\nggplot(gapminder, mapping = aes(x='continent', y='lifeExp', color='continent'))+\ngeom_jitter()+\ngeom_boxplot()\n)\n\n\n\n\n\n\n\n\nSee? No more repeating yourself! You can still add layer-specific settings or arguments within individual geom_ functions if needed.\n\n\n\n\n\n\nTip\n\n\n\nWhen building complex plots, start by adding one layer at a time. Once you’ve got the basic structure, move any common arguments up to the ggplot() function. This keeps your code tidy and easier to read.\n\n\nGreat job so far! In the next lesson, we’ll explore even more ways to enhance your visualizations. See you there!"
  },
  {
    "objectID": "visualization.html#trend-lines",
    "href": "visualization.html#trend-lines",
    "title": "Visualization",
    "section": "Trend lines",
    "text": "Trend lines\nWelcome back! Now, let’s take a closer look at the relationship between GDP per capita and life expectancy.\nAt first glance, life expectancy seems to improve as countries get richer. But is this relationship consistent across continents? Let’s find out by adding trend lines to our plot.\nTrends are essentially linear regression lines. You might remember them from school—they represent the best-fit line through your data. Here’s how we can add them to highlight differences in this relationship by continent:\n\n(\nggplot(gapminder, mapping = aes(x='gdpPercap', y='lifeExp', color='continent')) +\ngeom_point(alpha=0.5) +\ngeom_smooth(method='lm') +\nscale_x_log10()\n)\n\n\n\n\n\n\n\n\nTake a moment to run the code and see the result.\n\nWhat do you observe? By default, geom_smooth() creates a regression line for each continent, and plotnine even adds confidence intervals—those shaded gray areas around the lines. These intervals give us an idea of how well the model fits the data.\nWe also used the alpha argument to make our points semi-transparent. Why? It reduces visual clutter and lets the trend lines stand out more. Did you know that transparency can also be mapped to a variable? That’s right—just like color or size, you can use alpha as a mapping aesthetic to make transparency vary based on your data. Try experimenting with that later!\nHere’s a task for you: Modify the code we just used so that instead of creating separate regression lines for each continent, plotnine creates a single trend line for all data points.\nHere’s the code to start with:\n\n\n\n\n\n\nChallenge\n\n\n\nCreate a single regression line for all data points modifying this sample code\n(\nggplot(gapminder, mapping = aes(x='gdpPercap', y='lifeExp', color='continent'))+\ngeom_point(alpha=0.5)+\ngeom_smooth(method='lm')+\nscale_x_log10()\n)\n\n\nTake a moment to think about it. How can you combine the points colored by continent with a single global regression line?\nThere’s more than one way to solve this problem—see what you can come up with!\nDid you find this challenge hard? It’s ok! Let’s step through it together!\nIn our previous example, we declared all the mappings—x, y, and color—at the global level, in the ggplot() function. This means that every layer inherited these mappings. While this works well for most situations, it’s not what we need here.\nTo build a single trend line for all data points, we must ensure that the color aesthetic applies only to the points and not to the trend line. How do we do that? By moving the color mapping from the global level into the geom_point() function.\nHere’s how the updated code looks:\n\n(\nggplot(gapminder, mapping = aes(x='gdpPercap', y='lifeExp'))+\ngeom_point(aes(color='continent'), alpha=0.5)+\ngeom_smooth(method='lm')+\nscale_x_log10()\n)\n\n\n\n\n\n\n\n\nBy moving the color aesthetic into geom_point(), it now affects only the points layer. Notice that it’s still wrapped in the aes() function because it remains a data-linked property. Meanwhile, the trend line—added by geom_smooth()—inherits only the global mappings for x and y. This creates a single linear model across all continents, as we wanted.\nTake a moment to observe how this subtle adjustment changes the visualization and makes the trend line easier to interpret.\nSome of you might have come up with an alternative solution. Instead of changing the color aesthetic’s scope, we can override it directly within the geom_smooth() layer. In this case, the color aesthetic remains global, but we specify a non-data-linked property for the trend line, such as making it black:\n\n(\nggplot(gapminder, mapping = aes(x='gdpPercap', y='lifeExp', color='continent'))+\ngeom_point(alpha=0.5)+\ngeom_smooth(method='lm', color='black')+\nscale_x_log10()\n)\n\n\n\n\n\n\n\n\nHere, geom_smooth() ignores the global color aesthetic and instead applies the color black uniformly to the trend line. The result? A single black trend line stands out clearly, while the points remain color-coded by continent.\nBoth approaches work well, and the choice depends on how you want to structure your code and highlight different layers. Managing global and layer-specific mappings is a powerful feature in plotnine that gives you flexibility in creating clean, insightful plots."
  },
  {
    "objectID": "visualization.html#factors",
    "href": "visualization.html#factors",
    "title": "Visualization",
    "section": "Factors",
    "text": "Factors\nDo you want to learn a nifty trick that can improve your data visualization? This method can be useful when you want to visualize a continuous variable which has a limited number of distinct values.\nImagine we’re working with year, which is technically a continuous variable. But for some visualizations it might make more sense to treat each year as a separate category. How do we do that without modifying the data?\nSimple! Instead of referencing year as a string ('year'), wrap it in factor(), like this 'factor(year)'. This shorthand plotnine function converts a continuous variable into a categorical one on the fly, with each distinct value treated as its own category.\nLet’s put this into practice with a couple of challenges!\nCreate a boxplot of life expectancy over time, treating year as a categorical variable. Using this plot, can you detect when the interquartile range of life expectancy—the middle 50% of values—was the smallest?\nThen apply the same concept to gdpPercap.\nCreate a boxplot of GDP per capita by year, but this time keep it on a logarithmic scale. Remember that we used the scale_y_log10() function to make the data easier to interpret. Compare the interquartile range of GDP per capita in 2007 with that in 1952. Is the world today more or less diverse in terms of economic inequality?\n\n\n\n\n\n\nChallenge\n\n\n\n\nMake a boxplot of life expectancy by year. When was interquartile range of life expectancy the smallest?\nMake the same plot of gdpPercap (on a log scale) per year. Is the world today more or less diverse than in 1952?\n\n\n\nGo ahead and give it a try. Pause the video and come back once you have your answer!\n\n(\nggplot(gapminder)+\ngeom_boxplot(mapping = aes(x='factor(year)', y='lifeExp', group='year'))\n)\n\n\n\n\n\n\n\n\n\n(\nggplot(gapminder)+\ngeom_boxplot(mapping = aes(x='factor(year)', y='gdpPercap', group='year')) +\nscale_y_log10()\n)\n\n\n\n\n\n\n\n\nLooking at these two plots, you might notice a fascinating pattern.\nYou’re absolutely right: economic inequality has grown in the recent decades. In 1952, the world was much poorer, but there was a greater sense of uniformity across nations. By 2007, while the world is significantly wealthier on average, the disparities have widened.\nAnd those outliers? Intriguing, aren’t they? Three countries stand apart from the rest in the 1952. Which ones could they be? We’ll revisit these mysteries after diving into data wrangling techniques.\nGreat work on these challenges! These exercises show the power of visualizing data in different ways and how little tricks like factor() can make your plots much clearer. Next, we’ll explore other types of plots that can uncover even more insights. Stay tuned, and I’ll see you in the next lesson!"
  },
  {
    "objectID": "visualization.html#more-geoms",
    "href": "visualization.html#more-geoms",
    "title": "Visualization",
    "section": "More geoms",
    "text": "More geoms\nBy now, you’ve learned so much about using plotnine to create insightful visualizations. But we’ve barely scratched the surface!\nOne of the most exciting features of plotnine is the sheer variety of geoms—the building blocks for visualizing data. Start typing geom_ in your code editor, and you’ll see a list of options pop up. It’s like a treasure chest of possibilities, and each geom offers a unique perspective on your data.\nLet’s put your skills to the test with a few new challenges!\nHistograms are perfect for exploring the distribution of a single variable. Let’s start with life expectancy. Create a histogram and observe the shape of the distribution. How many peaks—or modes—does it have? Play around with the bins parameter. Adjusting the number of bins changes the granularity of your histogram, which can affect how you interpret the distribution. What value of bins seems reasonable to you?\n\n\n\n\n\n\nChallenge\n\n\n\nMake a histogram of life expectancy. What is the shape of the distribution? How many modes (peaks) does the distribution of life expectancy have? What value of the bins parameter look reasonable?\n\n\n\n(\nggplot(gapminder)+\ngeom_histogram(mapping = aes(x='lifeExp'), bins=100)\n)\n\n\n\n\n\n\n\n\nNext up: density plots. These are smoothed-out versions of histograms, showing the probability distribution of your data.\nCreate a simple density plot for life expectancy. You can do it! Start typing and you will find the function you need. Do you see it? What if you want to compare distributions across continents? Add a color aesthetic.\n\n\n\n\n\n\nChallenge\n\n\n\nBuild a density function. How would you compare density functions of different continents?\n\n\n\n(\nggplot(gapminder)+\ngeom_density(mapping = aes(x='lifeExp'))\n)\n\n\n\n\n\n\n\n\n\n(\nggplot(gapminder)+\ngeom_density(mapping = aes(x='lifeExp', color='continent'))\n)\n\n\n\n\n\n\n\n\nRight! You can split the data by continent by adding a color aesthetic and linking it to the variable continent. Or take it one step further! Use the fill aesthetic (in addition to color) to fill the areas under the curves. Add some transparency with alpha for a cleaner visualization like this:\n\n(\nggplot(gapminder)+\ngeom_density(mapping = aes(x='lifeExp', color='continent', fill='continent'), alpha=0.3)\n)\n\n\n\n\n\n\n\n\nThese plots help us see how life expectancy varies not just overall, but also within each continent. Notice any interesting patterns? What might explain the peaks—or modes—you see in the distributions?\nNow let’s level up with 2D density plots. These are excellent for visualizing relationships between two variables. Start by creating a density plot of log GDP per capita vs. life expectancy (use geom_density_2d() function):\n\n\n\n\n\n\nChallenge\n\n\n\nBuild a graph using geom_density2d() for log GDP per capita vs life expectancy. How many clusters of datapoints can you identify? What if you look at it by continent?\n\n\n\n(\nggplot(gapminder)+\ngeom_density_2d(mapping = aes(x='gdpPercap', y='lifeExp'))+\nscale_x_log10()\n)\n\n\n\n\n\n\n\n\nWhat do you see? Notice the two distinct clusters? One cluster represents countries that are poorer and have lower life expectancy, while the other includes those that are wealthier and healthier.\nNow let’s break it down by continent.\n\n\n\n\n\n\nChallenge\n\n\n\nAdd a color aesthetic to see how regions of the world are distributed\n\n\n\n(\nggplot(gapminder)+\ngeom_density_2d(mapping = aes(x='gdpPercap', y='lifeExp', color='continent'))+\nscale_x_log10()\n)\n\n\n\n\n\n\n\n\nIsn’t that fascinating? The lower cluster is primarily made up of African countries, while the higher cluster mostly includes Europe and Oceania. Asia? It’s scattered across both clusters, reflecting its diversity in economic and health outcomes. These exercises highlight the flexibility and power of plotnine. Whether it’s histograms, density plots, or advanced 2D density visualizations, each plot adds a new layer of understanding to your data."
  },
  {
    "objectID": "visualization.html#faceting",
    "href": "visualization.html#faceting",
    "title": "Visualization",
    "section": "Faceting",
    "text": "Faceting\nWhen your graph starts to feel a bit too crowded—perhaps with too many layers or overlapping aesthetics—there’s a simple solution: faceting. Faceting allows you to split your data into separate panels, creating multiple similar graphs for subsets of your data. This can make complex trends easier to spot and comparisons much clearer.\nIn plotnine, faceting is incredibly easy to use. Let’s revisit one of our earlier graphs and apply faceting to organize it by continent.\n\n(\nggplot(gapminder, mapping = aes(x = 'gdpPercap', y = 'lifeExp')) +\n  geom_point() +\n  geom_smooth(color=\"blue\") +\n  scale_x_log10() + \n  facet_wrap('continent')\n)\n\n\n\n\n\n\n\n\nHere’s what’s happening:\n\nfacet_wrap('continent') instructs plotnine to create a separate panel for each unique value in the continent column.\nPanels are arranged from left to right, and when they don’t fit on one row, they “wrap” onto the next line.\n\nThe result? A clean, organized set of charts where each panel highlights the GDP-per-capita and life expectancy trends for a specific continent. Faceting is especially helpful when the number of panels is manageable, and it lets us compare trends within each group side by side.\nLet’s take this idea further. What happens to the relationship between GDP per capita and life expectancy over time?\nTry faceting by year instead of continent.See if you can answer these questions\n\n\n\n\n\n\nQuestion\n\n\n\n\nDo the slopes of the trend lines change over the years?\nHow does the clustering of data points evolve as time progresses?\n\n\n\nThis exercise offers an incredible opportunity to see how historical events, global growth, and inequality have shaped the world over decades.\n\n\n\n\n\n\nChallenge\n\n\n\nFacet the following plot by year, keeping the linear smoother. You can edit this sample code\n(\nggplot(gapminder, mapping = aes(x = 'gdpPercap', y = 'lifeExp')) +\n  geom_point() +\n  geom_smooth(color='blue') +\n  scale_x_log10() + \n  facet_wrap('continent')\n)\n\n\n\n# facet by year\n(\nggplot(data = gapminder, mapping = aes(x = 'gdpPercap', y = 'lifeExp')) +\n  geom_point() +\n  geom_smooth(color='blue') +\n  scale_x_log10() + \n  facet_wrap('year')\n)\n\n\n\n\n\n\n\n\nWith everything we’ve learned so far, we can summarize the plotnine template as follows::\n(\nggplot(&lt;DATA&gt;) + \n  &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;)) + \n  &lt;SCALE_FUNCTION&gt; +\n  &lt;FACET_FUNCTION&gt;\n)\nFaceting is a powerful addition to your visualization toolkit, especially when your data has distinct groups or categories. Whether you’re analyzing trends over continents or time, faceting can make your insights clearer and more impactful.\nSo, go ahead—try faceting your own graphs. You’ll be amazed at what you uncover!"
  },
  {
    "objectID": "visualization.html#labeling-and-styling-the-chart",
    "href": "visualization.html#labeling-and-styling-the-chart",
    "title": "Visualization",
    "section": "Labeling and styling the chart",
    "text": "Labeling and styling the chart\nWe’ve built our chart layer by layer, and now it’s time to refine it for presentation—whether for your boss, a client, or publication. The final touches, like annotations and labels, can make all the difference in ensuring your audience understands your insights clearly.\nLet’s start with some practical data transformations. Instead of showing GDP per capita in raw numbers, wouldn’t it be better to express it in thousands of dollars? Similarly, population is easier to interpret when expressed in millions.\nWith plotnine, we don’t need to preprocess our data for this. You can specify transformations directly in your chart code. For example gdpPercap/1e3 divides GDP per capita by 1,000, and uses scientific notation (1e3) for convenience. Similarly, you can use pop/1e6 to show population in millions.\nTo make our chart clear and professional, we’ll use the labs() function. This function gathers all labels in one place, allowing us to customize:\n\nTitle and subtitle at the top,\nCaption at the bottom,\nLabels for x, y, and any mapped aesthetics, like color or size.\n\nHere’s an example of a polished, annotated chart:\n\n(\nggplot(data = gapminder) + \n  geom_point(mapping = aes(x = 'gdpPercap/1e3', y = 'lifeExp', size='pop/1e6', color='continent')) +\n  scale_x_log10() +\n  facet_wrap('year') + \n  labs(title=\"Life Expectancy vs GDP per capita over time\",\n        subtitle=\"In the past 50 years, life expectancy has improved in the world\",\n        caption=\"Source: Gapminder foundation, www.gapminder.org\",\n        x=\"GDP per capita, '000 USD\",\n        y=\"Life expectancy, years\",\n        color=\"Continent\",\n        size=\"Population, mln\")\n)\n\n\n\n\n\n\n\n\nEach label corresponds to the aesthetics used in the aes() mappings. Make sure all mapped aesthetics are labeled, even if they appear in just one layer.\nNow, let’s make your chart stand out! plotnine offers pre-selected themes that adjust the colors, fonts, and overall style of your plots.\nOne of my favorites is theme_minimal(). It simplifies the design, creating a clean and modern look:\n\n(\nggplot(data = gapminder) + \n  geom_point(mapping = aes(x = 'gdpPercap/1e3', y = 'lifeExp', size='pop/1e6', color='continent')) +\n  scale_x_log10() +\n  facet_wrap('year') + \n  labs(title=\"Life Expectancy vs GDP per capita over time\",\n        subtitle=\"In the past 50 years, life expectancy has improved in the world\",\n        caption=\"Source: Gapminder foundation, www.gapminder.org\",\n        x=\"GDP per capita, '000 USD\",\n        y=\"Life expectancy, years\",\n        color=\"Continent\",\n        size=\"Population, mln\") +\n  theme_minimal()\n)\n\n\n\n\n\n\n\n\nplotnine offers plenty of built-in themes to match your purpose:\n\ntheme_dark() for a sleek, high-contrast look.\ntheme_linedraw() for a simple, hand-drawn aesthetic.\ntheme_xkcd() for a playful, comic-style appearance.\ntheme_538() for a polished, professional newsroom feel.\n\nThemes contributed by the community can add even more variety. So, explore, experiment, and find the one that best suits your data story.\nCongratulations!\nYou’ve learned about aesthetics, scales, different types of geoms and now you also know how to annotate and apply themes to your visuals to make them more compelling. With these skills, you’re ready to create polished, professional-quality charts that truly stand out.\nGood luck, and we can’t wait to see the insights you’ll uncover!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Literacy with Python",
    "section": "",
    "text": "Welcome to Data Literacy with Python!\nLet me ask you something: Can you imagine living in today’s world but being unable to read? Think about it—street signs wouldn’t make sense, advertisements would just be noise, and most of the internet? Completely out of reach.\nNow, even with videos and voice assistants everywhere, written text is still the backbone of how we communicate and navigate life. Without it, you’d feel lost.\nBut here’s the thing: today’s world doesn’t just run on words. It runs on data.\nEvery day, we’re creating over 400 million terabytes of data. That’s every single day. And here’s a wild stat—90% of all the world’s data was created in just the last two years.\nThis explosion of information is transforming how we make decisions, whether it’s in business, science, or society as a whole. To keep up, you need to know how to make sense of it.\nData isn’t just numbers on a screen—it’s stories waiting to be uncovered. And understanding data has become just as important as being able to read or write.\nThat’s where this course comes in.\nWe’re going to teach you how to take raw, messy data and turn it into something meaningful. You’ll work with rectangular data—the kind you find in spreadsheets or databases.\nAnd don’t worry—this isn’t just about crunching numbers. It’s about answering real-world questions, solving problems, and making decisions based on insights you uncover.\nBy the end of this course, you’ll have the skills to transform data into knowledge.\nLet’s talk about the tools you need to work with data.\nYou might be tempted by low-code or no-code solutions—those point-and-click interfaces that make everything seem so easy. And sure, they’re great for quick wins. But when it comes to serious data analysis, they have some big limitations.\nData analysis isn’t just about getting answers—it’s about getting credible answers.\nTo trust your insights, you need to leave a trail. Think about it—during analysis, you make dozens of tiny decisions:\n\nWhich part of the data should you focus on?\nWhat variables should you use?\nWhich patterns caught your eye?\n\nEvery decision shapes your results. And if you—or anyone else—can’t retrace those steps, how can you be sure your conclusions hold up?\nThat’s why scripting your analysis is so important.\nWith a script, every step is recorded. You can spot mistakes, refine your work, or pick up right where you left off—even months later. Low-code tools? They don’t give you that kind of transparency.\nSo, what’s the best language for scripting your data analysis?\nThe answer is Python.\nPython is the world’s most popular programming language, and for good reason. Created in 1990 by Guido van Rossum, Python has become the go-to language for everything from building websites to powering cutting-edge AI. It may not be the fastest language out there, but it’s arguably the most readable. And in today’s data-driven world, readability matters more than ever.\nThe Python ecosystem for data analysis is enormous. Whatever your question, there’s a good chance Python has a library—or ten—that can help.\nData analysis is unique—it’s less about traditional programming and more about crafting a story with your data. Your code should be clear and intuitive, not just for you, but for anyone who needs to understand your work. And that includes “future you”—because six months from now, you might not even recognize your own analysis without clear documentation!\nSo, as we dive into this course, we’ll emphasize simplicity, transparency, and readability. Because great analysis isn’t just about crunching numbers—it’s about telling a story that stands the test of time.\nData analysis is evolving. Today, some of the most cutting-edge tools are built on high-performance programming languages like Rust, Java, or C++. Why? Because these languages are fast—lightning fast. But here’s the best part: you don’t need to write in these languages to enjoy their benefits.\nModern tools now separate the user interface from the engine. That means the algorithms working behind the scenes are the same, no matter which scripting language you use.\nInitiatives like Apache Arrow go even further—they create standardized data formats, making it easy to move between tools and platforms without losing performance or compatibility.\nIn this course, we’re diving into tools built on Rust—one of the fastest, most efficient programming languages out there. Specifically, we’ll use uv for managing packages and environments and polars for data wrangling.\nThese tools are not just fast—they’re scalable.\nThe examples we’ll explore together are small—easy to follow and understand. But don’t let that fool you. The same tools we use here can scale effortlessly to handle datasets with billions of rows, processed across dozens of parallel machines.\nWhat’s even better? The interface doesn’t change.\nSo whether you’re working on a personal project, academic research, or a large-scale business application, the skills you gain here will translate directly to the real world.\nThe datasets may be small, but the questions and challenges we tackle are universal. By the end of this course, you’ll be equipped to uncover meaningful insights from your own data, no matter its size or complexity.\nLet’s get started on this exciting journey into the world of data literacy!"
  },
  {
    "objectID": "index.html#video-studio-10-min",
    "href": "index.html#video-studio-10-min",
    "title": "Data Literacy with Python",
    "section": "",
    "text": "Welcome to Data Literacy with Python!\nLet me ask you something: Can you imagine living in today’s world but being unable to read? Think about it—street signs wouldn’t make sense, advertisements would just be noise, and most of the internet? Completely out of reach.\nNow, even with videos and voice assistants everywhere, written text is still the backbone of how we communicate and navigate life. Without it, you’d feel lost.\nBut here’s the thing: today’s world doesn’t just run on words. It runs on data.\nEvery day, we’re creating over 400 million terabytes of data. That’s every single day. And here’s a wild stat—90% of all the world’s data was created in just the last two years.\nThis explosion of information is transforming how we make decisions, whether it’s in business, science, or society as a whole. To keep up, you need to know how to make sense of it.\nData isn’t just numbers on a screen—it’s stories waiting to be uncovered. And understanding data has become just as important as being able to read or write.\nThat’s where this course comes in.\nWe’re going to teach you how to take raw, messy data and turn it into something meaningful. You’ll work with rectangular data—the kind you find in spreadsheets or databases.\nAnd don’t worry—this isn’t just about crunching numbers. It’s about answering real-world questions, solving problems, and making decisions based on insights you uncover.\nBy the end of this course, you’ll have the skills to transform data into knowledge.\nLet’s talk about the tools you need to work with data.\nYou might be tempted by low-code or no-code solutions—those point-and-click interfaces that make everything seem so easy. And sure, they’re great for quick wins. But when it comes to serious data analysis, they have some big limitations.\nData analysis isn’t just about getting answers—it’s about getting credible answers.\nTo trust your insights, you need to leave a trail. Think about it—during analysis, you make dozens of tiny decisions:\n\nWhich part of the data should you focus on?\nWhat variables should you use?\nWhich patterns caught your eye?\n\nEvery decision shapes your results. And if you—or anyone else—can’t retrace those steps, how can you be sure your conclusions hold up?\nThat’s why scripting your analysis is so important.\nWith a script, every step is recorded. You can spot mistakes, refine your work, or pick up right where you left off—even months later. Low-code tools? They don’t give you that kind of transparency.\nSo, what’s the best language for scripting your data analysis?\nThe answer is Python.\nPython is the world’s most popular programming language, and for good reason. Created in 1990 by Guido van Rossum, Python has become the go-to language for everything from building websites to powering cutting-edge AI. It may not be the fastest language out there, but it’s arguably the most readable. And in today’s data-driven world, readability matters more than ever.\nThe Python ecosystem for data analysis is enormous. Whatever your question, there’s a good chance Python has a library—or ten—that can help.\nData analysis is unique—it’s less about traditional programming and more about crafting a story with your data. Your code should be clear and intuitive, not just for you, but for anyone who needs to understand your work. And that includes “future you”—because six months from now, you might not even recognize your own analysis without clear documentation!\nSo, as we dive into this course, we’ll emphasize simplicity, transparency, and readability. Because great analysis isn’t just about crunching numbers—it’s about telling a story that stands the test of time.\nData analysis is evolving. Today, some of the most cutting-edge tools are built on high-performance programming languages like Rust, Java, or C++. Why? Because these languages are fast—lightning fast. But here’s the best part: you don’t need to write in these languages to enjoy their benefits.\nModern tools now separate the user interface from the engine. That means the algorithms working behind the scenes are the same, no matter which scripting language you use.\nInitiatives like Apache Arrow go even further—they create standardized data formats, making it easy to move between tools and platforms without losing performance or compatibility.\nIn this course, we’re diving into tools built on Rust—one of the fastest, most efficient programming languages out there. Specifically, we’ll use uv for managing packages and environments and polars for data wrangling.\nThese tools are not just fast—they’re scalable.\nThe examples we’ll explore together are small—easy to follow and understand. But don’t let that fool you. The same tools we use here can scale effortlessly to handle datasets with billions of rows, processed across dozens of parallel machines.\nWhat’s even better? The interface doesn’t change.\nSo whether you’re working on a personal project, academic research, or a large-scale business application, the skills you gain here will translate directly to the real world.\nThe datasets may be small, but the questions and challenges we tackle are universal. By the end of this course, you’ll be equipped to uncover meaningful insights from your own data, no matter its size or complexity.\nLet’s get started on this exciting journey into the world of data literacy!"
  },
  {
    "objectID": "pivotjoin.html",
    "href": "pivotjoin.html",
    "title": "Pivots and joins",
    "section": "",
    "text": "Hello and welcome back to Data Literacy with Python!\nWe’re continuing our exciting journey into data wrangling—a cornerstone of data analysis and storytelling. If you’ve been with us through the previous modules, congratulations! You’ve covered a lot of ground and gained some serious data skills.\nLet’s quickly recap:\n\nWe explored subsetting data, learning how to select specific rows and columns with functions like .select(), .drop(), and the powerful suite of polars.selectors.\nFor observations, we used .head() and .tail() to view subsets and .filter() to fine-tune our subsetting of the data.\nWe mastered creating new columns using expressions wrapped in .with_columns().\nAnd finally, we learned how to summarize data using .group_by() and .agg(), creating insightful summaries of our datasets.\n\nThese are all foundational skills, and you’re doing great!\nBut now, it’s time to level up. Today, we’re tackling data reshaping and joins, two powerful techniques for reorganizing and enriching your datasets. I also promised you some nice-looking tables, and I intend to deliver! We’ll be using the amazing great_tables library for our table designs. Let’s load up the libraries and dive right in.\n\n\nCode\nimport polars as pl\nimport polars.selectors as cs\nfrom plotnine import *\nfrom great_tables import GT\n\n\nToday’s dataset comes from the Break from Plastics environmental campaign — a sample of data with a powerful story. Here’s the description of the data:\n\n\n\n\n\n\nNote\n\n\n\n\n\nIn 2020, thanks to our members and allies, Break Free From Plastic engaged 14,734 volunteers in 55 countries to conduct 575 brand audits. These volunteers collected 346,494 pieces of plastic waste, 63% of which was marked with a clear consumer brand. Despite the challenges of organizing during a global pandemic, our volunteers safely coordinated more brand audit events in more countries this year than in the previous two years. As a special activity during the pandemic, we also worked with over 300 waste pickers to highlight their roles as essential workers. Participants catalogued over 5,000 brands in this year’s global audit. Our analysis reveals the following as the 2020 Top 10 Global Polluters: The Coca-Cola Company; PepsiCo; Nestlé; Unilever; Mondelez International; Mars, Inc.; Procter & Gamble; Philip Morris International; Colgate-Palmolive; and Perfetti Van Melle.\n\n\n\nHere’s the code to bring this data into our workspace:\n\n\nCode\nplastics_df = pl.read_csv('bffp/BFFplastics.csv')\n\nplastics_docs = pl.DataFrame({\n    'Variable': ['region', 'country_code' , 'country', 'year', 'parent_company', 'empty', 'hdpe', 'ldpe', 'o', 'pet', 'pp', 'ps', 'pvc', 'grand_total', 'num_events', 'volunteers'],\n    'Class': ['character','character','character', 'double', 'character', 'double', 'double', 'double', 'double', 'double', 'double', 'double', 'double', 'double', 'double', 'double'],\n    'Description': ['Region', 'Alpha 3 ISO 3166 code','Country of cleanup', 'Year (2019 or 2020)', 'Source of plastic (company name)', 'Category left empty count', 'High density polyethylene count (Plastic milk containers, plastic bags, bottle caps, trash cans, oil cans, plastic lumber, toolboxes, supplement containers)', 'Low density polyethylene count (Plastic bags, Ziploc bags, buckets, squeeze bottles, plastic tubes, chopping boards)', 'Category marked other count', 'Polyester plastic count (Polyester fibers, soft drink bottles, food containers (also see plastic bottles)', 'Polypropylene count (Flower pots, bumpers, car interior trim, industrial fibers, carry-out beverage cups, microwavable food containers, DVD keep cases)', 'Polystyrene count (Toys, video cassettes, ashtrays, trunks, beverage/food coolers, beer cups, wine and champagne cups, carry-out food containers, Styrofoam)', 'PVC plastic count (Window frames, bottles for chemicals, flooring, plumbing pipes)', 'Grand total count (all types of plastic)', 'Number of counting events', 'Number of volunteers']\n})\n\n\nIn your notebook you can see the code for importing the data, as well as a DataFrame containing a data dictionary — a detailed description of the variables in this dataset.\nSo far, we’ve always imported data from CSV files or pre-built datasets. But now, it’s time to talk about creating data frames by hand. This is super handy when working with small examples, prototypes, or mock data. To do that, we need to talk about two foundational Python data structures: dictionaries and lists.\nThink of a dictionary as a way to describe an object. It’s a collection of “key-value” pairs—like writing down standard characteristics of something along with their values. Dictionaries are specified with curly brackets {}. Let’s say I want to describe my bike:\n\n\nCode\nBicycle = {\n    'Type': 'Hybrid',\n    'Size': 28,\n    'Make': 'Merida',\n    'Color': 'Grey',\n    'Price': 250\n}\n\n\nHere, each characteristic—like Type or Size—has one value. Easy, right? But what if I also wanted to describe the bikes of my twins? I’d need three records, not one.\nThis is where lists come in. A list is a collection of items—typically of the same type—and it’s denoted with square brackets []. Let’s use lists to describe all the bikes in my garage:\n\n\nCode\nBikes = {\n    'Type': ['Hybrid', 'MTX', 'BMX'],\n    'Size': [28, 24, 26], \n    'Make': ['Merida', 'Giant', 'Specialized'],\n    'Color': ['Grey', 'White', 'Orange'],\n    'Price': [250, 180, 220]\n}\n\n\nNow, we have a dictionary of lists, representing three bikes. To turn this into a Polars DataFrame, all we need to do is pass it to the pl.DataFrame() function:\n\n\nCode\nbikes_df = pl.DataFrame(Bikes)\n\n\nAnd just like that, we’ve created a data frame by hand! This is a simple yet powerful way to structure and manipulate small datasets.\nOur plastics_docs specifies three characteristics: the variable names (Variable), their data types (Class), and their descriptions (Description).\nThis table essentially acts as documentation for the plastics_df. But let’s face it — raw data frames, while functional, don’t always look polished or presentation-ready. That’s where the Great Tables package comes in.\nGreat Tables is like a graphic design toolkit for your tables! It introduces a “grammar of tables,” similar to how plotnine provides a “grammar of graphics.” This makes it super easy to transform plain data frames into beautifully styled tables with minimal effort.\nThe core function in Great Tables is GT(), and it works similarly to how we use ggplot for creating plots. Let’s take a sneak peek at its capabilities by styling our plastics_docs data frame. Here’s how we do it:\nVoilà! .opt_stylize() method has some pre-built styles, which we used to convert a boring data frame into a polished and professional-looking table, ready to be shared or included in reports. Don’t worry about memorizing the details just yet — we’ll explore GT() more thoroughly in the upcoming sections.\nBefore we move on, let’s take a quick look at the data itself. Here’s a snippet of the first five rows of the plastics_df:\nAs you can see, the first few variables look familiar. They describe general metadata, like the region, country, and year. But let’s focus on the variables starting from empty and going down to pvc. These columns count the number of plastic pieces of different types collected during the cleanup.\nThe grand_total column sums up all these individual plastic counts. Finally, the last two columns—num_events and volunteers—capture operational details:\n\nHow many trash counting events took place in each country during a given year?\nHow many volunteers participated in these campaigns?\n\nThis dataset offers a wealth of insights into plastic pollution patterns across the globe. By organizing, reshaping, and visualizing this data, we’ll uncover powerful stories about the environmental challenges we face—and the steps we can take to address them.\nLet’s take a closer look at the data. Check out this very first row for Argentina. Notice the parent_company column contains the value: “Grand Total.” This suggests that this first row contains the totals for all Argentinian records in 2019.\nLet’s look at the next year:\nHere’s something curious: for the year 2020, the rows with country totals aren’t marked with “Grand Total.” Instead, the parent_company field is left blank, or in technical terms, it’s marked as missing - Null. Hmm! What do we do with those?\nNull values aren’t just limited to parent_company. Let’s take a look at records collected in 2019 from unidentified locations.\nBefore we dive deeper into analyzing top contributors to plastic waste, let’s calculate the totals per country and year ourselves. Why?\nWell, the dataset has pre-computed totals marked with “Grand Total” or Null in the parent_company, but the logic seems inconsistent. Recomputing the totals ensures transparency and accuracy in our analysis.\nHere’s how we start:\nLet’s break this down:\n\ndrop(\"grand_total\"): The grand_total column is a pre-computed sum of all plastic types, which we can recalculate if needed.\nExclude “Grand Total” rows: We filter out rows where the parent_company column is populated with the phrase “Grand Total.”\nExclude Null values in parent_company column: These rows lack a meaningful company label and often represent aggregated data.\n\nBy cleaning the data in this way, we ensure that our analysis is based on individual contributions, not pre-summarized totals."
  },
  {
    "objectID": "pivotjoin.html#stacking",
    "href": "pivotjoin.html#stacking",
    "title": "Pivots and joins",
    "section": "Stacking",
    "text": "Stacking\n\n# indoor air pollution\nhhap_deaths = pl.read_csv(\"hhap/hhap_deaths.csv\")\nclean_fuels = pl.read_csv(\"hhap/clean_fuels_cooking.csv\")\nfuel_types = pl.read_csv(\"hhap/cooking_by_fuel_type.csv\")\n\nAnother common type of operation is stacking two identical datasets together (vertically). This is possible to do when the meaning of the columns in the datasets is the same and we are interested in combining two parts of identical data into a new and larger dataset.\nRecall that in our household air pollution case study we had three files: - hhap_deaths - containing death cases, associated with air pollution - fuel_types - describing information about the fuels used by population in different countries for household needs - clean_fuels - containing the fraction of population in each country with access to clean fulels for cooking\nAll three of these datasets contain three identical columns describing the country of observation: region, country_code and country. The countries listed in each of the datasets is largely similar, but not completely overlapping. Let’s see if we can compile a single master set of all countries with the codes and the regions they belong to. Because the data is recorded over many years each of the datasets contains many duplicates entries. This problem will be even larger when we stack the data from several datasets together, so we will need to ensure the records in our final (combined) dataset are unique.\n\nidcols=cs.by_name(\"region\", \"country_code\", \"country\")\ncountry_regions = (hhap_deaths.select(idcols)\n    .vstack(fuel_types.select(idcols))\n    .vstack(clean_fuels.select(idcols))\n    .unique()\n)\n\nNote, that here we created a temporary object idcols, which will store only selector object for the three columns we are interested in. Polar selectors are independent entities which can live both inside the querying contexts as well as in the global environment, i.e. in memory accessible\nLets compare our country codes with the full list of codes issued by ISO. Here’s a file with all Alpha 2 and Alpha 3 codes issued to nation states and territories.\n\niso_df = pl.read_csv(\"hhap/CountryCodes_Alpha2_Alpha3.csv\")\n\n(country_regions\n    .join(iso_df, left_on=\"country_code\", right_on=\"alpha3\", how=\"anti\"))\n\n(country_regions\n    .join(iso_df, left_on=\"country_code\", right_on=\"alpha3\", how=\"left\"))\n\n# How many countries are not present in the combined household air pollution dataset? \n# What proportion of those countries have the world \"Island\" in their name?\n\ntmp_df1 = (iso_df\n    .join(country_regions, left_on=\"alpha3\", right_on=\"country_code\", how=\"anti\"))\n\n(iso_df\n    .join(country_regions, left_on=\"alpha3\", right_on=\"country_code\", how=\"anti\")\n    .select(pl.col(\"country\").str.contains(\"Island\").mean())\n    )\n\n(iso_df\n    .join(country_regions, left_on=\"alpha3\", right_on=\"country_code\", how=\"anti\")\n    .group_by(pl.col(\"country\").str.contains(\"Island\").alias(\"island\"))\n    .len()\n    .with_columns(pl.col(\"len\")/pl.sum(\"len\"))\n    .filter(\"island\")\n    )\n\n\nshape: (1, 2)\n\n\n\nisland\nlen\n\n\nbool\nf64\n\n\n\n\ntrue\n0.259259\n\n\n\n\n\n\nHorizontal stacking is possible, but you probably want to do a join instead, because horizontal stacking assumes that row order is the same and observations are identical. This is better ensured with unique IDs which could be used for join."
  }
]