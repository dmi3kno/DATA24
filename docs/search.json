[
  {
    "objectID": "wrangling.html",
    "href": "wrangling.html",
    "title": "Wrangling",
    "section": "",
    "text": "VIDEO 1: Data {greenscreen} ~ 4 min\n\nWelcome to the module on data wrangling! In the last lesson, we explored a small, but exciting gapminder dataset, and created quite a few visualizations with it. But in real-world scenarios, datasets are often much larger, which brings new challenges. We may need to focus on specific subsets - maybe observations from a single time period or a set of variables related to a particular phenomenon.\nIn this module, we’ll learn how to subset data and create meaningful summaries that provide a high-level overview of trends or differences between groups. Summarized data is typically presented in a tabular form, so we’ll also introduce a package for creating clear, professional-looking tables.\nMost importantly, we’ll dive into the blazingly fast Polars package for data manipulation. As I mentioned earlier, Polars is powered by Rust, a high-performance programming language. Its core functionality is exposed to Python but can also be accessed from other languages. This means that the data wrangling skills you gain here will be transferable beyond Python.\nBut first, let’s load the necessary packages for this module. In Python, it’s common to use the alias pl for Polars. We’ll also use a submodule called polars.selectors, aliasing it as cs - don’t worry, we’ll cover selectors in more detail soon. We’ll also import everything from Plotnine for data visualization and bring in the main function from the great_tables package for generating beautifully looking tables.\nHere’s the setup code. Place it in its own cell in your notebook and run it:\n&lt; PAUSE &gt;\nNow, let’s explore our data. We’ll be looking at the WHO dataset about household pollution around the world.\nHousehold air pollution is primarily caused by burning polluting fuels like wood, animal dung, charcoal, agricultural waste, and kerosene using open fires or inefficient stoves. Over 2 billion people around the world rely on these fuels for cooking, heating, and lighting. The poor combustion of these fuels leads to numerous health issues, such as pneumonia in children, and chronic illnessess like obstructive pulmonary disease, lung cancer, stroke, and cardiovascular problems in adults.\nThis is a complex issue, and like many complex issues, it can be examined from different angles. We have three datasets to work with:\n\nCauses of death linked to household pollution\nTypes of fuel used for cooking in various countries, and\nProportion of the population with access to clean cooking fuels\n\nEach dataset offers a unique perspective on this issue. Let’s dive in and start exploring!\n\n\nVIDEO 2: Data import {greenscreen} ~ 12 min\n\nIn this section, we’ll start working with the datasets about household air pollution. These datasets are stored as comma-separated value, or CSV files. CSV is a simple text format often used for storing tabular data. Think of it as a stripped-down version of Excel - just the data, no formatting or formulas. In fact, you can even open CSV files directly in Excel, if you want to take a look at them.\nWe’ve prepared these datasets for you and stored them in the course repository on GitHub. While Polars allows us to read files directly from remote locations, you can also download the files and the load them into Python from your local project directory.\nHere’s the code to read the three datasets we’ll use. Add these lines to a new cell in your notebook and run it.\n&lt; PAUSE &gt;\n\nLet’s break this down. We’re using the read_csv() function from Polars to load the data. This function takes a single mandatory argument: the file path, written as a string in quotes. The equal sign indicates that we assign the content of the file to the variable, listed on the left. The function is located in the polars package. This is why it is prepended with pl. prefix.\nThe read_csv() function returns a data frame. So, from now on, we can simply refer to h-hap underscore deaths (hhap_deaths) whenever we need to access the first dataframe without needing to re-import it.\n\nTo understand the data, it’s helpful to preview it. You do it in a few different ways. For instance, you can simply type the name of a dataset - like clean_fuels. It will display a preview of the first five and last five rows of the corresponding data frame.\nRows in a data frame are often referred to as observations or records, while columns are known as variables or features. If you want to see more rows than the default preview, you can use the .head() method and specify the number of rows to display:\n&lt; PAUSE &gt;\nYou may have noticed the parentheses around the code block. This little trick allows us to break the code into multiple lines without worrying about indentation. To keep things readable I will place each methods on a new line. Normally, Python enforces strict indentation rules. But because we wrap our code into outer patherentheses, we override that requirement. In this case, we prioritize readability over traditional coding style.\nPreviewing the first few rows gives us an initial sense of the dataset’s structure and content. If you’re curious about the last few rows, there’s also a .tail() method you can use in a similar way.\nFor a broader overview of your data, you can use the .describe() method:\n&lt; PAUSE &gt;\nThe .describe() method provides a statistical summary of numerical columns, including key metrics like the mean, standard deviation, minimum, maximum, and various quantiles. It’s especially useful for quickly gaining insights into large datasets.\nOne important note: if a column contains missing values, Polars displays them as null. Since Polars treats missing values as “contagious,” any operation involving them will also produce missing values in the output. This behavior applies to all statistical operations, and we’ll explore more examples later.\nBoth .head() and .describe() return a DataFrame, allowing you to chain these operations together. Method chaining is a powerful technique that makes your code cleaner, more readable, and easier to maintain.\nNow, here are a couple of challenges for you to practice method chaining:\n\nTake the first 25 records from clean_fuels DataFrame and generate a statistical summary.\nCompute the statistical summary of the entire dataset, and then extract only the quantile summaries for each column, including the minimum, maximum, and the 25th, 50th and 75th percentiiles.\n\n\n&lt; PAUSE &gt;\nSometimes a dataset may have so many columns that methods like .head() or .describe() don’t quite give a full overview. In such cases, Polars offers a particularly useful method: .glimpse().\nThe .glimpse() method displays the dataset’s structure horizontally, listing each variable as a row. This format makes it easier to scan through all columns, even on a limited screen. Here’s how it looks in action::\n&lt; PAUSE &gt;\nNow, here’s a question for you: What happens if you try to use glimpse() in a method chain? Can you chain the operation head() after calling glimpse()? What do you think the output will be?\n\n&lt; PAUSE &gt;\nThe answer is: no, you cannot. glimpse(), does not return you a data frame. Instead, the output of glimpse() is the text printout meant solely for viewing. No further operations can be applied to it. If you attempt to chain additional methods, you’ll get an error. Give it a try! Polars will throw an error saying that the head() method cannot be applied to a NoneType, which is the type of output glimpse() returns.\nIf you understand how to use head(), tail(), describe(), and glimpse(), you have powerful tools at your disposal to explore and familiarize yourself with any dataset before diving deeper into your analysis.\n\n\nVIDEO 3 Select/ Drop {greenscreen} ~ 12 min\n\nOne of the most common tasks in data analysis is selecting specific variables or columns from a dataset. Let’s start by pulling out the country information from the clean_fuels data. Pause the video for a moment and try running this code:\n&lt; PAUSE &gt;\nHere, we’re using the select() method to isolate a column. Notice how the column name is wrapped in the pl.col() function. This wrapper explicitly tells Polars that we’re referring to a column in the dataframe.\nBut here’s something cool - you can skip the pl.col() wrapper in certain cases! For example, this code:\n&lt; SMALL PAUSE &gt;\n…does the exact same thing as this:\n&lt; PAUSE &gt;\nPretty neat, right? The select() method can directly interpret strings as column names, making your code a little cleaner and quicker to write.\nWhen you wrap a column name in pl.col(), you’re creating an expression. An expression is like a recepie - it doesn’t do anything on its own. For example, if you run this code:\n&lt; SMALL PAUSE &gt;\n…nothing happens. It just returns something called an “unevaluated expression”. But when you evaluate that expression in the context of a dataset, it returns something useful. For instance:\n&lt; PAUSE &gt;\nHere, the select() method acts as an evaluation environment, turning the pl.col() expression into actual data. select() is one of the several methods in Polars that can evaluate expressions. While select() is highly versatile and can do other things as well, for now, we’ll focus on its simplest use case: extracting columns from a data frame.\n\nThe pl.col() wrapper is super flexible, and it’s going to be central as we build more advanced expressions in Polars. For instance, you can use pl.col() to refer to multiple columns simultaneously:\n&lt; PAUSE &gt;\nSometimes, typing out long column names can feel like a chore, especially when you’re working with many columns. But don’t worry - Polars makes it easy to select columns by their position in the dataset. For example, this code selects the second and third columns by their numerical index:\n&lt; PAUSE &gt;\nNote that the column indices in Polars are 0-based. That means the first column is index 0, the second column is index 1, and so on.\nWhat about negative numbers? They’re a handy shortcut for selecting columns from the back of the dataset. For instance, -1 refers to the last column, and this code will select the first and last columns:\n&lt; PAUSE &gt;\nA note of caution: Selecting columns by the order of their appearance can be risky. If your dataset’s structure changes, you might accidentally select the wrong columns. So, use the nth() method sparingly.\nNow, let’s talk about the opposite of select() - the drop() method. The drop() method removes specific columns from your dataset, leaving everything else intact. For example:\n&lt; PAUSE &gt;\nDropping is equivalent to selecting all columns except the ones you want to exclude. Here’s how could would write the dropping expression using in terms of selection:\n&lt; PAUSE &gt;\nThe pl.all() function refers to all columns, and the .exclude() method lets you refine the selection by removing specific ones.\nA quick reminder: dropping columns doesn’t modify your original dataset. It only affects the result of that query. Unless you explicitly overwrite the original dataframe, everything stays the same. So feel free to experiment!\nNow it’s your turn. Select the columns related to the number and the proportion of the people with access to clean fuels. Try both selecting by name or index, and dropping the columns you dont need.\n\nPlease, pause the video and try couple of different ways of selecting these columns.\n&lt; PAUSE &gt;\nGot it? Great! Both approaches, selection and dropping, give you the same result. Expressions like these make your analysis more dynamic and efficient, so you can quickly adapt to different datasets or scenarios.\n\n\nVIDEO 4 Selectors {greenscreen} ~ 7 min\n\nSelecting columns is such a common task that Polars has a dedicated module for it. It is called polars.selectors. This module provides a collection of methods specifically designed to simplify picking columns from a data frame. polars.selectors is often aliased as cs for convenience.\n\nLet’s make sure we import the selectors module:\n&lt; PAUSE &gt;\nAmong the most useful selectors are, of course, selectors by name and by column index. Although, for this we might not really need selectors, because those can be picked out with pl.col() and pl.nth().\n&lt; PAUSE &gt;\nSelecting first and last columns are so common, there are useful shorthands cs.first() and cs.last(). To select all columns other than the one you specified, you can use the tilde ~ operator. Tilde operator works with all methods in the cs. module and negates the selection. For example ~cs.last() refers to all columns other than the last one.\nSelectors can target columns based on their data types! For example, cs.numeric() picks all numeric columns. And if you want non-numeric columns, you can just negate it with ~.\nAnd now it is your turn! Practice selecting first, everyhing other than the first, as well as all non-numeric columns. Use selector class for this. Pause the video and give it a try!\n&lt; PAUSE &gt;\n\nFantastic work! With a wide menu of polars.selectors, plus the column and index-based expressions like pl.col() and pl.nth(), Polars gives you incredible flexibility in working with your data. These tools will become invaluable as we move into crafting more complex expressions.\nStay tuned - there’s a lot more to explore!\n\n\nVIDEO 5 Filter {greenscreen} ~ 12 min\n\nNow let’s talk about filtering - an essential part of data analysis. In Polars, you can use filtering to subset your data based on logical conditions, using the magic of expressions. Logical operations are one of the simplest and most common use cases for expressions. For example, you can compare every value in the region column to the string “Europe”. If there’s a match, Polars returns True; otherwise, it returns False.\nLet’s see how this works in code:\n&lt; PAUSE &gt;\nHere, the filter() method applies the logical condition, and only rows where the region is “Europe” are included in the result. Notice that for exact comparisons, we use the double equals sign ==. Similarly, for inequalities, we can use operators like &lt;=, &gt;=, &lt;, or &gt;. Not equal is spelled out as !=.\nBut filtering doesn’t stop there - you can combine multiple conditions to create more complex filters. Just write multiple expressions into the .filter() method, separating them with a comma!\nHere’s a challenge for you. Can you find all the European countries where the majority of the population lacked access to clean fuels the end of 2022? Take a moment to write this expression. Pause the video if you need to.\n&lt; PAUSE &gt;\n\nWhat did you get? Oh, wow! Over half the population of Bosnia still lacks access to clean fuels for cooking. That’s a striking insight!\nLet’s zoom in on Bosnia to better understand the data. Bosnia’s country code is “BIH”, but you can also use country name to filter, if you prefer.\n&lt; PAUSE &gt;\nWe are interested in tracking how the proportion of the population with access to clean fuels for cooking has changed over the years. Let’s create a plot, with the year on the x-axis and the proportion of population on the y-axis. If you remember from the Plotnine module, the dataset goes into the first argument of the ggplot function.\nHere’s one way to do this:\n&lt; PAUSE &gt;\nThis works, but the code feels a little cluttered. It’s not immediately clear where the dataset comes from.\nLet’s clean this up using the .pipe() method. The .pipe() method hands the data to the ggplot() function, placing it as the first argument.\n\nPipe keeps the code clean and modular. Everything after .pipe(ggplot) is Plotnine-specific code.\n&lt; PAUSE &gt;\nNice!\nWhat if you’re not sure how a country’s name is spelled in the dataset? For example, is it “Czech Republic” or just “Czechia”?\nIn this case, you can use partial string matching to find it.\n&lt; PAUSE &gt;\nHere, we use the str.starts_with() method, checking if the strings in the country column start with the letters “Cz.” Ah, there it is - “Czechia”!\nPolars offers several handy string operations. For example:\n\nstr.starts_with()\nstr.ends_with() and\nstr.contains()\n\nYou’ll see more of these as we progress, but these three are powerful enough to help you tackle the following challenge.\nFilter the data for your own country and visualize the proportion of people with access to clean fuels. Once you’re happy with the subset of your data, use ggplot and everything you’ve learned about Plotnine to create a polished visualization.\n&lt; PAUSE &gt;\n\nThis looks fantastic! Great work visualizing your country’s data.\nIn the next section, we’ll explore adding more columns to our dataset and practice advanced subsetting and visualization techniques. Stay tuned!\n\n\nVIDEO 6 Deaths data {greenscreen} ~ 3 min\n\nLet’s apply what we’ve learned about filtering to visualize the causes of death in some European countries.\n&lt; PAUSE &gt;\nThis dataset contains both summarized and detailed breakdowns of deaths for every country and year. Take a look at the column labeled cause_of_death. When this column says “All causes,” it represents the total deaths for that country and year - a sum of all the other rows.\nLet’s zoom in on Bosnia for a single year, say 2010, to understand this better.\n&lt; PAUSE &gt;\nOne of the rows is labeled “All causes” with 4,816 deaths. This total matches the sum of the individual causes of death. While it’s useful to have the total, it can lead to double counting if we include it in our analysis.\nNow let’s expand our view to include all European countries for which we have death data. We’ll exclude the totals and focus on trends for each specific cause of death. Faceting will help us visualize these trends country by country.\n&lt; PAUSE &gt;\nMost of the trends appear to be decreasing, which is good news. However, even with free y-axis for each country, the differences in scale make it hard to compare trends across countries. Look at Moldova! There’s a dramatic improvement in death cases here. Meanwhile, heart- and stroke-related deaths in the close-by Russia are on the rise.\nIt would be good to put these numbers in perspective using population of these countries. Is this something we could calculate from our clean_fuels dataset?\n\n\nVIDEO 7 Mutating with columns {greenscreen} ~ 11 min\n\nRemember, the clean_fuels data told us both the number of people with access to non-polluting fuels and the proportion of the population they represent, expressed as a percentage. With this, we can reverse-engineer the population for each country.\nTo create new variables, we use the with_columns() method. This method lets us add or modify columns using expressions.\nLet’s start by converting the percentage of people with access to clean fuels into a true proportion by dividing it by 100.\n&lt; PAUSE &gt;\nHere, we create a new column called prop. The expression starts with a column name, and then we specify the operation: dividing by 100. The prop on the left of the equal sign will become a new column name. Easy enough, right?\nNow that we have the true proportion, we can calculate the total population. Let’s divide the population by proportion.\n&lt; PAUSE &gt;\nNotice that I wrapped the division operation in parentheses to ensure it’s evaluated correctly. I also used the alias() method to specify a name for the new column: population. This code calculates and adds two new columns:\n\nprop: the true proportion of the population with access to clean fuels, and\npopulation: the estimated total population for each row.\n\nThis looks good. Lets assign it to a variable and inspect the updated dataset.\n&lt; PAUSE &gt;\nOh, look! We’ve got some NaN, or “Not a Number”, in the population column. These appear because we divided by zero, wherever the proportion was zero. Division by zero is, understandably, illegal in most places - and in Python, it results in NaN.\nNaN is a special marker for missing or undefined values. Missing values in Polars propagate through calculations, which means any further operations on these rows will also result in missing values. This is why the mean and standard deviation of the population column are also NaN in the summary.\nLet’s see how many rows in our dataset contain illegal population estimates. We can use the .is_nan() method, which evaluates whether a column contains a NaN value.\n&lt; PAUSE &gt;\nUh-oh! 35 rows! Perhaps fuel_types, the third dataset, could be a better source of population data?\nThe fuel_types dataset contains both the proportion of people using a specific cooking fuel and the absolute number of users. Since it includes multiple estimates for each country and year - one for each fuel type - it might give us more opportunities to calculate valid population estimates.\nGo ahead and add a population column to the fuel_types dataset. Save the extended data frame under a new name - we will need it later.\n&lt; PAUSE &gt;\n\nThis looks promising! Let’s have a look at our favorite Bosnia\n&lt; PAUSE &gt;\nInteresting! We still get some NaN values, for example, look here: in 2022 no one was using Kerosene to cook food. Thanks goodness! But now we get several estimates of Bosnia’s population - 3.0, 3.5, 3.53, 3.48 million. These slight differences arise from rounding imprecisions in the proportions or the number of people using the specific fuel type. While these variations are minor, they make it tricky to work with the data directly.\nWouldn’t it be nice to level out these differences, by say, averaging? In the next section, we’ll learn how to do just that: grouping and aggregating. Stay tuned!\n\n\nVIDEO 8 Summaries {greenscreen} ~ 14 min\n\nOne of the most important tasks for data analysts is creating summaries of the data, particularly by groups. In our newly prepared dataset, we’re interested in summarizing the total population for each country by year. In Polars, this is a two-step process. First, we define the groups using .group_by(). Then, we aggregate the data using .agg().\n&lt; PAUSE &gt;\nThis gives us a mean population estimate for each country and year. But you might notice something suspicious - some countries and years still show NaN values for the population. These missing values are a result of the division by zero we encountered earlier. To address this, we have a couple of options:\nWe could drop the missing values directly before averaging, using .drop_nans(); OR We could filter out zero-valued records in the “proportion of population” column, which was used in the denominator of the population estimate.\n&lt; PAUSE &gt;\nNow, let’s build on this idea. What if we wanted to summarize populations at a higher level - say, by region - and visualize how total population changes over time?\nGive it a try! I suggest you start by by calculating the average population for each country and year. Then, you can group this data by region and year, summing the populations for all countries in each region. Finally, plot the results.\nTake your time and think through what type of aggregation you would need to do.\n&lt; PAUSE &gt; \nWhen we visualize the results, the trends are striking. Populations in Africa and Southeast Asia are growing at a much faster pace compared to other regions. For Africa, this growth rate might even be accelerating. On the other hand, regions like Asia and the Western Pacific show signs of population growth slowing down. These insights help us understand global population dynamics and can inform policies in areas like health, infrastructure, and environmental planning.\nNow that we’ve explored population estimates and regional dynamics, let’s take on another challenge: examining the changes in the popularity of different cooking fuels worldwide.\nHere’s your new task: Visualize the total number of people using each type of fuel for cooking, for every year, worldwide.\n&lt; PAUSE &gt; \nThis plot looks fantastic! By aggregating by fuel type and year, we can clearly see trends in fuel usage globally. But we can take this further. What if we wanted to break this data down by region? Faceting would allow us to examine the trends within each region more closely.\n&lt; PAUSE &gt;\nWith this plot, we can see unique trends for every region. Across the globe, electricity and gas are becoming more prevalent. Yet, biomass remains crucial in regions like Africa and Southeast Asia. However, there’s still a challenge here. Because the number of people using each fuel type varies so widely across regions, it’s difficult to compare the energy mix within each region.\nTo make the energy mix clearer, we can represent the share of the population relying on each fuel type within a region. This requires dividing the number of people using each fuel by the annual total population for the region. We’ll use Polars’ .over() method to calculate these totals for subgroups without collapsing the data.\n&lt; PAUSE &gt;\nI wrap the expression into parenthesis and pass it to the .over() method which defines the scope for the .sum() operation. Note that this is different from aggregation, because the number of rows actually does not change. Lets visualize it!\n&lt; PAUSE &gt;\nWhat a graph! What a story! In Africa, biomass remains dominant, though natural gas is beginning to make inroads. The Americas, Europe, and Eastern Mediterranean rely heavily on gas, with electricity emerging as a significant player in Europe and the Western Pacific. Southeast Asia, meanwhile, is undergoing a rapid transition from biomass to gas - a remarkable shift in just a few decades. This visualization gives us a profound understanding of how the popularity of cooking fuels has evolved globally and regionally over time.\n\n\nVIDEO 9 Final plot {greenscreen} ~ 3 min\nIt’s time for you to take on another challenge - one that highlights the global reliance on coal and charcoal for cooking. Those are environmentally sensitive fuels and understanding thier use across the world is crucial. Create a column chart showing the top 10 countries by the number of people using coal and charcoal for cooking in 2022. Color the columns by continent. How many of these top-10 countries are in Africa?\n&lt; PAUSE &gt; \nSeven - possibly eight, if you include Somalia - of the top-10 countries using coal for cooking are in Africa! Many African nations rely heavily on these highly polluting and dangerous carbon-based fuels. Ensuring access to cleaner, safer energy sources is essential to improving health outcomes and accelerating the transition to sustainable energy.\nIn the next section, we’ll learn how to combine datasets through joins and reshape them using pivots - powerful tools for transforming and enriching our analyses. See you soon, as we dive deeper into the art and science of data manipulation!"
  },
  {
    "objectID": "pivotjoin.html",
    "href": "pivotjoin.html",
    "title": "Pivots and joins",
    "section": "",
    "text": "VIDEO 1 BFFP data {greenscreen} ~ 9 min\nHello and welcome back to Data Literacy with Python!\nWe’re continuing our exciting journey into data wrangling — a cornerstone of analysis and storytelling. If you’ve been following along, congratulations! You’ve covered a lot of ground and gained some serious data skills.\nLet’s quickly recap:\n\nWe learned how to subset data, selecting specific columns using .select(), .drop(), and the powerful suite of polars.selectors. We used .head(), .tail() and .filter() to pick out key observations.\nWe mastered creating new columns with expressions inside .with_columns().\nFinally, we explored summarizing data using .group_by() and .agg(), generating meaningful insights from our datasets.\n\nThese are all essential skills, and you’re doing great!\nBut now, it’s time to level up. Today, we’re tackling data reshaping and joins, two powerful techniques for reorganizing and enriching datasets. I also promised you some nice-looking tables, and I intend to deliver! We’ll be using the amazing great_tables library for our table designs. Let’s load up the libraries and dive right in.\n&lt; PAUSE &gt;\nToday’s dataset comes from the Break from Plastics environmental campaign — a sample with a powerful story. Here’s a quck overview:\n&lt; PAUSE &gt;\n\nHave a look at the code that brings this data into our workspace:\n&lt; PAUSE &gt;\nIn your notebook you can see the code for importing the data, as well as a DataFrame containing a data dictionary — a detailed description of the variables in this dataset.\nSo far, we’ve always imported data from CSV files or pre-built datasets. But now, it’s time to talk about creating data frames by hand. This is super handy when working with small examples, prototypes, or mock data. To do that, we need to talk about two foundational Python data structures: dictionaries and lists.\nThink of a dictionary as a way to describe an object. It’s a collection of “key-value” pairs—like writing down standard characteristics of something along with their values. Dictionaries are specified with curly brackets {}. Let’s say I want to describe my bike:\n&lt; PAUSE &gt;\nHere, each characteristic—like Type or Size—has one value. Easy, right? But what if I also wanted to describe the bikes of my twins? I’d need three records, not one.\nThis is where lists come in. A list is a collection of items—typically of the same type—and it’s denoted with square brackets []. Let’s use lists to describe all the bikes in my garage:\n&lt; PAUSE &gt;\nNow, we have a dictionary of lists, representing three bikes. To turn this into a Polars DataFrame, all we need to do is pass it to the pl.DataFrame() function:\n&lt; PAUSE &gt;\nAnd just like that, we’ve created a data frame by hand! This is a simple yet powerful way to structure and manipulate small datasets.\nOur plastics_docs specifies three characteristics: the variable names (Variable), their data types (Class), and their descriptions (Description).\nThis table essentially acts as documentation for the plastics_df. But let’s face it — raw data frames, while functional, don’t always look polished or presentation-ready. That’s where the Great Tables package comes in.\nGreat Tables is like a graphic design toolkit for your tables! It introduces a “grammar of tables,” similar to how plotnine provides a “grammar of graphics.” This makes it super easy to transform plain data frames into beautifully styled tables with minimal effort.\nThe core function in Great Tables is GT(), and it works similarly to how we use ggplot for creating plots. Let’s take a sneak peek at its capabilities by styling our plastics_docs data frame. Here’s how we do it:\n&lt; PAUSE &gt;\nVoilà! .opt_stylize() method has some pre-built styles, which we used to convert a boring data frame into a polished and professional-looking table, ready to be shared or included in reports. Don’t worry about memorizing the details just yet — we’ll explore GT() more thoroughly in the upcoming sections.\nBefore we move on, let’s take a quick look at the data itself. Here’s a snippet of the first five rows of the plastics_df:\n&lt; PAUSE &gt;\nAs you can see, the first few variables look familiar. They describe general metadata, like the region, country, and year. But let’s focus on the variables starting from empty and going down to pvc. These columns count the number of plastic pieces of different types collected during the cleanup.\nThe grand_total column sums up all these individual plastic counts. Finally, the last two columns—num_events and volunteers—capture operational details:\n\nHow many trash counting events took place in each country during a given year?\nHow many volunteers participated in these campaigns?\n\nThis dataset offers a wealth of insights into plastic pollution patterns across the globe. By organizing, reshaping, and visualizing this data, we’ll uncover powerful stories about the environmental challenges we face—and the steps we can take to address them.\nLet’s take a closer look at the data. Check out this very first row for Argentina. Notice the parent_company column contains the value: “Grand Total.” This suggests that this first row contains the totals for all Argentinian records in 2019.\nLet’s look at the next year:\n&lt; PAUSE &gt;\nHere’s something curious: for the year 2020, the rows with country totals aren’t marked with “Grand Total.” Instead, the parent_company field is left blank, or in technical terms, it’s marked as missing - Null. Hmm! What do we do with those?\nNull values aren’t just limited to parent_company. Let’s take a look at records collected in 2019 from unidentified locations.\n&lt; PAUSE &gt;\nBefore we dive deeper into analyzing top contributors to plastic waste, let’s calculate the totals per country and year ourselves. Why?\nWell, the dataset has pre-computed totals marked with “Grand Total” or Null in the parent_company, but the logic seems inconsistent. Recomputing the totals ensures transparency and accuracy in our analysis.\nHere’s how we start:\n&lt; PAUSE &gt;\nLet’s break this down:\n\ndrop(\"grand_total\"): The grand_total column is a pre-computed sum of all plastic types, which we can recalculate if needed.\nExclude “Grand Total” rows: We filter out rows where the parent_company column is populated with the phrase “Grand Total.”\nExclude Null values in parent_company column: These rows lack a meaningful company label and often represent aggregated data.\n\nBy cleaning the data in this way, we ensure that our analysis is based on individual contributions, not pre-summarized totals.\n\n\nVIDEO 2 Aggregation {greenscreen} ~ 3 min\nNow, let’s talk about the two special columns in our dataset: num_events and volunteers. These capture the number of counting events and the number of volunteers who participated in the cleanup campaigns.\nBut here’s the catch. These columns don’t vary within groups of rows for the same year and country. Instead, they represent aggregate metrics that make sense at the country-year level, but are less meaningful at the company level.\nFor now, let’s set aside the rankings of companies responsible for the most plastic waste. Instead, we’ll focus on the campaign itself — the Break from Plastics community engagement on a country level.\nIt’s time to roll up our sleeves and aggregate the data at the country-year level. First, let’s think about the groups we’re interested in. Our primary group identifiers would be region, country_code, country, and year. These columns define the unique groups for aggregation since each country belongs to one region and has a unique country code.\nNow, let’s decide what to aggregate. We are interested in aggregating the eight columns from empty to pvc. These need to be summed for each group. However, columns like num_events and volunteers shouldn’t be summed because their values remain constant within each group. Instead, we’ll take the first non-null value in each group.\nWith the powerful multi-column expressions in polars, we don’t need to write repetitive code for each plastic type. Using selectors, we can quickly choose and process groups of columns. I encourage you to try it out now.\n\n\n\n\n\n\nChallenge\n\n\n\nAggregate the plastic counts at the country-year level, ensuring that the num_events and volunteers columns remain intact. When you’re done, store the result into a new variable called plastics_countries_df.\n\n\nTake a moment to pause the video and give it a try.\n&lt; PAUSE &gt;\nFantastic! Now that we have our cleaned and aggregated dataset, let’s talk about total plastic counts.\nTo calculate the total number of plastic items inspected, you’d typically list all relevant columns and sum them up: total_count=pl.col(\"empty\")+pl.col(\"hdpe\")+pl.col(\"ldpe\")+ and so on. But imagine if you had hundreds of columns instead of eight! This approach would quickly become overwhelming.\nLuckily, there’s a clever trick to make this task more manageable: pivoting. By reorganizing the data into a different format, we can simplify calculations and enable more flexible analysis.\nIn the next section, we’ll dive into pivoting and explore how to reshape plastics_countries_df for deeper insights.\nLet’s keep going!\n\n\nVIDEO 4 Wide and long data {greenscreen} ~ 9 min\nLet’s take a moment to reflect on two common ways of organizing the same data: wide and long formats.\nOn the left, we see a wide format dataframe. Here, each row represents a single observation, and the variables are spread across multiple columns.\nOn the right, we have the same data in a long format. In this version:\n\nThe first two columns, known as ID or index variables, are repeated for each observation.\nA new column, plastic_type, gathers the names of the measured variables.\nThe final column, quantity, contains the values corresponding to those variables.\n\n&lt; PAUSE &gt;\n\n\n\n\n\n\n\nThese two formats represent the exact same data, but the way they’re structured can have a big impact on how we work with them.\n\nWide Format:\n\nEasier for humans to read, especially in spreadsheets.\nGood for summarizing data at a glance.\n\nLong Format:\n\nPreferred for computations, visualizations, and many data analysis tools.\nSimplifies aggregations and calculations.\n\n\nWhen working with data, being able to switch between these formats is essential. Many analysis tasks, like creating summaries or visualizations, become much easier when the data is in long format.\nThe process of transforming data from wide to long is called unpivoting or melting. This makes sense because the data seems to “melt” down into fewer columns while increasing the number of rows.\nWhen we unpivot data, we typically end up with three columns:\n\nID (or Index): These are the identifying variables that stay constant across the observations.\nKey (or Variable Name): This gathers the names of the measured variables into a single column.\nValue: This records the corresponding values for each of the variables.\n\nIn our case the ID variables are region, country_code, country, and year. We will name the key column plastic_type. The column containing values will be called quantity.\nOur first task is to unpivot the plastic count columns, transforming plastics_countries_df into a long format. This will simplify how we analyze and summarize the data.\nAs we’ve seen, the long-format data typically consists of three primary columns:\n\nID columns that uniquely identify each group or observation.\nKey column that gathers variable names.\nValue column that stores the corresponding values for each variable.\n\nLet’s begin by checking out the documentation for the .unpivot() method. This function gives us precise control over how to reshape the data.\n&lt; SMALL PAUSE &gt;\n\n.unpivot() takes four arguments:\n\non: Specifies the columns to be unpivoted.\nindex: Indicates which columns to keep as ID columns. You can use polars.selectors to specify these.\nvariable_name: Sets the name for the Key column.\nvalue_name: Sets the name for the Value column.\n\nNote that the on argument is optional. If you don’t specify it, any column not listed as an index will be unpivoted automatically.\nHere’s how we can transform the dataset into a long format. We can select the index columns either by name or by position. We can also, optionally, specify the on argument to make it extra clear which columns should be melted.\n&lt; PAUSE &gt;\nBoth approaches produce the same result:\n\nThe columns region, country_code, country, year, num_events, and volunteers are preserved as ID columns.\nThe plastic types (e.g., empty, hdpe, etc.) become the values in the plastic_type column.\nThe corresponding counts are stored in the quantity column.\n\nOnce the data is in long format, it becomes much easier to perform calculations. For instance, let’s group the data by region, country_code, country, year, num_events, and volunteers, and calculate the total quantity of plastic collected:\n&lt; PAUSE &gt;\nAnd there we have it — a simple aggregation of total plastic quantities by country and year! This process highlights the power of unpivoting: it organizes our data in a way that makes calcultions across multiple columns more straightforward.\nWe now have 106 records of Break From Plastics campaign activity in every country in 2019 and 2020. We have total number of events every year, number of volunteers who participated every year and the quantity of plastics collected and sorted through.\nHow would you answer this question: What are the top 5 countries in terms of the growth in productivity, defined is the number of plastic processed per event? So we are not just interested in productivity every year, but also how it has grown from 2019 to 2020. I am looking at Taiwan, for example that engaged the mind-blowing 31 thousand volunteers and sorted through 120 thousand pieces of plastic in just 2 huge events held in 2019. Now that’s an achievement! Especially if they managed to keep the level of engagement up the following year.\nWe can certainly calculate productivity per event as quantity divided by num_events, but how would you calculate the growth in productivity for every country? We need to pivot the productivity column across years and calculate the growth from the pivoted data.\n&lt; PAUSE &gt;\n\nPivot method has these main arguments:\n\non specifies which column(s) should be pivoted\nindex optional selector for which columns will NOT be pivoted - i.e. those that will stay unchanged\nvalues where should the values for the pivoted columns come from.\n\nThe index column is optional. It can be inferred from the other arguments, meaning that everything which is not in on or values will be treated as an index set of columns.\nYou can pivot multiple columns at once. Just indicate several columns in the values argument and the pivoted table will have one column for combination of the factor in on and each or the values columns. So here in our example I have a categorical variable year with 2 levels (2019/2020). I am interested in seeing productivity split by year into productivity_2019 and productivity_2020 columns. But I can at the same time pivot quantity, so I also have quantity_2019 and quantity_2020.\nBefore I do the pivot, I want to drop the num_events and volunteers since I am not doing anything with them (even though you could argue it makes sense to pivot them by year as well). But at this time, we are not interested in these columns so we can just drop them. The reason I do it, is so that I dont have to specify the index argument in the pivot function, relying on polars to figure it out for me.\n&lt; PAUSE &gt;\n\n\nVIDEO 5 great tables {greenscreen} ~ 6 min\nBefore we go any further, I want to quickly introduce you to the power of great_tables for producing engaing and informative tables. We will create a nice-looking table showing top-10 countries with the most impressive productivity growth. As we discussed earlier, the core function is GT() you also know that the default styling can be done with .opt_stylize() method. This is enough to produce decent looking table, but I want to show you a few more tricks.\nFirst of all, we have two groups of columns which are somehow related. A group of columns about quantity of plastic processed and a group of columns about productivity. We can introduce spanners - a grouping of columns in the table, where we can place a meaningful label. In GT this can accomplished with .tab_spanner(). As almost all methods in great_tables(), .tab_spanner() relies on polars selectors. So we can scoop all columns starting with prod under the Productivity spanner, and all columns starting with quantity under the other spanner.\nThe second group of functions in great_tables is related to columns. These methods will start with .cols_ You can move and hide columns using .cols_move() and .cols_hide() without modifying the underlying data. Sometimes, it might be useful because the data is also used for something else, other than the table visualization and you don’t want to shuffle the columns around just because you need to show them in the table in a particular order. In our case, we want to move the quantity group closer to the beginning of the table. There’s also a convenient column labeler .cols_label(), which allows you to create arbitrary visual lables for columns without modifying the data. Again, in our table, because we introduced table spanners, we want some column names to be duplicates of each other, for example year numbers. But in polars all column names should be unique.\nFinally, the third block of functions I want to talk to you about is formatting. You can specify special formatting rules for one or more columns using .fmt_ method. There are many pre-defined methods for formatting numbers, integers, percentages etc, including exoting column contents like flags and icons. The 3-letter country codes can serve as indentifiers for country flags and we will use this column format in our table. As with many other function in GT we are relying on polars.selectors to fetch the columns and then specify decimals, forced +/- sign, etc.\nWe will use the default styling in .opt_stylize() but later in this module we will have a look at custom styling function for creating more effective communication.\nAs with plots, it is good to annotate the table. You can add table header with title and subtitle, as well as the caption, called source note where you cite the souce of your data.\n&lt; PAUSE &gt;\nYou may argue whether this table is informative, but it sure looks nice. And in the process of preparing it we have learned quite a lot about pivoting and great tables.\nI think it is your time to practice. Identifying the different types of plastic requires good knowledge of the different types of materials. Some materials were identified as belonging to predefined classes, while other samples were left unclassified (and ended up in the o (“other”) or empty categories). Identification rate (measured in %) is the proportion of total pieces of plastic that were assigned to the predefined categories (and not to “other” or “empty”). We wonder if the increase in the engagement (number of volunteers signed up for BFFP events) leads to improvement of identification rate?\n&lt; PAUSE &gt;\n\n\n\n\n\n\nChallenge\n\n\n\nPut together a table of top 10 countries where identification rate improved the most between 2019 and 2020. Insrease in the rate should be measured as a simple difference between the two rates. Also, please calculate the percent increase/decrease in volunteer engagement for these countries.\n\n\nThere could be several ways to solve this task. In my table the top country is South Africa and the bottom country in Bangladesh. Pause a video and give this task a try! When you come back we will walk through a solution together and discuss the learning from the pivoting lesson.\n&lt; PAUSE &gt;\nYou are done already? If not, then here are some tips:\n\nYou can create a boolean variable of whether a piece of plastic is identified or not. Include it into the list of grouping variables and calculate totals by it. Then pivot on this variable. This will allow you to calculate the identification rate.\nAlternatively, you can calculate the identification rate using the .over() method as we did in the previous module and then just filter out the rows you dont need.\nPivot both volunteers and id_rate on year. Make sure you drop all other columns which are not part of your index valriables.\nIts a good idea to filter out records containing null in either one of the growth metrics. This will eliminate countries which did not participate in BFFP campaigns one of the years.\n\nIf you find these tips helpful pause again and try solving the task. Otherwise, hang on, we will discuss the solution to this challenge together momentarily.\n&lt; PAUSE &gt;\nWhew! You did it! Was it hard? Well, I am sure you have learned a lot and should be proud of yourself! One thing you can be sure of is that although this task is a little contrived and the data is a little dirty, the process of solving it not untypical for the daily job of a data analyst. The data analysis tasks in real life do not come nicely packaged and most of the data requires substantial amount of care before it can be usable.\nSo, let us talk through this solution together.\n\nThe first unpivoting should be quite straightforward. We list all ID columns in the index argument and provide new names for the key and value columns, just as we discussed earlier.\nThen we create an “plastic_unidentified” variable in our data frame, which indicates whether a plastic waste is indentified by type or not. If the plastic_type takes “empty” or “o” value, our plastic_unidentified variable will contain the boolean value of True. You know how to write pl.col(\"plastic_type\")==\"empty\" | pl.col(\"plastic_type\")==\"o\" with explicit logical OR, but here I show you how to use .is_in() method with a list argument. Same result!\nThen we group by everything including this new column and summarize. This should be familiar.\nWe dont need num_events column. Let’s drop it so it does not interfere in our pivoting.\nWe are interested in calculating the proportion of identified plastic. At this moment you have two choices:\n\nCalculate the proportion using .over using expression (pl.col(\"quantity\")/pl.col(\"quantity\").sum()) and then only keep the records where plastic_unidentified is False (using ~ for negation). The relevant lines would look something like this:\n\n\n&lt; PAUSE &gt;\n- Alternatively, we can pivot the `plastic_unidentified` and compute the proportion across the two new columns (named True and False). This option would look something like this:\n&lt; PAUSE &gt;\n\nFinally we need to pivot on year. Here we pivot both volunteers and newly created id_rate because our task was to show change in both. As we discussed earlier, this will create total of 4 columns because there are 2 levels in year column and we are pivoting 2 variables.\nOur last step is to calculate the volunteer growth and the identification rate increase. One is percent growth and the other is a simple difference in percent values. Drop the records where either one of them is Null and sort by id_rate_increase. Then take top 10 records only.\n\n\n\nVIDEO 6 intro to joins {greenscreen} ~ 10 min\nIn a typical data analysis project, data often comes in multiple files or tables, each describing a different aspect of a problem or phenomenon. These separate tables might exist because the data comes from different sources, or simply because it’s easier to manage that way. As an analyst, you’ll often need to combine related datasets based on shared characteristics—such as countries, time periods, or other common identifiers. This is the case with the original gapminder data, collected by Hans Rosling, which consists of multiple tables linked by country and year.\nOne of the most common ways to merge datasets is through a join. A join allows you to bring data from one table into another based on matching values in one or more columns. In Polars, this is done using the .join() method.\nLet’s look at an example. We have two small datasets: one lists the countries in the Nordic Council, including their area and population; the other contains information on members of the European Free Trade Association (EFTA), with data on exports, imports, and GDP (in billion USD). Our goal is to combine these datasets in a meaningful way.\n&lt; PAUSE &gt;\nIceland and Norway are members of both the Nordic Council and EFTA. Let’s say we want to calculate GDP per capita for these countries. The challenge? GDP is stored in one table, while population is in another.\nHow do we solve this? That’s right—we need to join the tables! By merging them based on the shared “country_name” column, we can bring all the necessary information together. Once combined, we can simply use the .with_columns() method to compute GDP per capita.\nThis approach lets us seamlessly integrate data from different sources and perform new calculations with ease.\n&lt; PAUSE &gt;\nNotice that after our join, only Iceland and Norway remain in the result. The other Nordic countries were dropped because they had no match in the efta_countries table. This type of join is called an inner join, and it’s the default behavior in Polars.\nBut what if we don’t want to lose the unmatched records? That’s where a left join comes in.\nA left join keeps all records from the table on the “left” and adds matching information from the table on the “right.” If no match is found, the missing values are filled with null. Since we’re starting with nordic_council, it serves as our “left” table.\nLet’s see it in action:\n&lt; PAUSE &gt;\nNow, the result includes all five Nordic countries. The export, import, and gdp_bln columns contain values for Iceland and Norway but show null for the others, since they’re not part of EFTA.\nThis approach ensures that we retain all countries from our original dataset while still incorporating relevant details from the second table.\nJust as a left join keeps all records from the left table, a right join does the opposite—it preserves all records from the right table and only includes matching data from the left.\n&lt; PAUSE &gt;\nIn this case, the result contains four records - one for each country in the efta_countries table. Notice that the country_name column has shifted in the output, appearing after population and area. Also, since Switzerland and Liechtenstein weren’t in the nordic_council table, their population and area values are null.\nA full join combines all records from both tables, keeping everything from the left and right datasets. If there’s no match, the missing values are filled with null\n&lt; PAUSE &gt;\nThis results in seven records—one for each unique country appearing in either dataset. However, you’ll notice that the country_name column is now duplicated, explicitly showing which table contributed each row.\nTo merge these into a single country_name column, we can use the coalesce=True argument.\n&lt; PAUSE &gt;\nNow, we have a clean table that combines all available data while keeping a single country_name column.\nHowever, notice that five out of seven records contain missing values in at least some columns. For two of them, population and area are missing, while for the other three, economic data (gdp, export, and import) is absent.\nOne particularly useful - but less commonly discussed - join type is the anti-join. Unlike a left join, which retains all records from the left table, an anti-join returns only the rows that would have gone missing in a left join.\n&lt; PAUSE &gt;\nHere, the result includes only the unmatched records from the nordic_council table - countries that do not appear in the efta_countries dataset. Also, note that only columns from the left table (nordic_council) are shown, as no matching data exists in the right table.\nThe anti-join is particularly useful for diagnosing and troubleshooting joins. Before performing a left join, it helps identify which records will be left without a match, allowing analysts to spot inconsistencies or missing data early.\nTo wrap up, here’s a visual summary of the five join types we’ve covered:\n\nInner join: Keeps only matching records from both tables.\nLeft join: Keeps all records from the left table and matches from the right.\nRight join: Keeps all records from the right table and matches from the left.\nFull join: Combines all records from both tables, filling in missing values.\nAnti-join: Returns only unmatched records from the left table.\n\n\nVenn diagrams can sometimes be misleading, but in this case, they’re incredibly useful. I find myself referring back to this image whenever I need to decide which type of join to use in a given situation.\nJoins can be performed on one or multiple columns. If you need to join on more than one column, you can provide a list of column names to the on argument: on=[\"country\", \"region\"].\nHowever, there are a few things to watch out for:\n\nDuplicates: If either table contains duplicate values in the join column(s), Polars will match every instance on the left with every instance on the right - leading to data duplication.\nMissing values: If a column used for joining contains null values, it may cause unexpected behavior or missing records in your result.\n\nBefore performing a join, always check for duplicates and missing values in your key columns. Polars stores updated information about missing values in a DataFrame. This information can be quickly retreived by the .null_count().\nFor more details, I recommend checking out the official Polars documentation on the .join() method — it covers syntax, additional arguments, and advanced use cases.\n\n\nVIDEO 7 joining household pollution data {greenscreen} ~ 3 min\nIn the previous module, we explored WHO data on household pollution, which was spread across three CSV files:\n\nhhap_deaths – Causes of death linked to household pollution.\nfuel_types – Types of fuel used for cooking in various countries.\nclean_fuels – Proportion of the population with access to clean cooking fuels.\n\nWe wanted to put the deaths attributable to household pollution into context by comparing them to the total population of each country in the respective years. However, the hhap_deaths dataset lacked population estimates. To work around this, we inferred population estimates by aggregating the fuel_types dataset and saved it as pop_years_df. Now, our goal is to merge these population estimates into hhap_deaths so that we can analyze deaths per million people.\n\n\n\n\n\n\nChallenge\n\n\n\nVisualize European household pollution-related deaths per million population, broken down by cause of death:\n\nPerform a left join between hhap_deaths and pop_years_df using region, country, and year.\nCalculate deaths per million by dividing the deaths column by the inferred mean population.\nFilter the data to include only European countries and exclude summary rows (to avoid double-counting).\nPlot the results using ggplot.\n\n\n\n&lt; PAUSE &gt;\nHere’s the code which allows us to observe trends in household pollution-related deaths over time, broken down by cause of death and country.\nJoins allow us to break free from the constraints of a single dataset and uncover deeper insights by integrating data from multiple sources. Mastering the skill of merging datasets is a powerful addition to your data analysis toolkit, enabling you to answer more complex questions and build richer analyses.\nIn the next section, we’ll explore even more techniques for combining data from different sources. Stay tuned!\n\n\nVIDEO 8 stacking {greenscreen} ~ 7 min\nAnother common operation in data analysis is stacking datasets vertically—essentially appending one dataset to another. This works when the datasets have the same column structure and represent different parts of the same kind of data. By stacking them together, we can create a larger dataset that consolidates information from multiple sources.\nGoing back to our household air pollution case study, we worked with three datasets:\n\nhhap_deaths: Records of deaths linked to household air pollution.\nfuel_types: Information on fuel types used for cooking in different countries.\nclean_fuels: The proportion of the population with access to clean cooking fuels.\n\nEach dataset includes three key columns that describe the country of observation: region, country_code, and country. While these datasets share many of the same countries, they don’t completely overlap. Our goal is to create a master list of all countries, along with their codes and regions.\nSince each dataset spans multiple years, they contain duplicate entries for the same country. This issue will grow when we stack the datasets together, so we need to ensure our final dataset contains only unique records.\n&lt; PAUSE &gt;\nHere, we create a temporary object, idcols, to store the column selector for the three country-related columns. This makes our code more readable and reusable, as Polars selectors can be stored separately and used in different querying contexts.\nNext, we want to verify that our country codes match the official ISO country codes. We have a reference file containing all Alpha-2 and Alpha-3 country codes. Before performing a left join, we first check for any records that don’t have a corresponding ISO code.\nSince the column names in our dataset and the ISO file don’t match, we explicitly specify the columns to join on using left_on and right_on. The anti-join returns an empty table. It means all our country codes successfully matched an entry in the ISO dataset - an important step in ensuring data integrity before further analysis.\n&lt; PAUSE &gt;\nNow that we’ve combined our data from multiple sources, let’s dig a little deeper into the completeness of our dataset. Specifically, we want to find out how many countries are missing from our combined household air pollution dataset.\n&lt; PAUSE &gt;\nThis looks like another classic case for an anti-join. In this query, we start with the full list of ISO codes and perform an anti-join with our combined country_regions dataset. This will return only those countries that are in the ISO list but not in our merged dataset.\nThe result? We discover that 54 countries are missing from our combined dataset. Many of these are smaller island nations, where data collection might be too limited due to their size or population. Interestingly, the list also includes Antarctica, which, while not a country, has temporary settlements of scientists from around the world. We can only hope that these researchers use clean cooking fuels during their stay!\nIn this section, we focused on vertical stacking, a powerful technique for combining datasets with the same structure. By stacking datasets, we’re able to consolidate data from multiple sources into a single, larger dataset.\nWhile vertical stacking is often useful, there could be situations where horizontal stacking might seem appealing. Horizontal stacking requires that the row order is identical in both datasets, which can be risky. It is often much safer to perform a join, using a unique identifier like country code, year or other ID columns. Joins ensure that the data aligns correctly without depending on row order, providing a much more reliable way to combine data coming from different sources.\nBy mastering joins and stacking techniques, you’ll be able to take full advantage of multiple datasets, transforming them into a cohesive, insightful resource for analysis."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Literacy with Python",
    "section": "",
    "text": "LINDA:\nWelcome to Data Literacy with Python!\nHi, I am Linda Hartman. I am an instructor of Data Analysis and Statistical Learning at Lund University\nDMYTRO:\nAnd I am Dmytro Perepolkin. I help Linda develop interactive classroom experience through code-along sessions and hands-one excercises to help our students get started on their data analysis journey.\nLINDA:\nLet me ask you something: Can you imagine living in today’s world but being unable to read? Think about it—street signs wouldn’t make sense, advertisements would just be noise, and most of the internet? Completely out of reach.\nNow, even with videos and voice assistants everywhere, written text is still the backbone of how we communicate and navigate life. Without it, you’d feel lost.\nBut here’s the thing: today’s world doesn’t just run on words. It runs on data.\nDMYTRO:\nEvery day, we’re creating over 400 million terabytes of data. That’s every single day. And here’s a wild stat—90% of all the world’s data was created in just the last two years.\nThis explosion of information is transforming how we make decisions, whether it’s in business, science, or society as a whole. To keep up, you need to know how to make sense of it.\nData isn’t just numbers on a screen—it’s stories waiting to be uncovered. And understanding data has become just as important as being able to read or write.\nThat’s where this course comes in.\nLINDA:\nWe’re going to teach you how to take raw, messy data and turn it into something meaningful. You’ll work with rectangular data—the kind you find in spreadsheets or databases.\nAnd don’t worry—this isn’t just about crunching numbers. It’s about answering real-world questions, solving problems, and making decisions based on insights you uncover.\nBy the end of this course, you’ll have the skills to transform data into knowledge.\nDMYTRO:\nLet’s talk about the tools you need to work with data.\nYou might be tempted by low-code or no-code solutions—those point-and-click interfaces that make everything seem so easy. And sure, they’re great for quick wins. But when it comes to serious data analysis, they have some big limitations.\nData analysis isn’t just about getting answers—it’s about getting credible answers.\nTo trust your insights, you need to leave a trail. Think about it—during analysis, you make dozens of tiny decisions:\n\nWhich part of the data should you focus on?\nWhat variables should you use?\nWhich patterns caught your eye?\n\nEvery decision shapes your results. And if you—or anyone else—can’t retrace those steps, how can you be sure your conclusions hold up?\nThat’s why scripting your analysis is so important.\nWith a script, every step is recorded. You can spot mistakes, refine your work, or pick up right where you left off—even months later. Low-code tools? They don’t give you that kind of transparency.\nSo, what’s the best language for scripting your data analysis?\nLINDA:\nThe answer is Python.\nPython is the world’s most popular programming language, and for good reason. Created in 1990 by Guido van Rossum, Python has become the go-to language for everything from building websites to powering cutting-edge AI. It may not be the fastest language out there, but it’s arguably the most readable. And in today’s data-driven world, readability matters more than ever.\nThe Python ecosystem for data analysis is enormous. Whatever your question, there’s a good chance Python has a library—or ten—that can help.\nData analysis is unique—it’s less about traditional programming and more about crafting a story with your data. Your code should be clear and intuitive, not just for you, but for anyone who needs to understand your work. And that includes “future you”—because six months from now, you might not even recognize your own analysis without clear documentation!\nSo, as we dive into this course, we’ll emphasize simplicity, transparency, and readability. Because great analysis isn’t just about crunching numbers—it’s about telling a story that stands the test of time.\nDMYTRO:\nData analysis is evolving. Today, some of the most cutting-edge tools are built on high-performance programming languages like Rust, Java, or C++. Why? Because these languages are fast—lightning fast. But here’s the best part: you don’t need to write in these languages to enjoy their benefits.\nModern tools now separate the user interface from the engine. That means the algorithms working behind the scenes are the same, no matter which scripting language you use.\nInitiatives like Apache Arrow go even further—they create standardized data formats, making it easy to move between tools and platforms without losing performance or compatibility.\nIn this course, we’re diving into tools built on Rust—one of the fastest, most efficient programming languages out there. Specifically, we’ll use uv for managing packages and environments and polars for data wrangling.\nThese tools are not just fast—they’re scalable.\nThe examples we’ll explore together are small—easy to follow and understand. But don’t let that fool you. The same tools we use here can scale effortlessly to handle datasets with billions of rows, processed across dozens of parallel machines.\nWhat’s even better? The interface doesn’t change.\nLINDA:\nSo whether you’re working on a personal project, academic research, or a large-scale business application, the skills you gain here will translate directly to the real world.\nThe datasets may be small, but the questions and challenges we tackle are universal. By the end of this course, you’ll be equipped to uncover meaningful insights from your own data, no matter its size or complexity.\nLet’s get started on this exciting journey into the world of data literacy!"
  },
  {
    "objectID": "index.html#video-1-introduction-studio-12-min",
    "href": "index.html#video-1-introduction-studio-12-min",
    "title": "Data Literacy with Python",
    "section": "",
    "text": "LINDA:\nWelcome to Data Literacy with Python!\nHi, I am Linda Hartman. I am an instructor of Data Analysis and Statistical Learning at Lund University\nDMYTRO:\nAnd I am Dmytro Perepolkin. I help Linda develop interactive classroom experience through code-along sessions and hands-one excercises to help our students get started on their data analysis journey.\nLINDA:\nLet me ask you something: Can you imagine living in today’s world but being unable to read? Think about it—street signs wouldn’t make sense, advertisements would just be noise, and most of the internet? Completely out of reach.\nNow, even with videos and voice assistants everywhere, written text is still the backbone of how we communicate and navigate life. Without it, you’d feel lost.\nBut here’s the thing: today’s world doesn’t just run on words. It runs on data.\nDMYTRO:\nEvery day, we’re creating over 400 million terabytes of data. That’s every single day. And here’s a wild stat—90% of all the world’s data was created in just the last two years.\nThis explosion of information is transforming how we make decisions, whether it’s in business, science, or society as a whole. To keep up, you need to know how to make sense of it.\nData isn’t just numbers on a screen—it’s stories waiting to be uncovered. And understanding data has become just as important as being able to read or write.\nThat’s where this course comes in.\nLINDA:\nWe’re going to teach you how to take raw, messy data and turn it into something meaningful. You’ll work with rectangular data—the kind you find in spreadsheets or databases.\nAnd don’t worry—this isn’t just about crunching numbers. It’s about answering real-world questions, solving problems, and making decisions based on insights you uncover.\nBy the end of this course, you’ll have the skills to transform data into knowledge.\nDMYTRO:\nLet’s talk about the tools you need to work with data.\nYou might be tempted by low-code or no-code solutions—those point-and-click interfaces that make everything seem so easy. And sure, they’re great for quick wins. But when it comes to serious data analysis, they have some big limitations.\nData analysis isn’t just about getting answers—it’s about getting credible answers.\nTo trust your insights, you need to leave a trail. Think about it—during analysis, you make dozens of tiny decisions:\n\nWhich part of the data should you focus on?\nWhat variables should you use?\nWhich patterns caught your eye?\n\nEvery decision shapes your results. And if you—or anyone else—can’t retrace those steps, how can you be sure your conclusions hold up?\nThat’s why scripting your analysis is so important.\nWith a script, every step is recorded. You can spot mistakes, refine your work, or pick up right where you left off—even months later. Low-code tools? They don’t give you that kind of transparency.\nSo, what’s the best language for scripting your data analysis?\nLINDA:\nThe answer is Python.\nPython is the world’s most popular programming language, and for good reason. Created in 1990 by Guido van Rossum, Python has become the go-to language for everything from building websites to powering cutting-edge AI. It may not be the fastest language out there, but it’s arguably the most readable. And in today’s data-driven world, readability matters more than ever.\nThe Python ecosystem for data analysis is enormous. Whatever your question, there’s a good chance Python has a library—or ten—that can help.\nData analysis is unique—it’s less about traditional programming and more about crafting a story with your data. Your code should be clear and intuitive, not just for you, but for anyone who needs to understand your work. And that includes “future you”—because six months from now, you might not even recognize your own analysis without clear documentation!\nSo, as we dive into this course, we’ll emphasize simplicity, transparency, and readability. Because great analysis isn’t just about crunching numbers—it’s about telling a story that stands the test of time.\nDMYTRO:\nData analysis is evolving. Today, some of the most cutting-edge tools are built on high-performance programming languages like Rust, Java, or C++. Why? Because these languages are fast—lightning fast. But here’s the best part: you don’t need to write in these languages to enjoy their benefits.\nModern tools now separate the user interface from the engine. That means the algorithms working behind the scenes are the same, no matter which scripting language you use.\nInitiatives like Apache Arrow go even further—they create standardized data formats, making it easy to move between tools and platforms without losing performance or compatibility.\nIn this course, we’re diving into tools built on Rust—one of the fastest, most efficient programming languages out there. Specifically, we’ll use uv for managing packages and environments and polars for data wrangling.\nThese tools are not just fast—they’re scalable.\nThe examples we’ll explore together are small—easy to follow and understand. But don’t let that fool you. The same tools we use here can scale effortlessly to handle datasets with billions of rows, processed across dozens of parallel machines.\nWhat’s even better? The interface doesn’t change.\nLINDA:\nSo whether you’re working on a personal project, academic research, or a large-scale business application, the skills you gain here will translate directly to the real world.\nThe datasets may be small, but the questions and challenges we tackle are universal. By the end of this course, you’ll be equipped to uncover meaningful insights from your own data, no matter its size or complexity.\nLet’s get started on this exciting journey into the world of data literacy!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "VIDEO 1 Grammar of Graphics {greenscreen} ~ 7 min\n\nWelcome to the lesson on data visualization!\nData visualization is one of the most important skills in data analysis. Why? Because a well-made chart can reveal patterns, trends, and insights that might otherwise stay hidden in a spreadsheet. It’s like turning a jumble of numbers into a picture that tells a story.\nBut let’s be honest—visualizing data can sometimes feel overwhelming. There are so many types of charts to choose from and endless options for customizing them. Scatter plots, bar graphs, heatmaps—what should you use? And even after you pick a plot, there are all these parameters: axis, labels, color scales, gridlines… It’s easy to feel like you’re drowning in options!\nPoint-and-click tools for visualization, like those built into some software, can be helpful, but they come with their own challenges. They often overwhelm you with choices, and worse, they don’t always give you an easy way to reproduce or share your work.\nWhen you create visualizations using a script instead of a mouse, you unlock an entirely new level of power. Scripting your plots means they’re reproducible. You can tweak them, reuse them, and share the code with others. It’s like building a recipe that others can follow, modify, or inspect to understand exactly how the visualization was made.\nNow, let’s take a step back into history for a moment. In 1999, a statistician named Leland Wilkinson published a groundbreaking book called The Grammar of Graphics. Think of it like this: Just as grammar gives structure to language, Wilkinson’s framework gave structure to statistical graphics. He introduced a way to think about and construct plots systematically, rather than relying on intuition or tradition alone.\nHis ideas were revolutionary and influenced countless tools for making visualizations. One of the most famous examples is the R package ggplot2, created by Hadley Wickham. Wickham built on Wilkinson’s Grammar of Graphics and created what is now considered one of the most powerful and popular visualization tools in the world of data science.\nIn 2017 a passionate Python developer from Uganda by the name Hassan Kibridge ported ggplot2 into Python. His project became known as plotnine and it soon became a universal success. Here’s the story of plotnine in his own words:\n&lt; PAUSE &gt;\n\n\nWhile the syntax of plotnine might feel a bit different from typical Python code, don’t worry—there’s a reason for it! The goal is to keep the grammar intact, and that consistency makes it easy to learn and incredibly flexible to use.\nIn this lesson, we’ll dive into plotnine and explore how it allows you to create clear, beautiful, and insightful visualizations. We’ll guide you step by step so you can quickly become comfortable with its intuitive and expressive syntax.\nSo, join us on this journey into the wonderful world of data visualization. By the end, you’ll be creating plots that don’t just look good but also communicate your data’s story effectively. Let’s get started!\n\nBefore we dive in, let’s talk about the tools and libraries we’ll be using in this lesson.\n&lt; PAUSE &gt;\nFirst up is polars, the powerful data analysis library we’ll be relying on throughout the course. If you’re familiar with Python, you know it’s common practice to use shorthand or aliases when importing libraries. For polars, the standard alias is pl, so that’s what we’ll use here. Anytime we call a function from polars, it will be prefixed with pl. — simple and consistent.\nNow, when it comes to our visualization library, plotnine, we’ll take a slightly different approach. Instead of using a prefix, we’ll import all its functions directly into our workspace. This means we’ll use the from plotnine import * syntax, which essentially says, “Hey Python, bring in everything from plotnine!” Why? Because it makes our plotting code cleaner, easier to read, and more expressive.\nFinally, let’s talk about the dataset we’ll be exploring today. It comes from the gapminder package. If you’re not familiar with Gapminder, it’s a non-profit organization founded by Hans Rosling and his children back in 2005. Their mission? To promote a better understanding of global development through data—focusing on health, economics, and the environment.\nThe Gapminder Foundation maintains an incredible collection of statistics about the world, and this package is a small extract from their database. It’s packed with fascinating data on public health, economic development, and global welfare.\nHans Rosling himself is famous for his captivating TED Talk in 2007, where he used data to tell the story of global development. He spoke about life expectancy, GDP, and even the humble washing machine—and how it changed the world. If you haven’t watched that talk yet, I can’t recommend it enough. It’s a masterclass in how to make data come alive.\nSo, with our tools in hand and an inspiring dataset at our fingertips, we’re ready to start exploring and visualizing. Let’s get to it!\n\n\nVIDEO 2 Data {greenscreen} ~ 5 min\n\nThe dataset has been conveniently imported for us by the gapminder package. To get started, we can simply type gapminder into our notebook cell and hit Enter. When we do that, we’ll see a preview of the data in the form of a table — what we call a DataFrame. In this table, each row represents an observation, and each column represents a variable.\n&lt; PAUSE &gt;\nThis dataset has 1,704 rows and 6 columns, so it’s fairly compact but still rich with information. Let’s walk through what each of these columns means:\n\ncountry: This column lists the names of countries. If you take a look at the data, you’ll notice it starts with Afghanistan at the top and ends with Zimbabwe at the bottom. It seems the data is sorted alphabetically by country.\ncontinent: Here, we have the names of continents. For example, Afghanistan is listed under Asia, while Zimbabwe is under Africa. Makes perfect sense.\nyear: This column tells us the year of the observation. You’ll notice that each country has multiple rows because data was collected at different times. The dataset starts in 1952 and progresses in 5-year increments, which gives us a nice snapshot of changes over time.\nlifeExp: This column stands for life expectancy at birth. If we look at Afghanistan in 1952, for example, the life expectancy was just 28.8 years. Let that sink in for a moment — only 28 years! It’s a sobering reminder of the challenges some nations faced in the mid-20th century.\npop: This column shows the population of each country. Again, looking at Afghanistan in 1952, the population was just under 8.5 million people.\ngdpPercap: Finally, this column contains the GDP per capita, expressed in US dollars. From what I understand, these figures have been adjusted for inflation, so they should be comparable across countries and over time.\n\nAltogether, these six columns give us a fascinating lens through which to explore global trends in health, wealth, and population growth. The dataset might look simple at first glance, but it’s packed with stories waiting to be uncovered.\nNow that we know what we’re working with, let’s roll up our sleeves and start exploring!\n\n\nVIDEO 3 First Plot {greenscreen} ~ 10 min\n\nNow, it’s time to create our very first plot! Here is a question we would like to answer using gapminder data: Do people in rich countries live longer than people in poor countries?\n\nThe answer may be quite intuitive, but we will continue our investigation further: How does the relationship between GDP per capita and Life expectancy look like? Is this relationship linear? Non-linear? Are there any exceptions to the general rule?\n\nIn order to answer these questions, we will create a plot from gapminder data. Here’s the code we’ll use. Take a moment to copy this code verbatim from your screen.\nWhen writing Python code with plotnine — and later when we use polars — you’ll notice that we often wrap our code in parentheses. This is a great habit to get into because it allows us to break our code into multiple lines without worrying about indentation.\n&lt; PAUSE &gt;\nLet’s walk through this step by step.\nInside the outer parentheses, the first thing we write is ggplot. This is the foundational function in plotnine, and it stands for Grammar of Graphics plot. Then, in parentheses again, we pass the dataset we want to use — in this case, gapminder.\nAfter that, we add a plus sign. The Grammar of Graphics, which plotnine is built on, thinks of plots as being made up of layers. The + sign we added tells Python that we’re adding more layers or components to our plot. Think of it as saying, “Wait, there’s more!”\nOn the next line, we write geom_point. This is the function that specifies the type of layer we’re adding to our plot. In this case, it’s a point plot, which means we’ll be drawing points on a graph. Without this layer, our plot would just be an empty canvas.\nInside the geom_point parentheses, we specify the argument: mapping followed by an = sign. This tells plotnine how we want to relate our data to the graph. AES stands for “aesthetics”. The inside of the aes function defines mapping of the variables in our data to certain aesthetical properies of our graph. We’re saying, “Take the GDP per capita (gdpPercap) and map it to the x-axis, and take life expectancy (lifeExp) and map it to the y-axis.” Notice that the column names are enclosed in quotes — that’s important!\nOnce you’ve written the code, go ahead and hit the Run button. If everything is correct, you should see your plot appear on the screen.\nLet’s take a moment to reflect on what we just did.\nIn our code, the first layer was the ggplot function, where we provided the dataset. The second layer was geom_point, which added points to our graph.\nThe result is a simple yet meaningful scatter plot. It shows a positive, non-linear relationship between GDP per capita on the x-axis and life expectancy on the y-axis. Does this align with what you initially expected? Or does it challenge your assumptions? Already, you can see how visualizing data helps uncover patterns and stories that might not be obvious at first glance.\nTake your time to review the code and compare it to the plot we created. Understanding this connection — how the code you write translates directly into what you see on the screen — is the key to mastering data visualization.\nIn fact, the structure of most plots in plotnine (and its R counterpart, ggplot2) can be summarized with a simple template:\n&lt; PAUSE &gt;\n\nThis template is incredibly flexible and serves as the foundation for almost every visualization we’ll create.\nIn the remainder of this lesson, we’ll explore how to extend and customize this template to create a wide variety of visualizations. Each new element we add will open up even more possibilities.\nI’ll see you in the next one!\n\n\nVIDEO 4 Axis {greenscreen} ~ 9 min\n\nHello again! Ready for a challenge? I’ve got a question for you: How has life expectancy changed over time? \nTake a moment to think about it. Better yet, try answering it by modifying the code we wrote in the last lesson.\nHere’s a quick hint before you pause the video: The gapminder dataset includes a column called year, which can go on the x-axis. Use that to tweak the code and see what you find. I’ll wait right here while you try it out!\nPause the video now and give it a shot. See you in a moment!\n&lt; PAUSE &gt;\n\nDone? Excellent! Let’s take a look at what we’ve got. Nice work! You should see a scatter plot showing life expectancy over time.\nHmm… notice how some of the points are stacked on top of each other? That’s called overplotting, and it’s pretty common when you have a lot of data points at the same x or y values. Don’t worry—it’s easy to fix!\nInstead of geom_point, try using geom_jitter. This will add a tiny bit of random noise to spread out the points so they’re easier to see.\nHere’s how you do it:\n&lt; PAUSE &gt;\nRun this code and check out the difference. Much better, right? Now we can see the points more clearly.\nLet’s keep going with this little game. Here’s your next challenge: Can you visualize life expectancy by continent?\n\nThink about which variable should go on the x-axis this time. Which continent do you think tends to have the highest life expectancy? Modify your code and give it a shot. Pause the video, try it out, and come back when you’re ready.\n\n&lt; PAUSE &gt;\nGreat job! What do we see here? Looks like life expectancy in Oceania is quite high, although there aren’t many points for that region. Europe is a close second. On the other hand, Africa seems to have the lowest life expectancy overall, judging by the density of points at the lower end of the y-axis.\nHere’s another question: Which continent has the widest spread in life expectancy values? That’s right—it’s Asia. There’s quite a bit of variation there, which is something we’ll dig into in more detail later in the course.\nFantastic work so far! Take a moment to review what you’ve done, and I’ll see you in the next section!\n\n\nVIDEO 5 Aesthetical mappings {greenscreen} ~ 7 min\n\n&lt;…Walks in, looking thoughtful….&gt;\nOh, hey there! You know, I’ve been thinking—what if we could combine the graphs from the last two challenges and show the relationship between not just two variables, but three?\nNow, don’t worry—we’re not diving into “three-dimensional” plots just yet. Instead, we can represent a third variable using color. Let me show you what I mean.\nHere’s modified code that maps the continent variable to the color aesthetic:\n&lt; PAUSE &gt;\nRun this code and take a look.\n&lt; PAUSE &gt;\n\nWhat do you see? Now we can see more clearly how life expectancy has changed over time by continent. For example, the points representing Africa stay clustered near the lower end of the y-axis throughout the years, while Europe’s points are generally higher. Oceania is there too, but it’s barely noticeable because there are so few observations. Pretty cool, right?\nNow, I’ve got a question for you: What happens if we switch the mappings of continent and year? Give it a try!\n\n&lt; PAUSE &gt;\n\nDone? Great! Do you still find this graph useful? Why or why not?\nNow let’s tweak it a bit more. What if, instead of mapping color to year, we mapped it to country? Give it a try!\n\n&lt; PAUSE &gt;\nWhat changed? How does mapping color to country differ from mapping it to year? Take a moment to think about it. What do you think is the main limitation of using the color aesthetic?\nAlright, here’s one last challenge for this section: Can you add a splash of color to our original graph of life expectancy by GDP per capita? Let’s color the points by continent.\n\nTake a moment to run your code and see what you get.\n&lt; PAUSE &gt;\n\nAmazing! By adding color, we can now spot trends and patterns more easily. But did you notice something else? There are a few outliers in this plot. Can you tell which continent those points belong to?\nThe points look a little crowded. But you know, you can always transform GDP per capita to a logarithmic scale for better visualization. Just add scale_x_log10() as an additional layer to your graph, like that:\n&lt; PAUSE &gt;\nYou’re making fantastic progress! Well done! In the next section, we’ll explore even more aesthetics that can help us tell richer stories with our data. See you there!\n\n\nVIDEO 6 More aesthetics {greenscreen} ~ 10 min\n\nHello again!\nSo far, we’ve explored some powerful ways to visualize data using the x, y, and color aesthetics. With these, we’ve been able to represent three variables in a single plot. Pretty amazing, right?\nNow, let’s quickly recap what we’ve learned about the color aesthetic. When we map a categorical variable like continent to color, plotnine automatically picks a distinct palette for each category. This works great when there are just a few categories, but as the number of categories grows, the colors start to blur together and lose their effectiveness.\nOn the other hand, when we map a continuous variable like year to color, we get a gradient. While individual values can be harder to pinpoint, the overall trends are beautifully highlighted by the gradient’s brightness.\nAlright, as promised, let me introduce you to another fantastic aesthetic: size.\nImagine we could vary the size of the points in our graph to represent something meaningful—like the population of a country. That would let us visualize not three, but four variables at the same time. Let’s give it a shot. Here’s the code:\n&lt; PAUSE &gt;\nWRun this and take a moment to appreciate the result. Isn’t it gorgeous?\nNow we can see the journey of countries like China and India over time. Their points stand out because of their large populations. Under the logarithmic transformation of the x-axis, the relationship between GDP per capita and life expectancy starts to look more linear—but not quite!\nNotice the outliers on the far right? They all seem to be from Asian countries. Are these countries rich or poor? Rich, right? But their life expectancy doesn’t quite follow the trend we see in Europe or the Americas. Fascinating, isn’t it?\nNow, let me share one more aesthetic property with you: shape.\nShape can be a great tool for visualizing low-cardinality categorical variables, like continent. Instead of just using circles, we can use distinct shapes for each category. This lets us pack even more information into the same graph.\nReady for a challenge? Let’s push the limits and visualize five dimensions in a single plot. Modify the previous example to map year to color and continent to shape. Take a moment and try it. I’ll wait.\n\n\n&lt; PAUSE &gt;\nWhat do you notice? Can you tell whether those Asian outliers come from small or large countries? Are they from earlier or later time periods?\nThese are the kinds of questions we can answer when we use multiple aesthetics thoughtfully. Isn’t it amazing how much information we can pack into a single visualization?\nFantastic work today! In the next lesson, we’ll continue exploring new tools and techniques to take your visualizations even further. See you soon!\n\n\nVIDEO 7 Non-data linked properties {greenscreen} ~ 7 min\n\nWelcome back!\nSo far, we’ve packed a lot of information into single graphs using data-mapped aesthetics like color, size, and shape. While this approach is powerful, let’s face it—combining too many aesthetics can make a plot feel busy and overwhelming.\nSometimes, less is more. A clean and simple graph, highlighting just one or two aspects of the data, can be just as insightful—and a lot easier on the eyes.\nNow, the default style in plotnine is already quite nice, but there may come a time when you want to tweak things to better suit your storytelling. So, let’s look at how to customize graphs using non-data-linked properties—those that aren’t mapped to a variable but instead apply globally to all points in the graph.\nHere’s an example. What if we want all the points in our plot to be the same color, say blue? And what if we also want to adjust their size and transparency? Here’s the code to do that:\n&lt; PAUSE &gt;\nGo ahead, give this a try.\n\nBeautiful, isn’t it? All the points are now blue, with a larger size and a soft transparency that makes overlapping points blend together nicely. This transparency, or alpha, helps highlight areas where the data is dense—like shadows on a heatmap.\nNotice something? The color, size, and alpha settings aren’t part of the aes() function. That’s because these properties aren’t mapped to any variable in the data. Instead, they’re applied uniformly to every point in the plot.\nLet’s break it down:\n\ncolor=\"blue\": The color is set as a character string, wrapped in quotes. You can experiment with other colors too—try red, green, or even hex codes like “#FF5755”.\nsize=2: The size of the points is specified as a number, in millimeters. Increase the size to make the points larger or decrease it for smaller ones.\nalpha=0.1: Transparency is a decimal value between 0 and 1, where 0 is completely transparent and 1 is fully opaque.\n\nFinally, let’s talk about shapes. In plotnine, shapes are represented by numbers. For example:\n\n0 is a square,\n1 is a circle,\n2 is a triangle,\n20 is a small filled circle.\n\nHere’s a challenge for you. Change the shape argument in the code to explore different shapes. Try values between 0 and 25, and see how your graph changes. You’ll find the full list of shapes in the plotnine documentation. \nSo, what do you think? With just a few tweaks, we’ve turned our scatter plot into a clean and stylish visual. Customizing non-data-linked properties like this is a great way to emphasize certain elements of your data without overwhelming your audience.\nIn the next lesson, we’ll explore even more ways to take your visualizations to the next level. See you there!\n\n\nVIDEO 8 Geometrical objects {greenscreen} ~ 12 min\n\nWelcome back! Let’s dive into another exciting aspect of creating visualizations in plotnine: geometrical objects, or geom_ functions.\nThese geom_ functions are the building blocks of your plots, allowing you to highlight different aspects of your data. By swapping or combining geom_ layers, you can tell entirely new stories with the same dataset.\nFor example, what if we wanted to show the development of life expectancy over time for each country? We could use geom_line() to connect individual data points belonging to the same country.\nHere’s the code to do just that:\n&lt; PAUSE &gt;\nNote that we have a new aesthetics called group. It indicates which points need to be connected together to for a line. Here we are drawing one line per country. Take a moment to run this and see what you get.\n\nDo you see it? Each country now has its own line, colored by continent. It’s fascinating to watch life expectancy trends unfold over time. But look closely—you might notice some sharp, sudden drops for certain countries. What do you think caused these declines? Wars? Epidemics?\nWe’ll learn how to zoom in on these tragic moments and identify the affected countries later in the course, once we’ve mastered some data wrangling with Polars. For now, make a mental note of this question so you can return to it later.\nAnother powerful geometrical object is geom_boxplot(). This creates a “box-and-whisker” plot that illustrates the distribution of values within categories.\nFor example, let’s visualize how life expectancy varies by continent:\n&lt; PAUSE &gt;\nRun the code and take a look.\n\nThe box represents the interquartile range—the middle 50% of data—while the line inside the box marks the median. The “whiskers” extend to show the 95% confidence interval, and any points outside this range are plotted as individual outliers.\nNow, wouldn’t it be great to combine this boxplot with our jittered points from earlier? This would help us see both the overall distribution and the outliers more clearly. Let’s layer them together:\n&lt; PAUSE &gt;\n\nLooks great, doesn’t it? But notice something—there’s some duplication in our code. We had to repeat the same mappings for both geom_jitter and geom_boxplot. That’s fine for now, but it can become cumbersome as your visualizations grow more complex.\nHere’s a trick to make your code cleaner: you can move shared mappings to the parent ggplot() function. This way, every layer will “inherit” these mappings automatically:\n&lt; PAUSE &gt;\nSee? No more repeating yourself! You can still add layer-specific settings or arguments within individual geom_ functions if needed.\n\n\n\n\n\n\nTip\n\n\n\nWhen building complex plots, start by adding one layer at a time. Once you’ve got the basic structure, move any common arguments up to the ggplot() function. This keeps your code tidy and easier to read.\n\n\nGreat job so far! In the next lesson, we’ll explore even more ways to enhance your visualizations. See you there!\n\n\nVIDEO 9 Trend lines {greenscreen} ~ 12 min\n\nWelcome back! Now, let’s take a closer look at the relationship between GDP per capita and life expectancy.\nAt first glance, life expectancy seems to improve as countries get richer. But is this relationship consistent across continents? Let’s find out by adding trend lines to our plot.\nTrends are essentially linear regression lines. You might remember them from school—they represent the best-fit line through your data. Here’s how we can add them to highlight differences in this relationship by continent:\n&lt; PAUSE &gt;\nTake a moment to run the code and see the result.\n\nWhat do you observe? By default, geom_smooth() creates a regression line for each continent, and plotnine even adds confidence intervals—those shaded gray areas around the lines. These intervals give us an idea of how well the model fits the data.\nWe also used the alpha argument to make our points semi-transparent. Why? It reduces visual clutter and lets the trend lines stand out more. Did you know that transparency can also be mapped to a variable? That’s right—just like color or size, you can use alpha as a mapping aesthetic to make transparency vary based on your data. Try experimenting with that later!\nHere’s a task for you: Modify the code we just used so that instead of creating separate regression lines for each continent, plotnine creates a single trend line for all data points.\nHere’s the code to start with:\n&lt; PAUSE &gt;\n\nTake a moment to think about it. How can you combine the points colored by continent with a single global regression line?\nThere’s more than one way to solve this problem—see what you can come up with!\nDid you find this challenge hard? It’s ok! Let’s step through it together!\nIn our previous example, we declared all the mappings—x, y, and color—at the global level, in the ggplot() function. This means that every layer inherited these mappings. While this works well for most situations, it’s not what we need here.\nTo build a single trend line for all data points, we must ensure that the color aesthetic applies only to the points and not to the trend line. How do we do that? By moving the color mapping from the global level into the geom_point() function.\nHere’s how the updated code looks:\n&lt; PAUSE &gt;\nBy moving the color aesthetic into geom_point(), it now affects only the points layer. Notice that it’s still wrapped in the aes() function because it remains a data-linked property. Meanwhile, the trend line—added by geom_smooth()—inherits only the global mappings for x and y. This creates a single linear model across all continents, as we wanted.\nTake a moment to observe how this subtle adjustment changes the visualization and makes the trend line easier to interpret.\nSome of you might have come up with an alternative solution. Instead of changing the color aesthetic’s scope, we can override it directly within the geom_smooth() layer. In this case, the color aesthetic remains global, but we specify a non-data-linked property for the trend line, such as making it black:\n&lt; PAUSE &gt;\nHere, geom_smooth() ignores the global color aesthetic and instead applies the color black uniformly to the trend line. The result? A single black trend line stands out clearly, while the points remain color-coded by continent.\nBoth approaches work well, and the choice depends on how you want to structure your code and highlight different layers. Managing global and layer-specific mappings is a powerful feature in plotnine that gives you flexibility in creating clean, insightful plots.\n\n\nVIDEO 10 Factors {greenscreen} ~ 5 min\n\nDo you want to learn a nifty trick that can improve your data visualization? This method can be useful when you want to visualize a continuous variable which has a limited number of distinct values.\nImagine we’re working with year, which is technically a continuous variable. But for some visualizations it might make more sense to treat each year as a separate category. How do we do that without modifying the data?\nSimple! Instead of referencing year as a string ('year'), wrap it in factor(), like this 'factor(year)'. This shorthand plotnine function converts a continuous variable into a categorical one on the fly, with each distinct value treated as its own category.\nLet’s put this into practice with a couple of challenges!\nCreate a boxplot of life expectancy over time, treating year as a categorical variable. Using this plot, can you detect when the interquartile range of life expectancy—the middle 50% of values—was the smallest?\nThen apply the same concept to gdpPercap.\nCreate a boxplot of GDP per capita by year, but this time keep it on a logarithmic scale. Remember that we used the scale_y_log10() function to make the data easier to interpret. Compare the interquartile range of GDP per capita in 2007 with that in 1952. Is the world today more or less diverse in terms of economic inequality?\n\nGo ahead and give it a try. Pause the video and come back once you have your answer!\n&lt; PAUSE &gt;\nLooking at these two plots, you might notice a fascinating pattern.\nYou’re absolutely right: economic inequality has grown in the recent decades. In 1952, the world was much poorer, but there was a greater sense of uniformity across nations. By 2007, while the world is significantly wealthier on average, the disparities have widened.\nAnd those outliers? Intriguing, aren’t they? Three countries stand apart from the rest in the 1952. Which ones could they be? We’ll revisit these mysteries after diving into data wrangling techniques.\nGreat work on these challenges! These exercises show the power of visualizing data in different ways and how little tricks like factor() can make your plots much clearer. Next, we’ll explore other types of plots that can uncover even more insights. Stay tuned, and I’ll see you in the next lesson!\n\n\nVIDEO 11 More geoms {greenscreen} ~ 12 min\n\nBy now, you’ve learned so much about using plotnine to create insightful visualizations. But we’ve barely scratched the surface!\nOne of the most exciting features of plotnine is the sheer variety of geoms—the building blocks for visualizing data. Start typing geom_ in your code editor, and you’ll see a list of options pop up. It’s like a treasure chest of possibilities, and each geom offers a unique perspective on your data.\nLet’s put your skills to the test with a few new challenges!\nHistograms are perfect for exploring the distribution of a single variable. Let’s start with life expectancy. Create a histogram and observe the shape of the distribution. How many peaks—or modes—does it have? Play around with the bins parameter. Adjusting the number of bins changes the granularity of your histogram, which can affect how you interpret the distribution. What value of bins seems reasonable to you?\n\n&lt; PAUSE &gt;\nNext up: density plots. These are smoothed-out versions of histograms, showing the probability distribution of your data.\nCreate a simple density plot for life expectancy. You can do it! Start typing and you will find the function you need. Do you see it? What if you want to compare distributions across continents? Add a color aesthetic.\n\n&lt; PAUSE &gt;\nRight! You can split the data by continent by adding a color aesthetic and linking it to the variable continent. Or take it one step further! Use the fill aesthetic (in addition to color) to fill the areas under the curves. Add some transparency with alpha for a cleaner visualization like this:\n&lt; PAUSE &gt;\nThese plots help us see how life expectancy varies not just overall, but also within each continent. Notice any interesting patterns? What might explain the peaks—or modes—you see in the distributions?\nNow let’s level up with 2D density plots. These are excellent for visualizing relationships between two variables. Start by creating a density plot of log GDP per capita vs. life expectancy (use geom_density_2d() function):\n\n&lt; PAUSE &gt;\nWhat do you see? Notice the two distinct clusters? One cluster represents countries that are poorer and have lower life expectancy, while the other includes those that are wealthier and healthier.\nNow let’s break it down by continent. Add a color aesthetic to see how regions of the world are distributed.\n\n&lt; PAUSE &gt;\nIsn’t that fascinating? The lower cluster is primarily made up of African countries, while the higher cluster mostly includes Europe and Oceania. Asia? It’s scattered across both clusters, reflecting its diversity in economic and health outcomes. These exercises highlight the flexibility and power of plotnine. Whether it’s histograms, density plots, or advanced 2D density visualizations, each plot adds a new layer of understanding to your data.\n\n\nVIDEO 12 Faceting {greenscreen} ~ 10 min\n\nWhen your graph starts to feel a bit too crowded—perhaps with too many layers or overlapping aesthetics—there’s a simple solution: faceting. Faceting allows you to split your data into separate panels, creating multiple similar graphs for subsets of your data. This can make complex trends easier to spot and comparisons much clearer.\nIn plotnine, faceting is incredibly easy to use. Let’s revisit one of our earlier graphs and apply faceting to organize it by continent.\n&lt; PAUSE &gt;\nHere’s what’s happening:\n\nfacet_wrap('continent') instructs plotnine to create a separate panel for each unique value in the continent column.\nPanels are arranged from left to right, and when they don’t fit on one row, they “wrap” onto the next line.\n\nThe result? A clean, organized set of charts where each panel highlights the GDP-per-capita and life expectancy trends for a specific continent. Faceting is especially helpful when the number of panels is manageable, and it lets us compare trends within each group side by side.\nLet’s take this idea further. What happens to the relationship between GDP per capita and life expectancy over time?\nTry faceting by year instead of continent.See if you can answer these questions\n\nThis exercise offers an incredible opportunity to see how historical events, global growth, and inequality have shaped the world over decades.\n\n&lt; PAUSE &gt;\nWith everything we’ve learned so far, we can summarize the plotnine template as follows::\n&lt; PAUSE &gt;\n\nFaceting is a powerful addition to your visualization toolkit, especially when your data has distinct groups or categories. Whether you’re analyzing trends over continents or time, faceting can make your insights clearer and more impactful.\nSo, go ahead—try faceting your own graphs. You’ll be amazed at what you uncover!\n\n\nVIDEO 13 Labeling and styling {greenscreen} ~ 7 min\n\nWe’ve built our chart layer by layer, and now it’s time to refine it for presentation—whether for your boss, a client, or publication. The final touches, like annotations and labels, can make all the difference in ensuring your audience understands your insights clearly.\nLet’s start with some practical data transformations. Instead of showing GDP per capita in raw numbers, wouldn’t it be better to express it in thousands of dollars? Similarly, population is easier to interpret when expressed in millions.\nWith plotnine, we don’t need to preprocess our data for this. You can specify transformations directly in your chart code. For example gdpPercap/1e3 divides GDP per capita by 1,000, and uses scientific notation (1e3) for convenience. Similarly, you can use pop/1e6 to show population in millions.\nTo make our chart clear and professional, we’ll use the labs() function. This function gathers all labels in one place, allowing us to customize:\n\nTitle and subtitle at the top,\nCaption at the bottom,\nLabels for x, y, and any mapped aesthetics, like color or size.\n\nHere’s an example of a polished, annotated chart:\n&lt; PAUSE &gt;\nEach label corresponds to the aesthetics used in the aes() mappings. Make sure all mapped aesthetics are labeled, even if they appear in just one layer.\nNow, let’s make your chart stand out! plotnine offers pre-selected themes that adjust the colors, fonts, and overall style of your plots.\nOne of my favorites is theme_minimal(). It simplifies the design, creating a clean and modern look:\n&lt; PAUSE &gt;\nplotnine offers plenty of built-in themes to match your purpose:\n\ntheme_dark() for a sleek, high-contrast look.\ntheme_linedraw() for a simple, hand-drawn aesthetic.\ntheme_xkcd() for a playful, comic-style appearance.\ntheme_538() for a polished, professional newsroom feel.\n\nThemes contributed by the community can add even more variety. So, explore, experiment, and find the one that best suits your data story.\nCongratulations! You’ve learned about aesthetics, scales, different types of geoms and now you also know how to annotate and apply themes to your visuals to make them more compelling. With these skills, you’re ready to create polished, professional-quality charts that truly stand out.\nGood luck, and we can’t wait to see the insights you’ll uncover!"
  },
  {
    "objectID": "sharing.html",
    "href": "sharing.html",
    "title": "Sharing",
    "section": "",
    "text": "VIDEO 1: Reproducible reports with Quarto ~ 6 min\nWe’re almost there! Over the last four modules, you’ve built a solid foundation in data analysis—from setting up a reproducible Python environment with uv and Jupyter Lab to crafting stunning visualizations and mastering Polars for data wrangling, reshaping, and joins. You’ve also learned how to document your findings in a Jupyter Notebook.\nAnd now - it’s time to share your insights with the world.\nBut before we do, let’s talk about reproducibility — and one quirk of Jupyter notebooks that sometimes gives them a bad reputation.\nBy now, you’ve probably noticed that Jupyter cells can be run in any order. You can scroll back up, re-run an earlier cell, tweak some code in the middle, or even insert new cells as you go. This flexibility is fantastic — it allows you to experiment, fix mistakes, and refine your analysis without starting over.\nBut here’s a tiny problem\nBecause cells don’t have to be executed sequentially, your dataset might be in a different state than you expect. Maybe you re-imported your data at some point, unaware that an earlier cell had filtered out some rows or modified a column. And because Jupyter doesn’t always throw an error when this happens, you might not even realize there’s an issue — until it’s too late.\nNow, imagine someone else — or even your future self — trying to re-run your notebook from top to bottom. Suddenly, things break. Missing columns. Unexpected outputs. Errors that weren’t there before.\nTo avoid this problem, get into the habit of periodically re-running your notebook in order. Here’s how:\n\nClick Run in the top menu.\nSelect Run All Above Selected Cell—this will execute everything from the top down to your current position.\nIf you’re at the last cell, this ensures that your entire notebook runs from start to finish without errors.\n\nThis one simple step can save you a lot of trouble—especially if you plan to share your work or revisit your notebook later. Trust me—your future self will thank you.\nWhile these challenges exist, there’s a fantastic solution: Quarto.\nQuarto is a powerful, multi-language publishing system that transforms your Jupyter notebooks (or Markdown files) into polished reports, presentations, websites, and even Word or PDF documents.\nThe best part? Quarto automatically runs your notebook from top to bottom. This means your entire analysis is executed sequentially, ensuring that everything flows correctly into a beautifully formatted output—tables, text, images, and all.\nBefore we start using Quarto, we need to install it—and thanks to uv, it’s super easy.\nFirst, open a terminal in your project folder and run:\nuv tool install quarto-cli\nThis might take a little while, but once it’s done, Quarto will be installed and ready to go.\nTo verify that everything is set up correctly, run:\nuv run quarto check\nThis will launch Quarto’s diagnostics tool, and you should see a checklist with lots of OK signs confirming that everything is working.\nOne of these checks will show the Python version that Quarto has detected. Look for the section labeled “Python 3 installation”—it should display the path to your current Python environment. If it matches your project setup, you’re good to go!\nWe’ll be using Quarto inside Jupyter Lab, so hop back into your notebook—and I’ll meet you there!\n\n\nVIDEO 2: Quarto report from notebook ~ 5 min\nTo make our report look even better, let’s add a Quarto header—also known as front matter—to the top of our notebook.\nHere’s how:\nInsert a new Markdown cell at the top of your notebook: Click on the first cell in your notebook, Press A to add a new cell above. Press M to convert it to a Markdown cell.\n\nCopy and paste the text underneath this video into the new cell. It’s important to preserve the exact formatting, including indentation and line breaks.\n\nAll set? Great! Let’s break it down.\n\nThe three dashes (—) at the start and end mark this as a front matter block.\nInside, we specify details like the title, subtitle, and author—all pretty straightforward.\nThe date field is special: last-modified automatically updates whenever you save the file. Perfect for keeping your reports up to date!\n\nNow, run this Markdown cell by pressing Ctrl + Enter (Cmd + Enter on Mac). Since we have the Quarto extension installed in Jupyter Lab, this will render as a nicely formatted heading.\nNow, let’s generate our first Quarto report!\nFirst, Open a new terminal inside Jupyter Lab:\n    Click the big blue \"+\" button on the left panel, OR\n\n    Click the small \"+\" button next to your notebook’s tab.\n\n    This opens the Launcher window.\n\n    Under \"Other\", click \"Terminal\".\nThis terminal is just like your system’s command line, and you can run all the same commands here. Since your original terminal is busy running Jupyter Lab, this new window will be more convenient.\nAssuming your notebook is named report.ipynb, type:\nuv run quarto preview report.ipynb\nIf your file has a different name, replace report.ipynb with the correct filename.\nHere’s what happens:\nuv run ensures the command runs in your current project environment—with the correct Python version, libraries, and dependencies.\n\nQuarto processes your notebook, executes all cells in order, and renders the output.\nOnce it’s done, you’ll see a message like this:\n#&gt; Watching files for changes\n#&gt; Browse at http://localhost:3636/\nClick the link, and your formatted report will open in a new browser window!\nFor the best experience, I suggest you arrange your screen so you can see both:\nJupyter Lab on one side, where you’re editing your notebook.\n\nThe rendered Quarto report on the other side, updating live as you make changes.\nAnd just like that—you’ve turned your Jupyter notebook into a polished, shareable report!\nNow that your Quarto report is up and running, here’s something really cool—every time you save your notebook, your Quarto report updates automatically!\nThis means you can see your changes in real time without having to restart the rendering process. It’s an incredibly useful feature for fine-tuning the layout and formatting of your report.\nLet’s try it out! Experimenting with Formatting\nI have a small challenge for you:\n\n\n\n\n\n\nChallenge\n\n\n\n\nAdd a few section titles using Markdown headers (#, ##, ###).\nMake some text bold (bold) or italic (italic).\nFormat your lists properly by adding bullet points (- item 1, - item 2).\nSave your notebook and watch your Quarto report update in real time!\n\n\n\nYou’ll immediately see how Quarto transforms your notebook into a professional-looking document with just a few simple tweaks.\nKeep your Quarto report open, because in the next section, we’ll explore even more ways to style and enhance your report.\nSee you in a bit!\n\n\nVIDEO 3: Quarto styling and rendering ~ 5 min\nA great analytics report isn’t just about having the right data—it’s about clarity and focus. Your audience should be able to quickly grasp key insights without being distracted by unnecessary details.\nOne way to achieve this is by structuring your notebook properly. In Jupyter, we have two main types of cells:\nMarkdown cells for text and explanations\n\nCode cells that generate outputs like tables, plots, or print statements\nTo keep things clear and readable, each code cell should ideally produce just one output—a printout, a table, or a plot. If you have a long block of code that does multiple things, consider splitting it into separate cells. This not only makes your report easier to follow but also gives you better control over your outputs.\nNow, let’s talk about a powerful Quarto feature: labels and cross-references.\nEvery code cell in a Quarto report has a label, even if you don’t explicitly assign one—Quarto generates it automatically. But if you want to refer to specific outputs in your text—like a figure or table—you’ll need to assign meaningful labels yourself.\nHere’s how it works:\nFigures should have labels that start with fig-.\n\nTables should have labels that start with tab-.\nThis allows you to reference figures and tables directly in your text, and Quarto will automatically create hyperlinks to them. This makes your report more interactive and easier to navigate.\nTo assign a label to a code cell, we use Quarto chunk options—a special type of comment that Quarto recognizes.\nYou might already know that Python uses # for comments. Quarto extends this by using #| for special commands. These must be placed at the top of the cell, before any code.\nFor example, to label a figure-generating cell, you’d write:\n#| label: fig-first-plot\nLater in your report, you can reference this figure using ?@fig-first-plot. Quarto will automatically replace this with “Figure 1” and create a clickable link to the figure’s location.\nTo add a caption to your figure, use another chunk option:\n#| fig-cap: “This is my first plot showing XYZ trends.”\nThis ensures that a descriptive caption appears under your figure, helping your audience interpret the data correctly.\nIf a cell produces a table, you should label it with tab-, like this:\n#| label: tab-summary-stats\nYou can also add a table caption for clarity:\n#| tab-cap: “Summary statistics for dataset XYZ.”\nThen, in your text, you can refer to the table using @tab-summary-stats, and Quarto will handle the rest—linking it properly and ensuring consistent formatting.\nThis small but powerful feature makes your reports much more professional by keeping figures and tables clearly referenced.\nNow, try adding some labels and captions to your own notebook! In the next section, we’ll explore even more ways to fine-tune your Quarto report.\n\nThe Markdown cells in your Quarto report don’t just display text—they can be formatted for clarity and impact. You can use:\nTabs to organize content\n\nLists for structured information\n\nCallouts to highlight key points\n\nAnd much more!\nTo explore all the ways you can enhance your reports, check out the Quarto documentation—I’ll include the link right below this video.\nSometimes, a code cell doesn’t produce a plot or a table, but instead prints intermediate results or debugging information. If that output isn’t essential to your audience, you can hide it from the final report using a chunk option.\nTo do this, simply add:\n#| echo: false\nat the top of the code cell. This prevents the printed output from appearing in the final document while still executing the code behind the scenes.\nA few things to keep in mind:\nThere should be a space between #| and the command: #| echo: false.\n\nThe word false is always lowercase in Quarto (unlike Python’s False or R’s FALSE).\nIf you want to hide code output across the entire document, you don’t have to modify each cell individually. Instead, you can set a default option in the front matter—the special Quarto header we added in the first cell.\nLet’s jump back to that cell, and I’ll show you how it’s done.\n\nIn the front matter of your document, you can set global code chunk options that apply to the entire report. This helps maintain a consistent style without manually adjusting each cell.\nTo hide all code outputs by default, add the following line before the closing three dashes (—) in the front matter:\necho: false\nNow, every code cell in your report will hide its output unless you explicitly override this setting. If you want to display the code in a specific cell, simply add:\n#| echo: true\nto that cell. Local settings always override global defaults, so you have full control over what your audience sees.\n\n\n\n\n\n\nChallenge\n\n\n\nTry it out! Open a Jupyter notebook you created in this course and: - Add labels and captions to your figures (fig-) and tables (tab-). - Insert a Markdown cell under each figure or table, explaining key insights. - Refer to figures and tables using ?@fig-label or @tab-label in your text. - Hide code printouts that don’t produce meaningful output, either: Globally (by setting echo: false in the front matter), or Individually (by adding #| echo: false to specific cells). - Ensure each cell outputs only one thing—a table, a plot, or text—for clarity.\n\n\nYou’ve come a long way! Your Jupyter notebook is now filled with exciting discoveries and insights—and thanks to Quarto, it’s polished, structured, and ready to be shared. You’ve seen your changes reflected in real-time using the Quarto preview window, and now it’s time to take the final step: sharing your work with the world!\nLet’s do it!\n\n\nVIDEO 4: Rendering the final report ~ 5 min\nYou’ve been working on your report interactively, running cells out of order to refine your insights and perfect the look and feel of the final product. But now, it’s time to run the report one last time before sending it out into the world.\nTo ensure everything works as expected, you should execute all cells sequentially from top to bottom. The easiest way to do this in JupyterLab is:\nGo to the \"Run\" menu.\n\nSelect \"Run All Cells\"—or even better—\n\nChoose \"Restart Kernel and Run All Cells.\"\nRestarting the kernel ensures that your Python session starts fresh and runs every cell in the correct order. If you still have a Quarto preview running, once you save the notebook, your report should now appear in its final form.\nAn alternative way to generate the final report is to use the command line, letting Quarto handle the rendering. This approach ensures that your report runs exactly as it would for a colleague on a different machine.\nIf you’re still previewing your report in Quarto, first stop the preview session by returning to the terminal and pressing:\nCtrl + C (Windows/Linux)\n\nCmd + C (Mac)\nThen, run the following command:\nuv run quarto render report.ipynb –execute\nWhat does this command do?\n✔ Runs Quarto to process your report. ✔ Executes all code from scratch in a clean Python session. ✔ Ignores pre-computed results, ensuring all outputs are fresh. ✔ Uses the same environment as when you created the report, ensuring all required packages are available.\nOnce completed, a fresh copy of your report will be saved in your project directory—ready to be shared.\nSo far, we’ve seen how Quarto generates HTML reports that can be viewed in a browser. These are great for interactive use and web hosting, but sometimes you need a Word or PDF version for sharing with colleagues.\nThankfully, Quarto supports multiple formats—it’s the Swiss Army knife of report generation! Try exporting your report to Word by adding the –to docx flag:\nuv run quarto render report.ipynb –to docx –execute\nYou should now see a Word document in your working directory. Quarto offers many additional options, including the ability to render reports using templates and other customization features—I encourage you to explore these further! You’ll find a link to the Quarto documentation below this video.\nWhile HTML and Word exports work right out of the box, generating PDF reports requires one extra step.\nPDFs are created using LaTeX, an established typesetting system used in scientific publishing. Quarto comes with its own built-in LaTeX engine called TinyTeX, which we need to install first.\nRun the following command in the terminal:\nuv run quarto install tinytex\nThis installs a lightweight LaTeX engine inside your Quarto setup, allowing you to produce beautiful, professional-looking PDFs from your reports.\nTo confirm the installation, run:\nuv run quarto check\nYou should see that the LaTeX engine is now pointing to TinyTeX, meaning everything is set up correctly. Now, to render your report as a PDF, simply run:\nuv run quarto render report.ipynb –to pdf –execute\nA PDF version of your report will appear in your project directory.\nJust like with Word documents, PDF reports can be customized using Quarto options. If you want to fine-tune the formatting or styling, check out the Quarto documentation for advanced features that will help you create polished, professional reports.\nWe’re nearing the final stretch of this course! In the last video, we’ll discuss how to share your work and ensure that others can reproduce your results exactly as you compiled them.\nSee you in a bit!\n\n\nVIDEO 5: Sharing the project ~ 5 min\nCongratulations! You’ve successfully completed an exciting project—from installing software to visualizing and exploring datasets, and finally, producing an analytical report that documents your entire journey. Now, it’s time to prepare this work to share with others.\nWhat Should You Include in the Project Folder?\nTo share your project effectively, there are a few key elements to include:\npyproject.toml – This file contains the specifications for your project environment.\n\nData – While we used GitHub to store the data in this course, if you used additional datasets or saved data locally, make sure to include them so others can replicate your results.\n\nNotebook File – Share the .ipynb file (like report.ipynb in our case), which contains the code, figures, and tables that document your analysis.\n\nRendered Report – Consider including a rendered HTML, Word, or PDF version of the report, so your audience can preview the results before opening the notebook itself.\nWhen you send your work to a colleague, they may not have the same setup as you, so it’s essential to ensure they can reproduce your results. Here’s how you can do that:\nEnvironment Management – Thanks to uv's powerful environment management capabilities, many reproducibility concerns are taken care of. This ensures that your colleague will have the same environment to run the report.\n\nComputational Notebooks – Your notebook documents all exploratory data analysis steps, allowing others to follow along and reproduce your work.\n\nRendered Quarto Report – For those without access to Python, the Quarto-rendered report provides insights that can still be accessed and understood without the underlying code.\nAs you prepare to share your project with others, consider leaving a “love note” to help them understand your work. In the world of programming and data analysis, these notes are called README files.\nIn your Jupyter Notebook launcher menu, there’s an option to create a Markdown file (look for the button with a large purple “M” on it). In this document, you can outline step-by-step guidelines for reproducing your results. Because it’s a Markdown file, you can use all the formatting tools you’ve learned, like headings, lists, and text formatting. Be sure to include the shell commands that users need to run, wrapped in three backtick symbols (```), at the beginning and end of each command block.\nHere’s a simple structure for your README:\nInstall uv – Point your reader to the installation instructions on Astral’s website. You’ll find that installation differs slightly between Linux/Mac and Windows, so make sure to include the correct instructions for your audience’s operating system.\n\nSynchronize Project Requirements – Instruct your reader to run uv sync, which will download and install all the necessary packages listed in the pyproject.toml file. This ensures all dependencies are in place for the report to run correctly.\n\nOpen the Report in Jupyter Lab – Finally, tell them how to open the report using the command uv run jupyter lab report.ipynb.\nHere’s an example README.md file that I’ve put together for my colleagues to reproduce my results.\n We highly recommend learning how to use a version control system, like Git, to share your code and collaborate on projects, especially through platforms like GitHub. While this course doesn’t cover Git in detail, we’ve included some useful links below to help you get started.\nThank you for being with us in this course! We hope you have learned a lot and feel confident in your ability to use Jupyter notebooks, Quarto, and the powerful tools we’ve covered to create interactive and well-structured analytical reports. You’ve gained valuable skills that will help you share insights clearly and effectively, whether through code, figures, tables, or polished final documents. We encourage you to continue exploring, experimenting, and improving your reporting techniques as you apply these skills in real-world projects. We wish you the best in your future endeavors and look forward to seeing the incredible work you will create!"
  }
]