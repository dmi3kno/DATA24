---
title: "Data Literacy with Python"
---

Welcome to Data Literacy with Python!

Let me ask you something: Can you imagine living in today’s world but being unable to read? Think about it—street signs wouldn’t make sense, advertisements would just be noise, and most of the internet? Completely out of reach.

Now, even with videos and voice assistants everywhere, written text is still the backbone of how we communicate and navigate life. Without it, you'd feel lost.

But here’s the thing: today’s world doesn’t just run on words. It runs on data.

Every day, we’re creating over 400 million terabytes of data. That’s every single day. And here’s a wild stat—90% of all the world’s data was created in just the last two years.

This explosion of information is transforming how we make decisions, whether it’s in business, science, or society as a whole. But here’s the catch: to keep up, you need to know how to make sense of it.

Data isn’t just numbers on a screen—it’s stories waiting to be uncovered. And understanding data has become just as important as being able to read or write.


That’s where this course comes in.

We’re going to teach you how to take raw, messy data and turn it into something meaningful. You’ll work with rectangular data—the kind you find in spreadsheets or databases.

And don’t worry—this isn’t just about working with numbers. It’s about answering real-world questions, solving problems, and making decisions based on insights you uncover.

By the end of this course, you’ll have the skills to transform data into knowledge.

Let’s talk about the tools you need to work with data.

You might be tempted by low-code or no-code solutions—those point-and-click interfaces that make everything seem so easy. And sure, they’re great for quick wins. But when it comes to serious data analysis, they have some big limitations.

Here’s the thing: data analysis isn’t just about getting answers—it’s about getting credible answers.

To trust your insights, you need to leave a trail. Think about it—during analysis, you make dozens of tiny decisions:

- Which part of the data should you focus on?
- What variables should you use?
- Which patterns caught your eye?

Every decision shapes your results. And if you—or anyone else—can’t retrace those steps, how can you be sure your conclusions hold up?

That’s why scripting your analysis is so important.

With a script, every step is recorded. You can spot mistakes, refine your work, or pick up right where you left off—even months later. Low-code tools? They don’t give you that kind of transparency.

So, what’s the best language for scripting your data analysis?

The answer is Python.

Python is the world’s most popular programming language, and for good reason. Created in 1990 by Guido van Rossum, Python has become the go-to language for everything from building websites to powering cutting-edge AI. It may not be the fastest language out there, but it’s arguably the most readable. And in today’s data-driven world, readability matters more than ever. 

The Python ecosystem for data analysis is enormous. Whatever your question, there’s a good chance Python has a library—or ten—that can help.

Data analysis is unique—it’s less about traditional programming and more about crafting a story with your data. Your code should be clear and intuitive, not just for you, but for anyone who needs to understand your work.

And that includes “future you”—because six months from now, you might not even recognize your own analysis without clear documentation!

So, as we dive into this course, we’ll emphasize simplicity, transparency, and readability. Because great analysis isn’t just about crunching numbers—it’s about telling a story that stands the test of time.

Data analysis is evolving.

Today, some of the most cutting-edge tools are built on high-performance programming languages like Rust, Java, or C++. Why? Because these languages are fast—lightning fast. But here’s the best part: you don’t need to write in these languages to enjoy their benefits.

Modern tools now separate the user interface from the engine. That means the algorithms working behind the scenes are the same, no matter which scripting language you use.

Initiatives like Apache Arrow go even further—they create standardized data formats, making it easy to move between tools and platforms without losing performance or compatibility.

In this course, we’re diving into tools built on Rust—one of the fastest, most efficient programming languages out there. Specifically, we’ll use uv for managing packages and environments and polars for data wrangling.

These tools are not just fast—they’re scalable.

The examples we’ll explore together are small—easy to follow and understand. But don’t let that fool you. The same tools we use here can scale effortlessly to handle datasets with billions of rows, processed across dozens of parallel machines.

What’s even better? The interface doesn’t change.

So whether you’re working on a personal project, academic research, or a large-scale business application, the skills you gain here will translate directly to the real world.

The datasets may be small, but the questions and challenges we tackle are universal. By the end of this course, you’ll be equipped to uncover meaningful insights from your own data, no matter its size or complexity.

Let’s get started on this exciting journey into the world of data literacy!


## Shell

:::{.callout-tip}
### Objective
Launch command line terminal in every OS
:::

## Installing uv

:::{.callout-tip}
### Objective
Install uv.
:::

Wheres my terminal


We will install the main tool. Open the terminal and run

```
# On macOS and Linux.
$ curl -LsSf https://astral.sh/uv/install.sh | sh

# On Windows.
$ powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
```

Alternative attempts to deal with installation problems

If you have python and pip install using pip (you will never need it again)

```
# With pip.
$ pip install uv
```

On Mac it can be built with Homebrew

```
# using Homebrew on Mac
$ brew install uv
```

::: {.callout-note}
Before you proceed, please check that installation is successful by running `uv` in the terminal. You should see something like this

```
uv

#> An extremely fast Python package manager.
#>
#> Usage: uv [OPTIONS] <COMMAND>
#> 
#> Commands:
#>  run      Run a command or script
#>  init     Create a new project
#>  ...
```

:::

## Managing python installation

:::{.callout-tip}
### Objective
Install Python using uv
:::

`uv` is not only powerful package and environment manager, but it also can help manage Python installations. Run the following to see if `uv` can detect an existing Python installation in your system.

```
$ uv python find
#> /path/to/your/installation/of/bin/python3
```
If no path is returned you can easily install `uv`-managed latest version of Python with

```
$ uv python install
```

## On files and folders

:::{.callout-tip}
### Objective
Understand folders. Decide on location for projects. Navigate there in command line
:::

The files on your computer are organized into folders. Your operating system managed most of the files on your computer but there are still quite a lot of files that you can create and use. You probably written some Word documents or made Excel tables. If you did, you probably know how important it is to remember where you saved your work, so that you can quickly find it.

When working with data analysis you will produce quite a few files. You will need to save your scripts, you will probably have some datasets you are going to use. At the end you probably want to produce some reports and data visualization. It means that a lot of files will be produced all related to the same project you are working on.

It is, therefore, important to not disperse these file all over your computer, but make sure that they are easily locatable together. 

The way operating system organizes the files on your computer is by using folders. Folders are nested containers of other files and folders which is best visualized as a tree. Open a file explorer on your computer and lets have a quick look

If you are on Windows, your file explorer may look something like this, if you are on Mac, your explorer will look like this and if you are on Linux you probably have something like this on your computer. 

The folders are easily recognizable by their distinct icon, but most importantly, if you click on one of those folders you will "drop down" inside of it and you will see its content often with more files and folders.

On every operating system there's usually a place where your computer expects you to place your files. 

The folders dedicated to user files are usually split by type. In your file explorer you can probably find a folder for music, documents, pictures, videos, and downloads, where as we say it in academia "PDF files go to die".

One special folder present in all operating systems is called Desktop. This is the folder, which content will be visible if you minimize all windows. Even though it is very convenient, most experts would agree that Desktop is not the best place to store your work long term. You can probably keep there a file or two you are currently working on, but you probably need to have a long-term home for your files to keep your Desktop free from clutter. We definitely do not recommend starting your data analysis project inside the Desktop folder.

![](img/desktop.jpeg)

Where should you place your data analysis work? I think the documents folder is a solid way to start. If your Documents folder is synched to the cloud, as the case often is on Windows or Mac, you might consider whether it is the right place to store your project files. Data analysis project tend to grow big and while back up for source files is important you may not want to take up your valuable cloud quota, which could be better used by your work documents and presentations.

Alternatively, you can create a folder on the same level as your documents and downloads, called, for example, Projects and host your data science projects there.  We will learn about version control for source code, which helps you keep track of changes, so the task of backing up your work will be taken care of by Git and GitHub, which we will introduce in Lesson 5. 

Wherever you decide to place your work, make sure it is easily locatable. In the next section we will talk about organizing your work in project folders, which ensure that your work is tidy and reproducible.

Navigating to project location using command line (ls, cd and cd ..).

## Projects

:::{.callout-tip}
### Objective
Create project. 
:::

Each of your data analysis endeavours should be stored in a separate project folder. 

```
$ uv init data-literacy-project
cd data-literacy-project
```

or alternatively

```
$ mkdir data-literacy-project
$ cd data-literacy-project
$ uv init
```

## Libraries

:::{.callout-tip}
### Objective
Add dependencies
:::

We will need a few libraries. Lets add them 

```
$ uv add gapminder plotnine polars jupyter great_tables setuptools ipykernel pyyaml nbformat nbclient
```
If you did these steps then you should have a file `data-literacy-project/pyproject.toml` which looks something like this

```yaml
[project]
name = "data-literacy-project"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
    "gapminder>=0.1",
    "ipykernel>=6.29.5",
    "jupyter>=1.1.1",
    "plotnine>=0.13.6",
    "polars>=1.8.2",
    "setuptools>=75.1.0",
]
```

## Activation

Finally we need to activate the environment we created

```
source .venv/bin/activate
```

